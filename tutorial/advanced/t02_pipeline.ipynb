{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Tutorial 2: Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will discuss the following topics:\n",
    "\n",
    "* [Iterating Through Pipeline](#ta02itp)\n",
    "    * [Basic Concept](#ta02bc)\n",
    "    * [Example](#ta02example)\n",
    "* [Dropping Last Batch](#ta02dlb)\n",
    "* [Padding Batch Data](#ta02pbd)\n",
    "* [Benchmark Pipeline Speed](#ta02bps)\n",
    "\n",
    "In the [beginner tutorial 4](../beginner/t04_pipeline.ipynb), we learned how to build a data pipeline that handles data loading and preprocessing tasks efficiently. Now that you have understood some basic operations in the `Pipeline`, we will demonstrate some advanced concepts and how to leverage them to create efficient `Pipelines` in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ta02itp'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterating Through Pipeline\n",
    "\n",
    "In many deep learning tasks, the parameters for preprocessing tasks are precomputed by looping through the dataset. For example, in the `ImageNet` dataset, people usually use a precomputed global pixel average for each channel to normalize the images. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ta02bc'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Concept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will see how to iterate through the pipeline in FastEstimator. First we will create a sample NumpyDataset from the data dictionary and load it into a `Pipeline`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from fastestimator.dataset.data import cifar10\n",
    "    \n",
    "# sample numpy array to later create datasets from them\n",
    "x_train, y_train = (np.random.sample((10, 2)), np.random.sample((10, 1)))\n",
    "train_data = {\"x\": x_train, \"y\": y_train}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastestimator as fe\n",
    "from fastestimator.dataset.numpy_dataset import NumpyDataset\n",
    "\n",
    "# create NumpyDataset from the sample data\n",
    "dataset_fe = NumpyDataset(train_data)\n",
    "\n",
    "pipeline_fe = fe.Pipeline(train_data=dataset_fe, batch_size=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the loader object for the `Pipeline`, then iterate through the loader with a for loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': tensor([[0.1288, 0.2118],\n",
      "        [0.9344, 0.5583],\n",
      "        [0.0879, 0.5939]], dtype=torch.float64), 'y': tensor([[0.8071],\n",
      "        [0.8469],\n",
      "        [0.9160]], dtype=torch.float64)}\n",
      "{'x': tensor([[0.7866, 0.8248],\n",
      "        [0.3285, 0.9311],\n",
      "        [0.7637, 0.9474]], dtype=torch.float64), 'y': tensor([[0.5504],\n",
      "        [0.8430],\n",
      "        [0.7415]], dtype=torch.float64)}\n",
      "{'x': tensor([[0.3689, 0.3373],\n",
      "        [0.3407, 0.0571],\n",
      "        [0.2216, 0.1906]], dtype=torch.float64), 'y': tensor([[0.6517],\n",
      "        [0.4824],\n",
      "        [0.5171]], dtype=torch.float64)}\n",
      "{'x': tensor([[0.6018, 0.4306]], dtype=torch.float64), 'y': tensor([[0.0023]], dtype=torch.float64)}\n"
     ]
    }
   ],
   "source": [
    "loader_fe = pipeline_fe.get_loader(mode=\"train\")\n",
    "\n",
    "for batch in loader_fe:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ta02example'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we have CIFAR-10 dataset and we want to find global average pixel value over three channels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastestimator.dataset.data import cifar10\n",
    "\n",
    "cifar_train, _ = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will take the `batch_size` 64 and load the data into `Pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_cifar = fe.Pipeline(train_data=cifar_train, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will iterate through batch data and compute the mean pixel values for all three channels of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_fe = pipeline_cifar.get_loader(mode=\"train\", shuffle=False)\n",
    "mean_arr = np.zeros((3))\n",
    "for i, batch in enumerate(loader_fe):\n",
    "    mean_arr = mean_arr + np.mean(batch[\"x\"].numpy(), axis=(0, 1, 2))\n",
    "mean_arr = mean_arr / (i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean pixel value over the channels are:  [125.32287898 122.96682199 113.8856495 ]\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean pixel value over the channels are: \", mean_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ta02dlb'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping Last Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the total number of dataset elements is not divisible by the `batch_size`, by default, the last batch will have less data than other batches.  To drop the last batch we can set `drop_last` to `True`. Therefore, if the last batch is incomplete it will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_fe = fe.Pipeline(train_data=dataset_fe, batch_size=3, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ta02pbd'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding Batch Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There might be scenario where the input tensors have different dimensions within a batch. For example, in Natural Language Processing, we have input strings with different lengths. For that we need to pad the data to the maximum length within the batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To further illustrate in code, we will take numpy array that contains different shapes of array elements and load it into the `Pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define numpy arrays with different shapes\n",
    "elem1 = np.array([4, 5])\n",
    "elem2 = np.array([1, 2, 6])\n",
    "elem3 = np.array([3])\n",
    "\n",
    "# create train dataset\n",
    "x_train = np.array([elem1, elem2, elem3])\n",
    "train_data = {\"x\": x_train}\n",
    "dataset_fe = NumpyDataset(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will set any `pad_value` that we want to append at the end of the tensor data. `pad_value` can be either `int` or `float`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_fe = fe.Pipeline(train_data=dataset_fe, batch_size=3, pad_value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's print the batch data after padding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': tensor([[4, 5, 0],\n",
      "        [1, 2, 6],\n",
      "        [3, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "for elem in iter(pipeline_fe.get_loader(mode='train', shuffle=False)):\n",
    "    print(elem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ta02bps'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Pipeline Speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often the case that the bottleneck of deep learning training is the data pipeline. As a result, the GPU may be underutilized. FastEstimator provides a method to check the speed of a `Pipeline` in order to help diagnose any potential problems. The way to benchmark `Pipeline` speed in FastEstimator is very simple: call `Pipeline.benchmark`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For illustration, we will create a `Pipeline` for the CIFAR-10 dataset with list of Numpy operators that expand dimensions, apply `Minmax` and finally `Rotate` the input images: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastestimator.op.numpyop.univariate import Minmax, ExpandDims\n",
    "from fastestimator.op.numpyop.multivariate import Rotate\n",
    "\n",
    "pipeline = fe.Pipeline(train_data=cifar_train,\n",
    "                       ops=[ExpandDims(inputs=\"x\", outputs=\"x\"),\n",
    "                            Minmax(inputs=\"x\", outputs=\"x_out\"),\n",
    "                            Rotate(image_in=\"x_out\", image_out=\"x_out\", limit=180)],\n",
    "                      batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's benchmark the pre-processing speed for this pipeline in training mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastEstimator: Step: 100, Epoch: 1, Steps/sec: 797.9008250605733\n",
      "FastEstimator: Step: 200, Epoch: 1, Steps/sec: 2249.3393577839283\n",
      "FastEstimator: Step: 300, Epoch: 1, Steps/sec: 2236.913774803168\n",
      "FastEstimator: Step: 400, Epoch: 1, Steps/sec: 2244.6406454903963\n",
      "FastEstimator: Step: 500, Epoch: 1, Steps/sec: 2303.2515324338206\n",
      "FastEstimator: Step: 600, Epoch: 1, Steps/sec: 2250.139806811566\n",
      "FastEstimator: Step: 700, Epoch: 1, Steps/sec: 2310.7264336983017\n"
     ]
    }
   ],
   "source": [
    "pipeline_cifar.benchmark(mode=\"train\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
