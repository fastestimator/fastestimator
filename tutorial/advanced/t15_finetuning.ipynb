{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "In this tutorial we are going to cover finetuning using FastEstimator. This tutorial is structured as follows:\n",
    "\n",
    "* [Setting Things Up](#ta15setup)\n",
    "    * [Define Reusable Methods](#ta15resuable)\n",
    "* [Tensorflow Workflow](#ta15tfworkflow)\n",
    "    * [Train Base Model](#ta15tftrain)\n",
    "    * [Extending Base Model for finetuning](#ta15tfmodify)\n",
    "        * [Import Pretrained Model](#ta15tffreeze)\n",
    "        * [Extending Base Model](#ta15tfunfreeze)\n",
    "        * [Combine Base Model and Finetune Model](#ta15tfcombine)\n",
    "    * [Start Finetuning](#ta15tffinetune)\n",
    "* [Pytorch Workflow](#ta15pytorchworkflow)\n",
    "    * [Train Base Model](#ta15pytorchtrain)\n",
    "    * [Extending Base Model for finetuning](#ta15pytorchmodify)\n",
    "        * [Import Pretrained Model](#ta15pytorchfreeze)\n",
    "        * [Extending Base Model](#ta15pytorchunfreeze)\n",
    "        * [Combine Base Model and Finetune Model](#ta15torchcombine)\n",
    "    * [Start Finetuning](#ta15pytorchfinetune)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Things Up <a id='ta15setup'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First let's get some imports out of the way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "\n",
    "import fastestimator as fe\n",
    "from fastestimator.trace.metric import Accuracy\n",
    "from fastestimator.op.numpyop.univariate import ChannelTranspose, CoarseDropout, Normalize, Onehot\n",
    "from fastestimator.op.numpyop.meta import Sometimes\n",
    "from fastestimator.op.numpyop.multivariate import HorizontalFlip, PadIfNeeded, RandomCrop\n",
    "from fastestimator.schedule.schedule import EpochScheduler\n",
    "from fastestimator.op.tensorop.loss import CrossEntropy\n",
    "from fastestimator.op.tensorop.model import ModelOp, UpdateOp\n",
    "from fastestimator.dataset.data import cifair100, cifair10\n",
    "from fastestimator.op.tensorop.model import ModelOp, UpdateOp\n",
    "\n",
    "from fastestimator.architecture.tensorflow import LeNet as lenet_tf\n",
    "from tensorflow.python.keras import Sequential, layers\n",
    "from tensorflow.keras import Model\n",
    "from fastestimator.architecture.pytorch import LeNet as lenet_torch\n",
    "import torch.nn as nn\n",
    "from torch import load, Tensor\n",
    "import torch.nn.functional as fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Reusable Methods <a id='ta15resuable'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pipeline(dataset, num_classes, batch_size, mode='tf', min_height=40, min_width=40):\n",
    "\n",
    "    train_data, eval_data = dataset.load_data()\n",
    "\n",
    "    mean_value = (0.4914, 0.4822, 0.4465)\n",
    "    std_value = (0.2471, 0.2435, 0.2616)\n",
    "\n",
    "    ops = [ Normalize(inputs=\"x\", outputs=\"x\", mean=mean_value, std=std_value),\n",
    "            PadIfNeeded(min_height=min_height, min_width=min_width, image_in=\"x\", image_out=\"x\", mode=\"train\"),\n",
    "            RandomCrop(32, 32, image_in=\"x\", image_out=\"x\", mode=\"train\"),\n",
    "            Sometimes(HorizontalFlip(image_in=\"x\", image_out=\"x\", mode=\"train\")),\n",
    "            CoarseDropout(inputs=\"x\", outputs=\"x\", mode=\"train\", max_holes=1),\n",
    "            Onehot(inputs=\"y\", outputs=\"y\", mode=\"train\", num_classes=num_classes, label_smoothing=0.2)]\n",
    "\n",
    "    if mode == 'torch':\n",
    "        ops.append(ChannelTranspose(inputs=\"x\", outputs=\"x\"))\n",
    "                \n",
    "    return fe.Pipeline(\n",
    "                train_data=train_data,\n",
    "                eval_data=eval_data,\n",
    "                batch_size=batch_size,\n",
    "                ops=ops)\n",
    "\n",
    "def get_network(model):\n",
    "    return  fe.Network(ops=[\n",
    "        ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),\n",
    "        CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),\n",
    "        UpdateOp(model=model, loss_name=\"ce\")])\n",
    "\n",
    "def get_estimator(pipeline, network, epochs, train_steps_per_epoch=None, eval_steps_per_epoch=None):\n",
    "    traces = [Accuracy(true_key=\"y\", pred_key=\"y_pred\")]\n",
    "\n",
    "    return fe.Estimator(pipeline=pipeline,\n",
    "                                network=network,\n",
    "                                epochs=epochs,\n",
    "                                traces=traces,\n",
    "                                log_steps=0,\n",
    "                                train_steps_per_epoch=train_steps_per_epoch, \n",
    "                                eval_steps_per_epoch=eval_steps_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's load some default training parameters as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training parameters\n",
    "epochs_pretrain = 10\n",
    "\n",
    "epochs_finetune = 5\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "base_num_classes = 100 \n",
    "\n",
    "finetune_num_classes = 10\n",
    "\n",
    "model_dir = tempfile.gettempdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow Workflow <a id='ta15tfworkflow'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Base Model <a id='ta15tftrain'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that boring stuff is done, let's train our first base model. We are using tensorflow LeNet to train on cifar100 with 100 classes. We are training for 10 epochs and saving the model at the end of the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastEstimator-Warn: Pipeline multiprocessing is disabled. OS must support the 'fork' start method.\n",
      "    ______           __  ______     __  _                 __            \n",
      "   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n",
      "  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n",
      " / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n",
      "/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n",
      "                                                                        \n",
      "\n",
      "FastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\n",
      "FastEstimator-Start: step: 1; logging_interval: 0; num_device: 0;\n",
      "FastEstimator-Eval: step: 782; epoch: 1; accuracy: 0.1447; ce: 3.6391912;\n",
      "FastEstimator-Eval: step: 1564; epoch: 2; accuracy: 0.2173; ce: 3.272597;\n",
      "FastEstimator-Eval: step: 2346; epoch: 3; accuracy: 0.2578; ce: 3.0625503;\n",
      "FastEstimator-Eval: step: 3128; epoch: 4; accuracy: 0.2893; ce: 2.9065394;\n",
      "FastEstimator-Eval: step: 3910; epoch: 5; accuracy: 0.3037; ce: 2.8252454;\n",
      "FastEstimator-Eval: step: 4692; epoch: 6; accuracy: 0.3158; ce: 2.7689734;\n",
      "FastEstimator-Eval: step: 5474; epoch: 7; accuracy: 0.3321; ce: 2.6793952;\n",
      "FastEstimator-Eval: step: 6256; epoch: 8; accuracy: 0.3431; ce: 2.649083;\n",
      "FastEstimator-Eval: step: 7038; epoch: 9; accuracy: 0.3437; ce: 2.6294968;\n",
      "FastEstimator-Eval: step: 7820; epoch: 10; accuracy: 0.3471; ce: 2.5796824;\n",
      "FastEstimator-Finish: step: 7820; model_lr: 0.001; total_time: 324.59 sec;\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'C:\\\\windows\\\\TEMP\\\\lenet_tf.h5'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_input_shape = (32, 32, 3)\n",
    "\n",
    "model_tf_pretrain = fe.build(model_fn=lambda: lenet_tf(input_shape=tf_input_shape, classes=base_num_classes), optimizer_fn=\"adam\")\n",
    "\n",
    "pipeline_tf_pretrain = get_pipeline(cifair100, base_num_classes, batch_size)\n",
    "\n",
    "network_tf_pretrain = get_network(model_tf_pretrain)\n",
    "\n",
    "estimator_tf_pretrain = get_estimator(pipeline_tf_pretrain, network_tf_pretrain, epochs_pretrain)\n",
    "\n",
    "estimator_tf_pretrain.fit(warmup=False)\n",
    "\n",
    "fe.backend.save_model(model_tf_pretrain, save_dir=model_dir, model_name= \"lenet_tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load a new dataset for finetuning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For finetuning, We use FastEstimator API to load the ciFAIR-10 dataset. You can use your own dataset by updating `get_pipeline` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastEstimator-Warn: Pipeline multiprocessing is disabled. OS must support the 'fork' start method.\n"
     ]
    }
   ],
   "source": [
    "pipeline_tf_finetune = get_pipeline(cifair10, finetune_num_classes, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extending Base Model for Finetuning <a id='ta15tfmodify'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import Pretrained Model <a id='ta15tffreeze'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to extend our base model with finetuning task.\n",
    "\n",
    "Let's load our pretrained weights saved in previous setup. The weights files are saved with `h5` extension, since we have given `lenet_tf` as model_name to the `save_model`  function the model weights are saved as `lenet_tf.h5`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_path = os.path.join(model_dir, \"lenet_tf.h5\")\n",
    "\n",
    "pretrained_lenet_tf = lenet_tf(input_shape=tf_input_shape, classes=base_num_classes)\n",
    "\n",
    "pretrained_lenet_tf.load_weights(weights_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extending Base Model <a id='ta15tfunfreeze'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove the classification head of pretrained model and build a backbone. We will be using `fe.build` to build a new fe model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf_backbone(pretrained_model):\n",
    "\n",
    "    model = Model(inputs=pretrained_model.inputs, outputs=pretrained_model.layers[-3].output)\n",
    "\n",
    "    return model\n",
    "\n",
    "backbone_tf = fe.build(model_fn=lambda: get_tf_backbone(pretrained_lenet_tf), optimizer_fn=\"adam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will define a classification head that can be used for the finetuning task. This is simply two `Dense` layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_head(finetune_num_classes):\n",
    "    return Sequential([layers.Dense(64, activation='relu', input_shape=(1024,)), \n",
    "                       layers.Dense(finetune_num_classes, activation='softmax')])\n",
    "    \n",
    "cls_head_tf_finetune = fe.build(model_fn=lambda: get_class_head(finetune_num_classes), optimizer_fn=\"adam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Combine Base Model and Finetune Model <a id='ta15tfcombine'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to save the finetune model, we can combine the `Backbone Model` and the `Class Head Model` and provide it to ModelSaver later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_tf_model(backbone_model, cls_head_finetune):\n",
    "\n",
    "    backbone_output = backbone_model.layers[-1].output\n",
    "    x = cls_head_finetune.layers[0](backbone_output)\n",
    "    x = cls_head_finetune.layers[1](x)\n",
    "    model = Model(inputs=backbone_model.inputs, outputs=x)\n",
    "    return model\n",
    "\n",
    "final_model_tf = fe.build(model_fn=lambda: combined_tf_model(backbone_tf, cls_head_tf_finetune),  optimizer_fn=\"adam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start Finetuning <a id='ta15tffinetune'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Finetuning, we want to train different part of the network in the following manner:\n",
    "- epoch 1-3: `freeze` backbone, `train` classification head only\n",
    "- epoch 4-end: `train` backbone and classification head `together`\n",
    "\n",
    "Let's use EpochScheduler to define when backbone and class head weights are updated. UpdateOp is responsible for weight updating. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_tf_finetune = fe.Network(ops=[\n",
    "                                ModelOp(model=backbone_tf, inputs=\"x\", outputs=\"feature\"),\n",
    "                                ModelOp(model=cls_head_tf_finetune, inputs=\"feature\", outputs=\"y_pred\"),\n",
    "                                CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\", from_logits=True),\n",
    "                                EpochScheduler({1: None, 4: UpdateOp(model=backbone_tf, loss_name=\"ce\")}),\n",
    "                                EpochScheduler({1: UpdateOp(model=cls_head_tf_finetune, loss_name=\"ce\")})])\n",
    "\n",
    "estimator_tf_finetune = get_estimator(pipeline_tf_finetune, network_tf_finetune, epochs_finetune)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train our finetune model using pretrained weights on our new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ______           __  ______     __  _                 __            \n",
      "   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n",
      "  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n",
      " / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n",
      "/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n",
      "                                                                        \n",
      "\n",
      "FastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\n",
      "FastEstimator-Start: step: 1; logging_interval: 0; num_device: 0;\n",
      "FastEstimator-Eval: step: 782; epoch: 1; accuracy: 0.6993; ce: 0.9487608;\n",
      "FastEstimator-Eval: step: 1564; epoch: 2; accuracy: 0.7094; ce: 0.93741965;\n",
      "FastEstimator-Eval: step: 2346; epoch: 3; accuracy: 0.7071; ce: 0.9292741;\n",
      "FastEstimator-Eval: step: 3128; epoch: 4; accuracy: 0.7089; ce: 0.938162;\n",
      "FastEstimator-Eval: step: 3910; epoch: 5; accuracy: 0.6997; ce: 0.94834113;\n",
      "FastEstimator-Finish: step: 3910; model1_lr: 0.001; model2_lr: 0.001; total_time: 90.7 sec;\n"
     ]
    }
   ],
   "source": [
    "estimator_tf_finetune.fit(warmup=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's save our finetuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\windows\\\\TEMP\\\\final_tf_finetune.h5'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fe.backend.save_model(final_model_tf, save_dir=model_dir, model_name= \"final_tf_finetune\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch Workflow <a id='ta15pytorchworkflow'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Base Model <a id='ta15pytorchtrain'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that boring stuff is done, let's train our first base model. We are using pytorch LeNet to train on cifar100 with 100 classes. We are training for 10 epochs and saving the model at the end of the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastEstimator-Warn: Pipeline multiprocessing is disabled. OS must support the 'fork' start method.\n",
      "    ______           __  ______     __  _                 __            \n",
      "   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n",
      "  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n",
      " / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n",
      "/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n",
      "                                                                        \n",
      "\n",
      "FastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\n",
      "FastEstimator-Start: step: 1; logging_interval: 0; num_device: 0;\n",
      "FastEstimator-Eval: step: 782; epoch: 1; accuracy: 0.1446; ce: 3.6544788;\n",
      "FastEstimator-Eval: step: 1564; epoch: 2; accuracy: 0.1972; ce: 3.340201;\n",
      "FastEstimator-Eval: step: 2346; epoch: 3; accuracy: 0.2335; ce: 3.1667938;\n",
      "FastEstimator-Eval: step: 3128; epoch: 4; accuracy: 0.2721; ce: 2.971765;\n",
      "FastEstimator-Eval: step: 3910; epoch: 5; accuracy: 0.2929; ce: 2.8845372;\n",
      "FastEstimator-Eval: step: 4692; epoch: 6; accuracy: 0.2981; ce: 2.8609293;\n",
      "FastEstimator-Eval: step: 5474; epoch: 7; accuracy: 0.3076; ce: 2.8059654;\n",
      "FastEstimator-Eval: step: 6256; epoch: 8; accuracy: 0.3181; ce: 2.7457006;\n",
      "FastEstimator-Eval: step: 7038; epoch: 9; accuracy: 0.3289; ce: 2.6596684;\n",
      "FastEstimator-Eval: step: 7820; epoch: 10; accuracy: 0.3299; ce: 2.6899476;\n",
      "FastEstimator-Finish: step: 7820; model4_lr: 0.001; total_time: 467.16 sec;\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'C:\\\\windows\\\\TEMP\\\\lenet_torch.pt'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_input_shape = (3, 32, 32)\n",
    "\n",
    "model_torch_pretrain = fe.build(model_fn=lambda: lenet_torch(input_shape=torch_input_shape, classes=base_num_classes), optimizer_fn=\"adam\")\n",
    "\n",
    "pipeline_torch_pretrain = get_pipeline(cifair100, base_num_classes, batch_size, 'torch')\n",
    "\n",
    "network_torch_pretrain = get_network(model_torch_pretrain)\n",
    "\n",
    "estimator_torch_pretrain = get_estimator(pipeline_torch_pretrain, network_torch_pretrain, epochs_pretrain)\n",
    "\n",
    "estimator_torch_pretrain.fit()\n",
    "\n",
    "fe.backend.save_model(model_torch_pretrain, save_dir=model_dir, model_name= \"lenet_torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load a new dataset for finetuning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For finetuning, We use FastEstimator API to load the ciFAIR-10 dataset. You can use your own dataset by changing `get_pipeline` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastEstimator-Warn: Pipeline multiprocessing is disabled. OS must support the 'fork' start method.\n"
     ]
    }
   ],
   "source": [
    "pipeline_torch_finetune = get_pipeline(cifair10, finetune_num_classes, batch_size, 'torch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extending Base Model for finetuning <a id='ta15pytorchmodify'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import Pretrained Model<a id='ta15pytorchfreeze'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to extend our base model with finetuning task.\n",
    "\n",
    "Let's load our pretrained weights saved in our previous setup. The weights files are saved with h5 extension, since we have given `lenet_torch` as model_name to the `save_model`  function the model weights are saved as `lenet_torch.pt`. Replace it if you used different model_name in `save_model` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_torch_pretrained = lenet_torch(input_shape=torch_input_shape, classes=base_num_classes)\n",
    "\n",
    "model_torch_pretrained.load_state_dict(load(os.path.join(model_dir, 'lenet_torch.pt')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extending Base Model <a id='ta15pytorchunfreeze'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove the last layer of pretrained model and build a new backbone. We will be using fe.build to build a new fe model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackboneTorch(nn.Module):\n",
    "    def __init__(self, model_torch_pretrained) -> None:\n",
    "        super().__init__()\n",
    "        self.pool_kernel = 2\n",
    "        self.backbone_layers = nn.Sequential(*(list(model_torch_pretrained.children())[:-2]))\n",
    "\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = fn.relu(self.backbone_layers[0](x))\n",
    "        x = fn.max_pool2d(x, self.pool_kernel)\n",
    "        x = fn.relu(self.backbone_layers[1](x))\n",
    "        x = fn.max_pool2d(x, self.pool_kernel)\n",
    "        x = fn.relu(self.backbone_layers[2](x))\n",
    "        return x\n",
    "\n",
    "backbone_torch = fe.build(model_fn=lambda: BackboneTorch(model_torch_pretrained), optimizer_fn=\"adam\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will define a classification head that can be used for the finetuning task. This is simply two `nn.Linear` layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierHead(nn.Module):\n",
    "    def __init__(self, classes=10):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(1024, 64)\n",
    "        self.fc2 = nn.Linear(64, classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = fn.relu(self.fc1(x))\n",
    "        x = fn.softmax(self.fc2(x), dim=-1)\n",
    "        return x\n",
    "\n",
    "# dimensions of last layer of backbone\n",
    "cls_head_torch_finetune = fe.build(model_fn=lambda: ClassifierHead(classes=finetune_num_classes), optimizer_fn=\"adam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Combine Base Model and Finetune Model <a id='ta15torchcombine'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to save the finetune model, we can combine the `Backbone Model` and the `Class Head Model` and provide it to ModelSaver later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedTorchModel(nn.Module):\n",
    "    def __init__(self, backbone, cls_head):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.cls_head = cls_head\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.cls_head(x)\n",
    "        return x\n",
    "\n",
    "final_torch_model = fe.build(model_fn=lambda: CombinedTorchModel(backbone_torch, cls_head_torch_finetune), optimizer_fn=None) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start Finetuning <a id='ta15pytorchfinetune'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Finetuning, we want to train different part of the network in the following manner:\n",
    "- epoch 1-3: `freeze` backbone, `train` classification head only\n",
    "- epoch 4-end: `train` backbone and classification head `together`\n",
    "\n",
    "Let's use EpochScheduler to define when backbone and class head weights are updated. UpdateOp is responsible for weight updating. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_torch_finetune = fe.Network(ops=[\n",
    "                                ModelOp(model=backbone_torch, inputs=\"x\", outputs=\"feature\"),\n",
    "                                ModelOp(model=cls_head_torch_finetune, inputs=\"feature\", outputs=\"y_pred\"),\n",
    "                                CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\", from_logits=True),\n",
    "                                EpochScheduler({1: None, 1: UpdateOp(model=backbone_torch, loss_name=\"ce\")}),\n",
    "                                EpochScheduler({1: UpdateOp(model=cls_head_torch_finetune, loss_name=\"ce\")})])\n",
    "\n",
    "estimator_torch_finetune = get_estimator(pipeline_torch_finetune, network_torch_finetune, epochs_finetune)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train our finetune model using pretrained weights on our new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ______           __  ______     __  _                 __            \n",
      "   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n",
      "  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n",
      " / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n",
      "/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n",
      "                                                                        \n",
      "\n",
      "FastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\n",
      "FastEstimator-Start: step: 1; logging_interval: 0; num_device: 0;\n",
      "FastEstimator-Eval: step: 782; epoch: 1; accuracy: 0.575; ce: 1.8876605;\n",
      "FastEstimator-Eval: step: 1564; epoch: 2; accuracy: 0.608; ce: 1.8521833;\n",
      "FastEstimator-Eval: step: 2346; epoch: 3; accuracy: 0.6366; ce: 1.8237936;\n",
      "FastEstimator-Eval: step: 3128; epoch: 4; accuracy: 0.6447; ce: 1.8164377;\n",
      "FastEstimator-Eval: step: 3910; epoch: 5; accuracy: 0.648; ce: 1.8117273;\n",
      "FastEstimator-Finish: step: 3910; model7_lr: 0.001; model8_lr: 0.001; total_time: 280.38 sec;\n"
     ]
    }
   ],
   "source": [
    "estimator_torch_finetune.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's save our finetuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\windows\\\\TEMP\\\\final_torch_finetune.pt'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fe.backend.save_model(final_torch_model, save_dir=model_dir, model_name= \"final_torch_finetune\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ta15finetune'></a>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
