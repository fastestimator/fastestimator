{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Tutorial 1: Dataset\n",
    "\n",
    "## Overview\n",
    "In this tutorial, we will talk about the following topics:\n",
    "* [Dataset Summary](#ta01summary)\n",
    "* [Dataset Splitting](#ta01splitting)\n",
    "    * [Random Fraction Split](#ta01rfs)\n",
    "    * [Random Count Split](#ta01rcs)\n",
    "    * [Index Split](#ta01is)\n",
    "* [Global Dataset Editing](#ta01gde)\n",
    "* [BatchDataset](#ta01bd)\n",
    "    * [Deterministic Batching](#ta01deterministic)\n",
    "    * [Distribution Batching](#ta01distribution)\n",
    "    * [Unpaired Dataset](#ta01ud)\n",
    "* [InterleaveDataset](#ta01id)\n",
    "    * [Dataset Interleaving](#ta01di)\n",
    "    * [Custom-Pattern Interleaving](#ta01cpi)\n",
    "    * [Operator Control](#ta01oc)\n",
    "    * [Batch Control](#ta01bc)\n",
    "* [Related Apphub Examples](#ta01rae)\n",
    "\n",
    "Before going through the tutorial, it is recommended to check [Beginner Tutorial 2](../beginner/t02_dataset.ipynb) for basic understanding of `dataset` from PyTorch and FastEstimator. We will talk about more details about `fe.dataset` API in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ta01summary'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset summary\n",
    "As we have mentioned in previous tutorial, users can import our inherited dataset class for easy use in `Pipeline`. But how do we know what keys are available in the dataset?   Well, obviously one easy way is just call `dataset[0]` and check the keys. However, there's a more elegant way to check information of dataset: `dataset.summary()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastestimator.dataset.data.mnist import load_data\n",
    "train_data, eval_data = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetSummary {'num_instances': 60000, 'keys': {'x': <KeySummary {'shape': [28, 28], 'dtype': 'uint8'}>, 'y': <KeySummary {'num_unique_values': 10, 'shape': [], 'dtype': 'uint8'}>}}>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or even more simply, by invoking the print function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"num_instances\": 60000, \"keys\": {\"x\": {\"shape\": [28, 28], \"dtype\": \"uint8\"}, \"y\": {\"num_unique_values\": 10, \"shape\": [], \"dtype\": \"uint8\"}}}\n"
     ]
    }
   ],
   "source": [
    "print(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ta01splitting'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Splitting\n",
    "\n",
    "Dataset splitting is nothing new in machine learning. In FastEstimator, users can easily split their data in different ways. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ta01rfs'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Fraction Split\n",
    "Let's say we want to randomly split 50% of the evaluation data into test data. This is easily accomplished by the following way. As a result of the split, the data in `test_data` is removed from `eval_data` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = eval_data.split(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, if you want to split evaluation data into two test datasets with 20% of the evaluation data each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data1, test_data2 = eval_data.split(0.2, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ta01rcs'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Count Split\n",
    "Sometimes instead of fractions, we want an actual number of examples to split; for example, randomly splitting 100 samples from the evaluation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data3 = eval_data.split(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And of course, we can generate multiple datasets by providing multiple inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data4, test_data5 = eval_data.split(100, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ta01is'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index Split\n",
    "There are times when we need to split the dataset in a specific way. For that, you can provide a list of indexes. For example, if we want to split the 0th, 1st and 100th element of evaluation dataset into new test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data6 = eval_data.split([0,1,100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you just want continuous index, here's an easy way to provide index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data7 = eval_data.split(range(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Needless to say, you can provide multiple inputs too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data7, test_data8 = eval_data.split([0, 1 ,2], [3, 4, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ta01gde'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Dataset Editing\n",
    "In deep learning, we usually process the dataset batch by batch. However, when we are handling tabular data, we might need to apply some transformation globally before the training. For example, we may want to standardize the tabular data using `sklearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastestimator.dataset.data.breast_cancer import load_data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "train_data, eval_data = load_data()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "train_data[\"x\"] = scaler.fit_transform(train_data[\"x\"])\n",
    "eval_data[\"x\"] = scaler.transform(eval_data[\"x\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another popular use case of global dataset editing is when you wanted to __add a new feature globally to all samples to a dataset__. For example, each sample of the above dataset currently has two keys: `x` and `y`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': array([-1.4407529 , -0.43531948, -1.3620849 , -1.139118  ,  0.7805734 ,\n",
      "        0.7189211 ,  2.8231344 , -0.11914958,  1.0926621 ,  2.458172  ,\n",
      "       -0.2638004 , -0.01605244, -0.4704136 , -0.4747609 ,  0.8383651 ,\n",
      "        3.251027  ,  8.438936  ,  3.3919873 ,  2.6211658 ,  2.0612078 ,\n",
      "       -1.2328612 , -0.47630954, -1.2479202 , -0.9739676 ,  0.7228946 ,\n",
      "        1.1867324 ,  4.672828  ,  0.9320124 ,  2.0972424 ,  1.8864503 ],\n",
      "      dtype=float32), 'y': 1}\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add a new key named `data_name`, and apply globally to all samples of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': array([-1.4407529 , -0.43531948, -1.3620849 , -1.139118  ,  0.7805734 ,\n",
      "        0.7189211 ,  2.8231344 , -0.11914958,  1.0926621 ,  2.458172  ,\n",
      "       -0.2638004 , -0.01605244, -0.4704136 , -0.4747609 ,  0.8383651 ,\n",
      "        3.251027  ,  8.438936  ,  3.3919873 ,  2.6211658 ,  2.0612078 ,\n",
      "       -1.2328612 , -0.47630954, -1.2479202 , -0.9739676 ,  0.7228946 ,\n",
      "        1.1867324 ,  4.672828  ,  0.9320124 ,  2.0972424 ,  1.8864503 ],\n",
      "      dtype=float32), 'y': 1, 'data_name': 'breast_cancer'}\n"
     ]
    }
   ],
   "source": [
    "train_data[\"data_name\"] = [\"breast_cancer\" for _ in range(len(train_data))]\n",
    "\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now every sample has an additional key and its value, this can be used further in Operator or Trace to do dataset-conditioned operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ta01bd'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BatchDataset\n",
    "\n",
    "There might be scenarios where we need to combine multiple datasets together into one dataset in a specific way. Let's consider three such use-cases now:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ta01deterministic'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deterministic Batching\n",
    "Let's say we have `mnist` and `cifair` datasets, and want to combine them with a total batch size of 8. If we always want 4 examples from `mnist` and the rest from `cifair`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastestimator.dataset.data import mnist, cifair10\n",
    "from fastestimator.dataset import BatchDataset\n",
    "\n",
    "mnist_data, _ = mnist.load_data(image_key=\"x\", label_key=\"y\")\n",
    "cifair_data, _ = cifair10.load_data(image_key=\"x\", label_key=\"y\")\n",
    "\n",
    "dataset_deterministic = BatchDataset(datasets=[mnist_data, cifair_data], num_samples=[4,4])\n",
    "# ready to use dataset_deterministic in Pipeline, you might need to resize them to have consistent shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ta01distribution'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution Batching\n",
    "Some people prefer randomness in a batch. For example, given total batch size of 8, let's say we want 0.5 probability of `mnist` and the other 0.5 from `cifair`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastestimator.dataset.data import mnist, cifair10\n",
    "from fastestimator.dataset import BatchDataset\n",
    "\n",
    "mnist_data, _ = mnist.load_data(image_key=\"x\", label_key=\"y\")\n",
    "cifair_data, _ = cifair10.load_data(image_key=\"x\", label_key=\"y\")\n",
    "\n",
    "dataset_distribution = BatchDataset(datasets=[mnist_data, cifair_data], num_samples=8, probability=[0.5, 0.5])\n",
    "# ready to use dataset_distribution in Pipeline, you might need to resize them to have consistent shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ta01ud'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unpaired Dataset\n",
    "Some deep learning tasks require random unpaired datasets. For example, in image-to-image translation (like Cycle-GAN), the system needs to randomly sample one horse image and one zebra image for every batch. In FastEstimator, `BatchDataset` can also handle unpaired datasets. The only restriction is that: **keys from two different datasets must be unique for unpaired datasets**.\n",
    "\n",
    "For example, let's sample one image from `mnist` and one image from `cifair` for every batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastestimator.dataset.data import mnist, cifair10\n",
    "from fastestimator.dataset import BatchDataset\n",
    "\n",
    "mnist_data, _ = mnist.load_data(image_key=\"x_mnist\", label_key=\"y_mnist\")\n",
    "cifair_data, _ = cifair10.load_data(image_key=\"x_cifair\", label_key=\"y_cifair\")\n",
    "\n",
    "dataset_unpaired = BatchDataset(datasets=[mnist_data, cifair_data], num_samples=[1,1])\n",
    "# ready to use dataset_unpaired in Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ta01id'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## InterleaveDataset\n",
    "\n",
    "When you train a network that can perform different tasks from multiple datasets, it is generally a good practice to mix different datasets in a batch for the ease of convergence. Unfortunately, it may not be possible to merge multiple datasets into one batch sometimes due to reasons like:\n",
    "1. __Not enough GPU memory__: When you have dozens of datasets & tasks or each sample's data dimension is too large, then you may not even fit a batch size of 1 for every dataset.\n",
    "2. __Inconsistent data dimension__: For a batch to form, it requires each sample having the same spatial dimension. However, this may not be feasible under certain situations. For example, you might have one dataset with resolution of [128, 128, 9] and another dataset with resolution of [384, 384, 256]. Resizing both datasets to a single size would inevitably introduce artifact or loss of information.\n",
    "\n",
    "To overcome such challenge, one solution is to __distribute multiple datasets across different training steps in a particular pattern__. For example, one way to do so can be:\n",
    "\n",
    "* step 1: train on dataset1\n",
    "* step 2: train on dataset2\n",
    "* step 3: train on dataset1\n",
    "* step 4: train on dataset2\n",
    "* ...\n",
    "\n",
    "We define this general way of dataset distribution as `Dataset Interleaving`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ta01di'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Interleaving\n",
    "\n",
    "One can simply achieve Dataset Interleaving by using `InterleaveDataset` API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, using dataset id: 0\n",
      "step: 1, using dataset id: 1\n",
      "step: 2, using dataset id: 0\n",
      "step: 3, using dataset id: 1\n",
      "step: 4, using dataset id: 0\n"
     ]
    }
   ],
   "source": [
    "from fastestimator.dataset.numpy_dataset import NumpyDataset\n",
    "from fastestimator.dataset.interleave_dataset import InterleaveDataset\n",
    "\n",
    "data1 = NumpyDataset(data={\"x\": [x for x in range(10)], \"ds_id\": [0 for _ in range(10)]})\n",
    "data2 = NumpyDataset(data={\"x\": [x for x in range(10)], \"ds_id\": [1 for _ in range(10)]})\n",
    "\n",
    "interleave_data = InterleaveDataset(datasets=[data1, data2])\n",
    "\n",
    "for idx in range(5):\n",
    "    print(\"step: {}, using dataset id: {}\".format(idx, interleave_data[idx][0]['ds_id']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ta01cpi'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom-Pattern Interleaving\n",
    "\n",
    "By default, `InterleaveDataset` switch dataset in a rotation. Sometimes people prefer a specific pattern of rotation. For example, 2 steps of dataset1 followed up 3 step of dataset2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, using dataset id: 0\n",
      "step: 1, using dataset id: 0\n",
      "step: 2, using dataset id: 1\n",
      "step: 3, using dataset id: 1\n",
      "step: 4, using dataset id: 1\n",
      "step: 5, using dataset id: 0\n"
     ]
    }
   ],
   "source": [
    "interleave_data = InterleaveDataset(datasets=[data1, data2], pattern=[0, 0, 1, 1, 1])\n",
    "for idx in range(6):\n",
    "    print(\"step: {}, using dataset id: {}\".format(idx, interleave_data[idx][0]['ds_id']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that when an interleaving pattern is defined, the length of the interleave dataset might shrink to guarantee full cycles. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ta01oc'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operator Control\n",
    "\n",
    "Since `InterleaveDataset` involves multiple data sources, then later in pipeline we might some operators condition on specific data source. For example, dataset1 might be a grey-scale image dataset, dataset2 might be colored image dataset. We may need a specific `ReadImage` Op for dataset1, and another `ReadImage` Op for dataset2.\n",
    "\n",
    "For such use cases, `InterleaveDataset` supports another input syntax below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, using dataset id: 0\n",
      "step: 1, using dataset id: 0\n",
      "step: 2, using dataset id: 1\n",
      "step: 3, using dataset id: 1\n",
      "step: 4, using dataset id: 1\n"
     ]
    }
   ],
   "source": [
    "interleave_data = InterleaveDataset(datasets={\"a\": data1, \"b\": data2}, pattern=[\"a\", \"a\", \"b\", \"b\", \"b\"])\n",
    "\n",
    "for idx in range(5):\n",
    "    print(\"step: {}, using dataset id: {}\".format(idx, interleave_data[idx][0]['ds_id']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once defined in the dictionary syntax, user can now plug in corresponding key name in Pipeline Operator's `ds_id` argument to condition operator on specific data source:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, dataset_id: 0, x: 7.5\n",
      "step: 1, dataset_id: 0, x: 8.5\n",
      "step: 2, dataset_id: 1, x: 6\n",
      "step: 3, dataset_id: 1, x: 8\n",
      "step: 4, dataset_id: 1, x: 9\n"
     ]
    }
   ],
   "source": [
    "import fastestimator as fe\n",
    "from fastestimator.op.numpyop import NumpyOp\n",
    "\n",
    "class PlusHalf(NumpyOp):\n",
    "    def forward(self, data, state):\n",
    "        return data + 0.5\n",
    "\n",
    "pipeline = fe.Pipeline(train_data=interleave_data, ops=[PlusHalf(inputs=\"x\", outputs=\"x\", ds_id=\"a\")])\n",
    "\n",
    "batches = pipeline.get_results(mode=\"train\", num_steps=5)\n",
    "for idx, batch in enumerate(batches):\n",
    "    print(\"step: {}, dataset_id: {}, x: {}\".format(idx, batch['ds_id'].item(), batch['x'].item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the `PlusHalf` is only activated for data1 (corresponding to key `a`), while the data2 (corresponding to key `b`) is left as is. Note that currently for InterleaveDataset, the operator conditioning is only available among Pipeline Operators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ta01bc'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Control\n",
    "\n",
    "Now that we know Pipeline Operators can be conditioned on a particular data source for `InterleaveDataset`, we can use different batch size for different data sources so that different data sources can fit in the GPU memory. This is done through the `Batch` Operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, dataset_id: [0 0], batch_size: 2\n",
      "step: 1, dataset_id: [1 1 1], batch_size: 3\n",
      "step: 2, dataset_id: [0 0], batch_size: 2\n",
      "step: 3, dataset_id: [1 1 1], batch_size: 3\n"
     ]
    }
   ],
   "source": [
    "from fastestimator.op.numpyop import Batch\n",
    "\n",
    "interleave_data = InterleaveDataset(datasets={\"a\": data1, \"b\": data2})\n",
    "\n",
    "\n",
    "pipeline = fe.Pipeline(train_data=interleave_data, \n",
    "                       ops=[Batch(batch_size=2, ds_id=\"a\"), \n",
    "                            Batch(batch_size=3, ds_id=\"b\")])\n",
    "\n",
    "batches = pipeline.get_results(mode=\"train\", num_steps=4)\n",
    "for idx, batch in enumerate(batches):\n",
    "    print(\"step: {}, dataset_id: {}, batch_size: {}\".format(idx, batch['ds_id'].numpy(), batch['ds_id'].size(0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ta01rae'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apphub Examples\n",
    "You can find some practical examples of the concepts described here in the following FastEstimator Apphubs:\n",
    "\n",
    "* [DNN](../../apphub/tabular/dnn/dnn.ipynb)\n",
    "* [CycleGAN](../../apphub/image_generation/cyclegan/cyclegan.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
