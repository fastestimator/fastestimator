{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2: Using In-Disk Data in FastEstimator\n",
    "\n",
    "In Tutorial 1, we introduced our 3 main APIs and general workflow of a deep learning task:  `Pipeline` -> `Network` -> `Estimator`. Then we used in-memory data for training. But what if the dataset size is too big to fit in memory? Say, data is in the size of ImageNet? \n",
    "\n",
    "The short answer is: user will use one more API for disk data: `RecordWriter`, such that the overall workflow becomes: `RecordWriter` -> `Pipeline` -> `Network` -> `Estimator`. In this tutorial, we are going to show you how to do in-disk data training in FastEstimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before we start:\n",
    "\n",
    "Two things are required regarding in-disk data : \n",
    "* Data files, obviously :)\n",
    "* A csv file that describes the data (prepare two csv files if you have a separate validation set)\n",
    "\n",
    "In the csv file, the rows of csv represent different examples and columns represent different features within example. For example, for a classification task, a csv may look like:\n",
    "\n",
    "| image  | label  |\n",
    "|---|---|\n",
    "|/data/image1.png   | 0  |\n",
    "|/data/image2.png   |  1 |\n",
    "|/data/image3.png | 0  |\n",
    "|... | .  |\n",
    "\n",
    "The csv of a multi-mask segmentation task may look like:\n",
    "\n",
    "| img  | msk1  | msk2  |\n",
    "|---|---|---|\n",
    "|/data/image1.png   | /maska/mask1.png  |/maskb/mask1.png|\n",
    "|/data/image2.png   |  /maska/mask2.png |/maskb/mask2.png|\n",
    "|/data/image3.png | /maska/mask3.png  |/maskb/mask3.png|\n",
    "|... | ...  |...|\n",
    "\n",
    "\n",
    "Please keep in mind that, there is no restriction on the data folder structures, number of features or name of features. Now, let's generate some in-disk data for this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastestimator.dataset.mnist import load_data\n",
    "\n",
    "train_csv, eval_csv, folder_path = load_data()\n",
    "print(\"training csv path is {}\".format(train_csv))\n",
    "print(\"evaluation csv path is {}\".format(eval_csv))\n",
    "print(\"mnist image path is {}\".format(folder_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the csv file and image file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_train = pd.read_csv(train_csv)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "img = plt.imread(os.path.join(folder_path, df_train[\"x\"][1]))\n",
    "plt.imshow(img)\n",
    "print(\"ground truth of image is {}\".format(df_train[\"y\"][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: RecordWriter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastestimator.record.preprocess import ImageReader\n",
    "import fastestimator as fe\n",
    "import tempfile\n",
    "\n",
    "writer = fe.RecordWriter(save_dir=os.path.join(folder_path, \"FEdata\"),\n",
    "                         train_data=train_csv,\n",
    "                         validation_data=eval_csv,\n",
    "                         ops=ImageReader(inputs=\"x\", parent_path=folder_path, outputs=\"x\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 and above: Pipeline -> Network -> Estimator, same routine as tutorial 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastestimator.pipeline.processing import Minmax\n",
    "from fastestimator.architecture import LeNet\n",
    "from fastestimator.network.model import FEModel\n",
    "from fastestimator.network.model import ModelOp\n",
    "from fastestimator.network.loss import SparseCategoricalCrossentropy\n",
    "\n",
    "pipeline = fe.Pipeline(batch_size=32, data=writer, ops=Minmax(inputs=\"x\", outputs=\"x\"))\n",
    "\n",
    "model = FEModel(model_def=LeNet, model_name=\"lenet\", optimizer=\"adam\", loss_name=\"loss\")\n",
    "network = fe.Network(ops=[ModelOp(inputs=\"x\", model=model, outputs=\"y_pred\"), \n",
    "                          SparseCategoricalCrossentropy(y_pred=\"y_pred\", y_true=\"y\", outputs=\"loss\")])\n",
    "estimator = fe.Estimator(network=network, pipeline=pipeline, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.ops_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
