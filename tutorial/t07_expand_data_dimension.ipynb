{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 7: Expanding data dimension in RecordWriter and Pipeline\n",
    "\n",
    "In many domains (especially medical imaging), multiple examples are sampled from the same data, we call it __data dimension expansion__. In this tutorial we will show you how to do data dimension expansion in FastEstimator.\n",
    "\n",
    "In general, expanding data dimension can be achieved in both `RecordWriter` and `Pipeline`. Here are the differences:\n",
    "\n",
    "* Expanding data in `RecordWriter` means occupying __more disk-space__, and the expanded samples are fixed during the training. The good thing is that it __saves preprocessing computation__ during training.\n",
    "\n",
    "* Expanding data in `Pipeline` means expanded data can be different for every batch during training, the computation is done __in-memory__ therefore __no extra disk space__ is required. The drawback is that __additional computation__ is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import fastestimator as fe\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - RecordWriter: expand data dimension and write it to the disk\n",
    "\n",
    "In RecordWriter, for each 28x28 image, let's create 4 different 27x27 images from corners:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastestimator.dataset.mnist import load_data\n",
    "from fastestimator.op import NumpyOp\n",
    "from fastestimator.util import RecordWriter\n",
    "from fastestimator.op.numpyop import ImageReader\n",
    "\n",
    "# Load Mnist data\n",
    "train_csv, eval_csv, data_path = load_data()\n",
    "\n",
    "# Create a custom Numpy op to sample 4 images from the corners of each image\n",
    "class SampleCorner(NumpyOp):\n",
    "    def forward(self, data, state):\n",
    "        x, y = data\n",
    "        # we sample 4 27x27 images from the corners:\n",
    "        x = np.array([x[:27, :27, :],\n",
    "                      x[1:, 1:, :],\n",
    "                      x[:27, 1:, :],\n",
    "                      x[1:, :27, :]])\n",
    "        y = np.array([y, y, y, y]) # the label does not change for each sampled image\n",
    "        return x, y\n",
    "\n",
    "# We insert this custom op in the ops list of RecordWriter.\n",
    "# We have to specify expand_dims=True to allow data dimension expansion.\n",
    "writer = RecordWriter(save_dir=os.path.join(data_path, \"FEdata_replicate\"),\n",
    "                         train_data=train_csv,\n",
    "                         validation_data=eval_csv,\n",
    "                         ops=[ImageReader(inputs=\"x\", outputs=\"x\", parent_path=data_path, grey_scale=True),\n",
    "                              SampleCorner(inputs=(\"x\", \"y\"), outputs=(\"x\", \"y\"))],\n",
    "                         expand_dims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Pipeline: expand dimension on the fly\n",
    "\n",
    "In the last step, we extracted 4 different 27x27 sub-images for each image. Now for each 27x27 image, let's randomly sub-sample two 20x20 images during the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastestimator.op.tensorop import Minmax\n",
    "from fastestimator.op import TensorOp\n",
    "import tensorflow as tf\n",
    "\n",
    "# We create a custom op for random sampling \n",
    "class RandomSample(TensorOp):\n",
    "    def forward(self, data, state):\n",
    "        x, y = data\n",
    "        # We randomly select the top-left point of our image for each sample (x and y coordinate)\n",
    "        # It cannot be greater than 8 as we will sample a 20x20 image from a 27x27 one\n",
    "        coord_x1_top_left = tf.random.uniform([], maxval=8, dtype=tf.int32)\n",
    "        coord_y1_top_left = tf.random.uniform([], maxval=8, dtype=tf.int32)\n",
    "        coord_x2_top_left = tf.random.uniform([], maxval=8, dtype=tf.int32)\n",
    "        coord_y2_top_left = tf.random.uniform([], maxval=8, dtype=tf.int32)\n",
    "        # We sample two 20x20 images with (x1,y1) and (x2,y2) top-left corner.\n",
    "        x = tf.stack([x[coord_x1_top_left:coord_x1_top_left+20, coord_y1_top_left:coord_y1_top_left+20, :], \n",
    "                      x[coord_x2_top_left:coord_x2_top_left+20, coord_y2_top_left:coord_y2_top_left+20, :]])\n",
    "        y = tf.stack([y, y]) # same label\n",
    "        return x, y\n",
    "\n",
    "# Create Pipeline with RandomSample op and expand_dims=True\n",
    "pipeline = fe.Pipeline(data=writer,\n",
    "                       batch_size=32,\n",
    "                       ops=[Minmax(inputs=\"x\", outputs=\"x\"),\n",
    "                            RandomSample(inputs=(\"x\", \"y\"), outputs=(\"x\", \"y\"))],\n",
    "                       expand_dims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Check pipeline results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastEstimator: Saving tfrecord to /tmp/.fe/Mnist/FEdata_replicate\n",
      "FastEstimator: Converting Train TFRecords 0.0%, Speed: 0.00 record/sec\n",
      "FastEstimator: Converting Train TFRecords 5.0%, Speed: 10469.19 record/sec\n",
      "FastEstimator: Converting Train TFRecords 10.0%, Speed: 10503.14 record/sec\n",
      "FastEstimator: Converting Train TFRecords 15.0%, Speed: 10543.56 record/sec\n",
      "FastEstimator: Converting Train TFRecords 20.0%, Speed: 10459.64 record/sec\n",
      "FastEstimator: Converting Train TFRecords 25.0%, Speed: 10502.02 record/sec\n",
      "FastEstimator: Converting Train TFRecords 30.0%, Speed: 10537.37 record/sec\n",
      "FastEstimator: Converting Train TFRecords 35.0%, Speed: 10582.11 record/sec\n",
      "FastEstimator: Converting Train TFRecords 40.0%, Speed: 10576.54 record/sec\n",
      "FastEstimator: Converting Train TFRecords 45.0%, Speed: 10580.37 record/sec\n",
      "FastEstimator: Converting Train TFRecords 50.0%, Speed: 10583.58 record/sec\n",
      "FastEstimator: Converting Train TFRecords 55.0%, Speed: 10575.13 record/sec\n",
      "FastEstimator: Converting Train TFRecords 60.0%, Speed: 10578.95 record/sec\n",
      "FastEstimator: Converting Train TFRecords 65.0%, Speed: 10547.38 record/sec\n",
      "FastEstimator: Converting Train TFRecords 70.0%, Speed: 10571.09 record/sec\n",
      "FastEstimator: Converting Train TFRecords 75.0%, Speed: 10564.39 record/sec\n",
      "FastEstimator: Converting Train TFRecords 80.0%, Speed: 10561.29 record/sec\n",
      "FastEstimator: Converting Train TFRecords 85.0%, Speed: 10539.61 record/sec\n",
      "FastEstimator: Converting Train TFRecords 90.0%, Speed: 10532.81 record/sec\n",
      "FastEstimator: Converting Train TFRecords 95.0%, Speed: 10514.78 record/sec\n",
      "FastEstimator: Converting Eval TFRecords 0.0%, Speed: 0.00 record/sec\n",
      "FastEstimator: Converting Eval TFRecords 5.0%, Speed: 9647.61 record/sec\n",
      "FastEstimator: Converting Eval TFRecords 9.9%, Speed: 10271.09 record/sec\n",
      "FastEstimator: Converting Eval TFRecords 14.9%, Speed: 10281.40 record/sec\n",
      "FastEstimator: Converting Eval TFRecords 19.8%, Speed: 10370.66 record/sec\n",
      "FastEstimator: Converting Eval TFRecords 24.8%, Speed: 10379.06 record/sec\n",
      "FastEstimator: Converting Eval TFRecords 29.8%, Speed: 10417.37 record/sec\n",
      "FastEstimator: Converting Eval TFRecords 34.7%, Speed: 10520.16 record/sec\n",
      "FastEstimator: Converting Eval TFRecords 39.7%, Speed: 10488.91 record/sec\n",
      "FastEstimator: Converting Eval TFRecords 44.6%, Speed: 10550.16 record/sec\n",
      "FastEstimator: Converting Eval TFRecords 49.6%, Speed: 10540.58 record/sec\n",
      "FastEstimator: Converting Eval TFRecords 54.6%, Speed: 10562.85 record/sec\n",
      "FastEstimator: Converting Eval TFRecords 59.5%, Speed: 10582.31 record/sec\n",
      "FastEstimator: Converting Eval TFRecords 64.5%, Speed: 10544.70 record/sec\n",
      "FastEstimator: Converting Eval TFRecords 69.4%, Speed: 10480.63 record/sec\n",
      "FastEstimator: Converting Eval TFRecords 74.4%, Speed: 10503.94 record/sec\n",
      "FastEstimator: Converting Eval TFRecords 79.4%, Speed: 10528.05 record/sec\n",
      "FastEstimator: Converting Eval TFRecords 84.3%, Speed: 10515.29 record/sec\n",
      "FastEstimator: Converting Eval TFRecords 89.3%, Speed: 10471.55 record/sec\n",
      "FastEstimator: Converting Eval TFRecords 94.2%, Speed: 10465.51 record/sec\n",
      "FastEstimator: Converting Eval TFRecords 99.2%, Speed: 10490.91 record/sec\n",
      "FastEstimator: Reading non-empty directory: /tmp/.fe/Mnist/FEdata_replicate\n",
      "FastEstimator: Found 240000 examples for train in /tmp/.fe/Mnist/FEdata_replicate/train_summary0.json\n",
      "FastEstimator: Found 40000 examples for eval in /tmp/.fe/Mnist/FEdata_replicate/eval_summary0.json\n",
      "shape of feature x is (32, 20, 20, 1)\n",
      "shape of feature y is (32,)\n"
     ]
    }
   ],
   "source": [
    "# Let's check our pipeline ops results with show_results\n",
    "\n",
    "result = pipeline.show_results()\n",
    "\n",
    "x = result[0][\"x\"]\n",
    "y = result[0][\"y\"]\n",
    "print(\"shape of feature x is {}\".format(x.shape))\n",
    "print(\"shape of feature y is {}\".format(y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .... and visualize!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground truth label is 2\n",
      "ground truth label is 2\n",
      "ground truth label is 5\n",
      "ground truth label is 5\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAABqCAYAAAClIwp2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAACUNJREFUeJzt3U+ITf8bwPHPTL+EUqxkQg3yp4iQMRTCShYWmFkQC2lQmvF3OzUppsbIbEgkTBl/NlhpsDGM8qcUC0Ua8if/UkZmM/Nd/U6f5+F+7syZe889z5n3a/V5+tx7z/GZ8XTnOZ/znLKBgQEHALCnvNQnAACIhwQOAEaRwAHAKBI4ABhFAgcAo0jgAGAUCRwAjCKBA4BR/0vyYGVlZdw1lMfAwEBZnPcNZ20nTJgQjWtra8XcwYMHRTx69Oicn9Pe3h6NT548KebevXsX9/QKJu7aOsfv7mCU4nd3pMi1tnwDBwCjSOAAYBQJHACMSrQGjnRYvHixiNva2qJxVVWVmBtKs7P9+/dHY11L37hxo4gfPXo06M9FOm3evLnUp5BZLS0tg3od38ABwCgSOAAYRQllBFq1apWIlyxZEutz3rx5I+Lp06dH44qKCjF37949ETc2Nkbj5ubmWMdH4U2ZMkXEuvS1adOmaFxdXZ3IOWWVLkHV19dH48GuLd/AAcAoEjgAGEUCBwCjqIHDlZWV/XP8Lz9//ozGK1asyPk5nZ2dYm727NkiPnr0aDR+8OCBmLt//36eM8ZwNDQ0iHgode2HDx9G45qaGjHX0dFRgLOzLXQNwV9n5/5ea39tp06dKuZ6enr+eTy+gQOAUSRwADCKBA4ARo24GngxWqc6J9unpqF1asiPHz9EHLpdvre3V8R1dXXR+OPHjznft2HDBhHrOrf/c6isrBRz1MCHLlR71TVv/dpQXVvXXru7u3Oew0ipgfvrma+u7fPX2bm/1/rKlStDPhe+gQOAUSRwADAq8yWUJDrvOSfLMWnvvHfhwgURf/36NRovX75czF26dEnEz58/H9QxFi1aJGK/ZIJ4/FuvQ7e4a1evXhVxa2trMB6J/LKSXttQCSpfWcQvQYXKT3HxDRwAjCKBA4BRJHAAMKpsKHXfYR+sBE+fPnDggIiPHTvmn4+YC61FqHWqfm9fX5+Y81unOhdun5qVJ3s/ffpUxPPnz8/52u3bt4v44sWLxTglE0+l97eSTZ48Wcz5W9R0XVvXYktR1077726hriH4a53UOvNUegDIGBI4ABhFAgcAozK/D1wrRutU52T71FDrVOfkbeVZum187ty50XjWrFliLrTWXV1dRTuntNO3T/u12H379om5ZcuWJXJOVum1HMo1BH+tLe2L5xs4ABhFAgcAozJfQkmi855zsvteqPOec7L7XrFKKP4TPdasWSPm5syZI+K7d+9G41evXok5vX0yZMuWLdFYd3LU6+7/CTuUY1inOwHq7WtW/5QvFb9sElpL57JZguIbOAAYRQIHAKNI4ABgVOZr4Em0TnVOtk8tRevUZ8+eidivgY8fPz74Xr817q9fv8Scrk/funUrGuvb5Q8dOhSN87VouH79enA+q1paWkSsn95E3TssdA1B17xHwlryDRwAjCKBA4BRme9GmBS/nBDqvOec7L6nO+/F7ejW398v1ta/8/Hx48fitdOmTROxX/IZSodGzX+vfp/ezunfqfnt27dBH2M40tCNMN96+p3uTpw4IebiPPQ2SUl0I9RrsHTp0mjslw2zhm6EAJAxJHAAMIoEDgBGUQOPye+855x88vyYMWOC750xY0Y01tv04tYRT506Jdb206dP0Vh3XtN17oULF0bjO3fuiLlt27aJ+MiRI3FOz7W3t4t469atsT5nONJQA/efCuOcc1VVVSL2t8mFnhKj6716O2IpJFEDD+Ur/VQia9cQQqiBA0DGkMABwCgSOAAYlYlb6XU90G+fmkTrVOdk+9RQ69ShHmewdu3aFfu9L168iMbjxo0Tc3ptQzXI0D7w2tranO/bu3eviPWe8SzRddhQXVbfNu7fht/T0yPmampqBv25lul/p38NQa9XR0dHzjiN1xDi4Bs4ABhFAgcAo1K7jXDmzJkiPn36dDTW3fX0n0P5uu/933A67+lueqF11OUDXVJRnxNrK1ZlZaU4gbdv3w76vf62xxs3boi51atXi7i/vz8af/nyRcz53Rx1SWfs2LE5j//+/XsRnzlzRsR//vzJeT7r1q3L+blaGrYRFkpDQ4OIjx8/LmL//0RS5YEkthEORagEpbdo+qWZNJaf2EYIABlDAgcAo0jgAGBUamrguialn6QzatSo0OeK2G+fmkTrVP3eUOtU58LtU+PWEevq6sTJ+9cMNP/pQc45d+7cuWg8b948fT4ibmpqisaNjY05j7Fy5UoRnz17VsSVlZXRON/P4ffv39FYP9EmdA5almrgWmjrqr59v4jnkKoaeEjoGkIatxhSAweAjCGBA4BRJHAAMCo1NfDXr1+L2K+ROudcX19fND5//ryY81unOifrf9Zap8atIw6oH6S/J1vvi1+/fn3Oz/n+/buI6+rqRHzt2rU4p/eXtWvXRuNJkyaJOX0Nwd+PPxxJ1cD1PmJ/zYazx9ivZdfX14u56upqEZdiX7OlGrjm//fR92kkdQ0hhBo4AGQMCRwAjEpNCeX27dsi9jsKOufc58+fo3FFRUVBzkd33mtubhbxzp07c743tPXNv93cOecuX74sYr/7ni4XFOOp9PpnrG+B7+rqisY7duwQc1nqDJhUCUV3CtS3dBeCfvqM/rO/tbW14MfMp1AlFL/ko0t2SZSg0tjZkRIKAGQMCRwAjCKBA4BRqamB61uvOzs7Rdzb2xuNFyxYIOZK3TrVOdk+NdQ61TnZPlW3Tm1qaopVR2xubhZr629dfPLkiXjt4cOHRfzy5cs4hzSnVLfSF2obml9b7+7uLshnFlKhauD+v7MY1w+cC19DKMX1g3yogQNAxpDAAcAoEjgAGJWaGrj24cMHEU+cODEa7969W8yVunWqc7KGH2qd6lx4j3Z5ebnZ25HTLsvtZNOgGLfSF/I29rRfQwihBg4AGUMCBwCjUltCaWtrE/GePXtyvlZv6fO776Wt855zsvuevlX95s2blFCKhBJKcVnuRph2lFAAIGNI4ABgFAkcAIxKtAYOACgcvoEDgFEkcAAwigQOAEaRwAHAKBI4ABhFAgcAo0jgAGAUCRwAjCKBA4BRJHAAMIoEDgBGkcABwCgSOAAYRQIHAKNI4ABgFAkcAIwigQOAUSRwADCKBA4ARpHAAcAoEjgAGEUCBwCjSOAAYNR/NiHIjBgNDVYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's visualize the first 4 images (keeping the order) from our post-pipeline data:\n",
    "fig, axes = plt.subplots(1, 4)\n",
    "for i in range(4):\n",
    "    axes[i].axis('off')\n",
    "    axes[i].imshow(np.squeeze(x[i]), cmap='gray')\n",
    "    print(\"ground truth label is {}\".format(y[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, each image is indeed generating two random patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
