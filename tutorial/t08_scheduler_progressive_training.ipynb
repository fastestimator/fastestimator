{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 8: Changing hyperparameters during training with Scheduler\n",
    "\n",
    "Before progressive training emerged, people had to use the same hyperparameters during the whole training. __Progressive training__ is essentially adding a time dimension in hyperparameters to allow any of them to change during the training loop. \n",
    "\n",
    "Examples of progressive training use cases:\n",
    "1. Use a batch size of 32 for the 0th epoch, then use 64 on the 5th epoch.\n",
    "2. Train with low resolution image (28x28) for the first 3 epochs, then double the resolution (52x52) for another 3 epochs.\n",
    "3. Train part of the model for the first 10 epochs, then train another part of the model for 10 more epochs.\n",
    "\n",
    "All of the examples above illustrate __hyperparameter change during the training__. In FastEstimator, `Scheduler` is used to handle these sort of requests. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) How to use Scheduler:\n",
    "\n",
    "Scheduler can be used in `Pipeline` and `Network`.  Before using Scheduler, user will need to create a dictionary where the key will be the epoch number, and the value whatever value the user wants to use from that epoch onwards. For example:\n",
    "\n",
    "```python\n",
    "from fastestimator.schedule import Scheduler\n",
    "\n",
    "mapping = {0: 32, 2:64, 5: 128}\n",
    "batch_scheduler = Scheduler(epoch_dict=mapping)\n",
    "```\n",
    "\n",
    "Then batch_scheduler can be used directly as batch size in `Pipeline`. Please note that the key in the dictionary indicates the epoch of change, therefore, in the example above, when the total training epoch is 8, the batch size for each epoch is:\n",
    "\n",
    "* epoch 0, batch size 32\n",
    "* epoch 1, batch size 32\n",
    "* epoch 2, batch size 64\n",
    "* epoch 3, batch size 64\n",
    "* epoch 4, batch size 64\n",
    "* epoch 5, batch size 128\n",
    "* epoch 6, batch size 128\n",
    "* epoch 7, batch size 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Scheduler example:\n",
    "\n",
    "In the next example, we'll define two image classification models with the same architecture(`model1` and `model2`). We want to train them by the following:\n",
    "\n",
    "* on epoch 0:  train `model1` with batch size 32, use image resolution 30x30 and Minmax normalization.\n",
    "* on epoch 1:  train `model2` with batch size 64, use image resolution 32x32 and Minmax normalization.\n",
    "* on epoch 2:  train `model1` with batch size 128, use image resolution 30x30 and Rescale normalization(multiply by 1/255)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0- Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import fastestimator as fe\n",
    "\n",
    "# We load MNIST dataset\n",
    "(x_train, y_train), (x_eval, y_eval) = tf.keras.datasets.mnist.load_data()\n",
    "train_data = {\"x\": np.expand_dims(x_train, -1), \"y\": y_train}\n",
    "eval_data = {\"x\": np.expand_dims(x_eval, -1), \"y\": y_eval}\n",
    "data = {\"train\": train_data, \"eval\": eval_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1- Prepare the Pipeline with the Schedulers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastestimator.schedule import Scheduler\n",
    "from fastestimator.op.tensorop import Minmax, Resize, Scale\n",
    "\n",
    "# We create a scheduler for batch_size with the epochs at which it will change and corresponding values.\n",
    "batchsize_scheduler = Scheduler({0:32, 1:64, 2:128})\n",
    "\n",
    "# We create a scheduler for the Resize ops.\n",
    "resize_scheduler = Scheduler({0: Resize(inputs=\"x\", size=(30, 30), outputs=\"x\"),\n",
    "                              1: Resize(inputs=\"x\", size=(32, 32), outputs=\"x\"),\n",
    "                              2: Resize(inputs=\"x\", size=(30, 30), outputs=\"x\")})\n",
    "\n",
    "# We create a scheduler for the different normalize ops we will want to use.\n",
    "normalize_scheduler = Scheduler({0: Minmax(inputs=\"x\", outputs=\"x\"),\n",
    "                                 2: Scale(inputs=\"x\", scalar=1.0/255, outputs=\"x\")})\n",
    "\n",
    "# In Pipeline, we use the schedulers for batch_size and ops.\n",
    "pipeline = fe.Pipeline(batch_size=batchsize_scheduler, \n",
    "                       data=data, \n",
    "                       ops=[resize_scheduler, normalize_scheduler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2- Prepare Network with the two models and a Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastestimator.architecture import LeNet\n",
    "from fastestimator.op.tensorop.model import ModelOp\n",
    "from fastestimator.op.tensorop.loss import SparseCategoricalCrossentropy\n",
    "\n",
    "# We create two models and build them with their optimizer and loss.\n",
    "model1 = fe.build(model_def=lambda: LeNet(input_shape=(30,30,1)), model_name=\"model1\", optimizer=\"adam\", loss_name='my_loss')\n",
    "model2 = fe.build(model_def=lambda: LeNet(input_shape=(32,32,1)), model_name=\"model2\", optimizer=\"adam\", loss_name='my_loss')\n",
    "\n",
    "# We create a Scheduler to indicate what model we want to train for each epoch.\n",
    "model_scheduler = Scheduler({0: ModelOp(inputs=\"x\", model=model1, outputs=\"y_pred\"),\n",
    "                             1: ModelOp(inputs=\"x\", model=model2, outputs=\"y_pred\"),\n",
    "                             2: ModelOp(inputs=\"x\", model=model1, outputs=\"y_pred\")})\n",
    "\n",
    "# We summarize the ops in Network, using model_scheduler for ModelOp.\n",
    "network = fe.Network(ops=[model_scheduler, SparseCategoricalCrossentropy(inputs=(\"y\", \"y_pred\"), outputs='my_loss')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3- Build the Estimator and train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = fe.Estimator(network=network, pipeline=pipeline, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ______           __  ______     __  _                 __            \n",
      "   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n",
      "  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n",
      " / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n",
      "/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n",
      "                                                                        \n",
      "\n",
      "FastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\n",
      "FastEstimator-Start: step: 0; total_train_steps: 3280; model1_lr: 0.001; model2_lr: 0.001; \n",
      "FastEstimator-Train: step: 0; my_loss: 2.3193202; \n",
      "FastEstimator-Train: step: 100; my_loss: 0.531848; examples/sec: 3623.1; progress: 3.0%; \n",
      "FastEstimator-Train: step: 200; my_loss: 0.1430535; examples/sec: 3647.4; progress: 6.1%; \n",
      "FastEstimator-Train: step: 300; my_loss: 0.2457301; examples/sec: 3670.5; progress: 9.1%; \n",
      "FastEstimator-Train: step: 400; my_loss: 0.0213109; examples/sec: 3672.2; progress: 12.2%; \n",
      "FastEstimator-Train: step: 500; my_loss: 0.0241418; examples/sec: 3670.4; progress: 15.2%; \n",
      "FastEstimator-Train: step: 600; my_loss: 0.2599721; examples/sec: 3656.0; progress: 18.3%; \n",
      "FastEstimator-Train: step: 700; my_loss: 0.0435993; examples/sec: 3674.1; progress: 21.3%; \n",
      "FastEstimator-Train: step: 800; my_loss: 0.0932014; examples/sec: 3676.2; progress: 24.4%; \n",
      "FastEstimator-Train: step: 900; my_loss: 0.1088856; examples/sec: 3671.1; progress: 27.4%; \n",
      "FastEstimator-Train: step: 1000; my_loss: 0.0201778; examples/sec: 3661.0; progress: 30.5%; \n",
      "FastEstimator-Train: step: 1100; my_loss: 0.0259394; examples/sec: 3665.9; progress: 33.5%; \n",
      "FastEstimator-Train: step: 1200; my_loss: 0.0384319; examples/sec: 3663.5; progress: 36.6%; \n",
      "FastEstimator-Train: step: 1300; my_loss: 0.0578654; examples/sec: 3677.3; progress: 39.6%; \n",
      "FastEstimator-Train: step: 1400; my_loss: 0.0041859; examples/sec: 3676.7; progress: 42.7%; \n",
      "FastEstimator-Train: step: 1500; my_loss: 0.0641367; examples/sec: 3669.6; progress: 45.7%; \n",
      "FastEstimator-Train: step: 1600; my_loss: 0.0384547; examples/sec: 3684.3; progress: 48.8%; \n",
      "FastEstimator-Train: step: 1700; my_loss: 0.0841084; examples/sec: 3675.7; progress: 51.8%; \n",
      "FastEstimator-Train: step: 1800; my_loss: 0.0110724; examples/sec: 3661.8; progress: 54.9%; \n",
      "FastEstimator-Eval: step: 1875; epoch: 0; my_loss: 0.0422232; min_my_loss: 0.042223167; since_best_loss: 0; \n",
      "FastEstimator-Train: step: 1900; my_loss: 0.607252; examples/sec: 3124.2; progress: 57.9%; \n",
      "FastEstimator-Train: step: 2000; my_loss: 0.1573484; examples/sec: 3977.2; progress: 61.0%; \n",
      "FastEstimator-Train: step: 2100; my_loss: 0.1285961; examples/sec: 3957.6; progress: 64.0%; \n",
      "FastEstimator-Train: step: 2200; my_loss: 0.1607811; examples/sec: 3966.4; progress: 67.1%; \n",
      "FastEstimator-Train: step: 2300; my_loss: 0.1570707; examples/sec: 3977.4; progress: 70.1%; \n",
      "FastEstimator-Train: step: 2400; my_loss: 0.1241959; examples/sec: 3968.7; progress: 73.2%; \n",
      "FastEstimator-Train: step: 2500; my_loss: 0.0067401; examples/sec: 3993.5; progress: 76.2%; \n",
      "FastEstimator-Train: step: 2600; my_loss: 0.0818045; examples/sec: 3962.0; progress: 79.3%; \n",
      "FastEstimator-Train: step: 2700; my_loss: 0.1402025; examples/sec: 3971.2; progress: 82.3%; \n",
      "FastEstimator-Train: step: 2800; my_loss: 0.0416286; examples/sec: 3962.1; progress: 85.4%; \n",
      "FastEstimator-Eval: step: 2812; epoch: 1; my_loss: 0.0546825; min_my_loss: 0.042223167; since_best_loss: 1; \n",
      "FastEstimator-Train: step: 2900; my_loss: 0.0508452; examples/sec: 4255.2; progress: 88.4%; \n",
      "FastEstimator-Train: step: 3000; my_loss: 0.0292688; examples/sec: 4765.9; progress: 91.5%; \n",
      "FastEstimator-Train: step: 3100; my_loss: 0.0403739; examples/sec: 4648.6; progress: 94.5%; \n",
      "FastEstimator-Train: step: 3200; my_loss: 0.0262227; examples/sec: 4607.7; progress: 97.6%; \n",
      "FastEstimator-Eval: step: 3280; epoch: 2; my_loss: 0.02717; min_my_loss: 0.02716998; since_best_loss: 0; \n",
      "FastEstimator-Finish: step: 3280; total_time: 48.75 sec; model1_lr: 0.001; model2_lr: 0.001; \n"
     ]
    }
   ],
   "source": [
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
