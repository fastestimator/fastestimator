{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1: Getting started with FastEstimator\n",
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In FastEstimator, there are 3 APIs that you need to know:\n",
    "\n",
    "* __Pipeline__: takes care of loading and preprocessing data.\n",
    "* __Network__: responsible for trainable and differentiable models, operations and loss.\n",
    "* __Estimator__: manages the training loop.\n",
    "\n",
    "Any deep learning implementation will follow the `Pipeline` -> `Network` -> `Estimator` process as illustrated below:<img src=\"image/workflow.png\">\n",
    "\n",
    "Pipeline operations will usually happen on CPU, whereas Network operations will happen on GPU when available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Prepare the Pipeline\n",
    "\n",
    "For in-memory data, `Pipeline` can take a nested **dictionary** like `{\"train\": {\"x\": x_train, \"y\": y_train}, \"eval\": {\"x\": x_eval, \"y\": y_eval}}`.  For both training and validation data, we link the images and labels to a key (respectively \"x\" and \"y\"). Note that the `eval` part is not needed, if validation data is not available.  \n",
    "We will explain `ops` arguments in more details in tutorial 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastestimator as fe\n",
    "from fastestimator.op.tensorop import Minmax\n",
    "\n",
    "# Import training and validation data as numpy array for instance\n",
    "(x_train, y_train), (x_eval, y_eval) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Add one channel dimension for convolution later\n",
    "x_train = np.expand_dims(x_train, -1) \n",
    "x_eval = np.expand_dims(x_eval, -1) \n",
    "\n",
    "# Create a dictionary to identify the training and evaluation data.\n",
    "# We specify for each x (images) and y (label) also in a dictionnary.\n",
    "data = {\"train\": {\"x\": x_train, \"y\": y_train}, \"eval\": {\"x\": x_eval, \"y\": y_eval}}\n",
    "\n",
    "# Creating the pipeline with the desired batch_size and preprocessing operation (here Minmax).\n",
    "pipeline = fe.Pipeline(batch_size=32, data=data, ops=Minmax(inputs=\"x\", outputs=\"x\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define the network\n",
    "\n",
    "For Network architecture, users can choose one of the following:\n",
    "* Define a custom network architecture using  `tf.keras.Model` or `tf.keras.Sequential`.  \n",
    "* Use an existing architecture provided by `tf.keras.applications` or `fe.architecture`. \n",
    "\n",
    "    In this tutorial, we are going to import a pre-defined LeNet architecture in [fastestimator.architecture.lenet](https://github.com/fastestimator/fastestimator/blob/master/fastestimator/architecture/lenet.py).\n",
    "\n",
    "Once the architecture is defined, we have to associate the model with its optimizer and expected loss name using `fe.build`.\n",
    "\n",
    "Finally, we create the Network to summarize all operations and loss. In this case, we have one ModelOp where we specify the input key, model we want to use, and output key.\n",
    "Network is also where we define the loss, here SparseCategoricalCrossentropy. We specify the key for predictions and groundtruths and the loss name as output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastestimator.architecture import LeNet\n",
    "from fastestimator.op.tensorop.model import ModelOp\n",
    "from fastestimator.op.tensorop.loss import SparseCategoricalCrossentropy\n",
    "\n",
    "# We first define a model, using FEModel to compile it.\n",
    "model = fe.build(model_def=LeNet, model_name=\"lenet\", optimizer=\"adam\", loss_name=\"loss\")\n",
    "\n",
    "# We summarize all operations and loss in the Network.\n",
    "network = fe.Network(ops=[ModelOp(inputs=\"x\", model=model, outputs=\"y_pred\"), \n",
    "                          SparseCategoricalCrossentropy(y_pred=\"y_pred\", y_true=\"y\", outputs=\"loss\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create the Estimator...\n",
    "\n",
    "`Estimator` takes both `pipeline` and `network` and combines them into the training loop. Here's the basic usage of `Estimator`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create the estimator and specify the number of epochs for training.\n",
    "estimator = fe.Estimator(network=network, pipeline=pipeline, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ... and train your model!\n",
    "\n",
    "Since the data is already loaded in memory, the training is happening without any disk reading. We only have to call the fit method to launch the training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "> ## Key take-aways:\n",
    ">\n",
    "> In order to implement your own ideas in FastEstimator, you first need to conceptually walk through the same thought process.\n",
    ">1. How do I want my data to be processed during the training? _Express this in `Pipeline`_.\n",
    ">2. How do I want my network architecture and loss to be defined? What are the connections between networks if there are multiple of them? _Express this in `Network`_.\n",
    ">3. How long do I want to train my model? What do I need during training loop? _Express this in `Estimator`_.\n",
    ">\n",
    ">It is important to note that the preprocessing defined in `Pipeline` will be executed repeatedly for each batch during the training loop. However, some preprocessing only needs to happen once up front (for instance resizing and rescaling). Next tutorial will introduce how to deal with this case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
