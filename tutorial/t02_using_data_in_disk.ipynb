{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2: Using In-Disk Data in FastEstimator\n",
    "\n",
    "In Tutorial 1, we introduced our 3 main APIs and general workflow of a deep learning task:  `Pipeline` -> `Network` -> `Estimator`. Then we used in-memory data for training. But what if the dataset size is too big to fit in memory? Say, data is in the size of ImageNet? \n",
    "\n",
    "The short answer is: user will use one more API for disk data: `RecordWriter`, such that the overall workflow becomes: `RecordWriter` -> `Pipeline` -> `Network` -> `Estimator`. In this tutorial, we are going to show you how to do in-disk data training in FastEstimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before we start:\n",
    "\n",
    "Two things are required regarding in-disk data : \n",
    "* Data files, obviously :)\n",
    "* A csv file that describes the data (prepare two csv files if you have a separate validation set)\n",
    "\n",
    "In the csv file, the rows of csv represent different examples and columns represent different features within example. For example, for a classification task, a csv may look like:\n",
    "\n",
    "| image  | label  |\n",
    "|---|---|\n",
    "|/data/image1.png   | 0  |\n",
    "|/data/image2.png   |  1 |\n",
    "|/data/image3.png | 0  |\n",
    "|... | .  |\n",
    "\n",
    "The csv of a multi-mask segmentation task may look like:\n",
    "\n",
    "| img  | msk1  | msk2  |\n",
    "|---|---|---|\n",
    "|/data/image1.png   | /maska/mask1.png  |/maskb/mask1.png|\n",
    "|/data/image2.png   |  /maska/mask2.png |/maskb/mask2.png|\n",
    "|/data/image3.png | /maska/mask3.png  |/maskb/mask3.png|\n",
    "|... | ...  |...|\n",
    "\n",
    "\n",
    "Please keep in mind that, there is no restriction on the data folder structures, number of features or name of features. Now, let's generate some in-disk data for this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastestimator.dataset.mnist import load_data\n",
    "\n",
    "train_csv, eval_csv, folder_path = load_data()\n",
    "print(\"training csv path is {}\".format(train_csv))\n",
    "print(\"evaluation csv path is {}\".format(eval_csv))\n",
    "print(\"mnist image path is {}\".format(folder_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the csv file and image file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_train = pd.read_csv(train_csv)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "img = plt.imread(os.path.join(folder_path, df_train[\"x\"][1]))\n",
    "plt.imshow(img)\n",
    "print(\"ground truth of image is {}\".format(df_train[\"y\"][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: RecordWriter\n",
    "\n",
    "\n",
    "In FastEstimator, we convert user's in-disk data to TFRecord for the best training speed. \n",
    "\n",
    "`save_dir` is required to specify the path to write the record. \n",
    "\n",
    "`train_data` can either be a csv path or a dictionary like the one used in tutorial 1. \n",
    "\n",
    "`validation_data` is optional and can take all input formats of `train_data`. In addition, `validation_data` can also take a floating point number between 0 to 1 which indicates the validation split ratio, then validation data will be randomly sampled from training data. \n",
    "\n",
    "Before converting data to TFRecord, users can apply a series of propcoessing tasks to the data in `ops` argument, we will talk about them in detail in tutorial 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastestimator.record.preprocess import ImageReader\n",
    "import fastestimator as fe\n",
    "import tempfile\n",
    "\n",
    "writer = fe.RecordWriter(save_dir=os.path.join(folder_path, \"FEdata\"),\n",
    "                         train_data=train_csv,\n",
    "                         validation_data=eval_csv,\n",
    "                         ops=ImageReader(inputs=\"x\", parent_path=folder_path, outputs=\"x\", grey_scale=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 and above: Pipeline -> Network -> Estimator, same as tutorial 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastestimator.pipeline.processing import Minmax\n",
    "from fastestimator.architecture import LeNet\n",
    "from fastestimator.network.model import FEModel\n",
    "from fastestimator.network.model import ModelOp\n",
    "from fastestimator.network.loss import SparseCategoricalCrossentropy\n",
    "\n",
    "pipeline = fe.Pipeline(batch_size=32, data=writer, ops=Minmax(inputs=\"x\", outputs=\"x\"))\n",
    "\n",
    "model = FEModel(model_def=LeNet, model_name=\"lenet\", optimizer=\"adam\", loss_name=\"loss\")\n",
    "network = fe.Network(ops=[ModelOp(inputs=\"x\", model=model, outputs=\"y_pred\"), \n",
    "                          SparseCategoricalCrossentropy(y_pred=\"y_pred\", y_true=\"y\", outputs=\"loss\")])\n",
    "estimator = fe.Estimator(network=network, pipeline=pipeline, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before we finish\n",
    "\n",
    "As mentioned in tutorial 1, the preprocessing in the `RecordWriter` is a place for \"once-for-all\" type preprocessing. If a preprocessing functions only needs to be done once (e,g, Resize and Rescale), then it is recommended to put them in `RecordWriter`. By doing this you can reduce the amount of computation needed during training and thereby train faster.\n",
    "\n",
    "Now, to summarize the conceptural workflow in FastEstimator for any deep learning task:\n",
    "0. Optional, but if I have in-disk data or want to apply some preprocessing once and for all  --> Express them in `RecordWriter`\n",
    "1. How do I want my data to be processed during the training, --> Express them in `Pipeline`\n",
    "2. How do I want my network architecture and loss to be defined, what are the connections between networks if there are multiple of them. --> Express them in `Network`\n",
    "3. How long do I train the model, what do I need during training loop --> Express them in `Estimator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
