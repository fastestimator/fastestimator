{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 9: Inference\n",
    "## Overview\n",
    "In this tutorial we are going to cover:\n",
    "* [Running inference with the transform method](#t09inference)\n",
    "    * [Pipeline.transform](#t09pipeline)\n",
    "    * [Network.transform](#t09network)\n",
    "* [Related Apphub Examples](#t09apphub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='t09inference'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running inference with transform method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running inference means using a trained deep learning model to get a prediction from some input data. Users can use `pipeline.transform` and `network.transform` to feed the data forward and get the computed result in any operation mode. Here we are going to use an end-to-end example (the same example code from [Tutorial 8](./t08_mode.ipynb)) on MNIST image classification to demonstrate how to run inference.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first train a deep leaning model with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ______           __  ______     __  _                 __            \n",
      "   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n",
      "  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n",
      " / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n",
      "/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n",
      "                                                                        \n",
      "\n",
      "FastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\n",
      "FastEstimator-Start: step: 1; num_device: 1; logging_interval: 100; \n",
      "FastEstimator-Train: step: 1; ce: 2.3060365; \n",
      "FastEstimator-Train: step: 100; ce: 1.3945999; steps/sec: 607.74; \n",
      "FastEstimator-Train: step: 200; ce: 1.2612264; steps/sec: 608.3; \n",
      "FastEstimator-Train: step: 300; ce: 1.0923076; steps/sec: 637.68; \n",
      "FastEstimator-Train: step: 400; ce: 1.0432141; steps/sec: 618.6; \n",
      "FastEstimator-Train: step: 500; ce: 1.1291115; steps/sec: 632.26; \n",
      "FastEstimator-Train: step: 600; ce: 1.1621585; steps/sec: 631.08; \n",
      "FastEstimator-Train: step: 700; ce: 0.8800679; steps/sec: 635.6; \n",
      "FastEstimator-Train: step: 800; ce: 0.881644; steps/sec: 626.44; \n",
      "FastEstimator-Train: step: 900; ce: 1.0101147; steps/sec: 634.27; \n",
      "FastEstimator-Train: step: 1000; ce: 0.6896187; steps/sec: 625.37; \n",
      "FastEstimator-Train: step: 1100; ce: 0.8749423; steps/sec: 622.53; \n",
      "FastEstimator-Train: step: 1200; ce: 0.92175865; steps/sec: 638.35; \n",
      "FastEstimator-Train: step: 1300; ce: 0.863528; steps/sec: 597.19; \n",
      "FastEstimator-Train: step: 1400; ce: 0.97089654; steps/sec: 564.34; \n",
      "FastEstimator-Train: step: 1500; ce: 0.80536413; steps/sec: 657.6; \n",
      "FastEstimator-Train: step: 1600; ce: 1.2070308; steps/sec: 624.45; \n",
      "FastEstimator-Train: step: 1700; ce: 0.7665318; steps/sec: 639.24; \n",
      "FastEstimator-Train: step: 1800; ce: 0.79776144; steps/sec: 629.49; \n",
      "FastEstimator-Train: step: 1875; epoch: 1; epoch_time: 3.58 sec; \n",
      "FastEstimator-Eval: step: 1875; epoch: 1; ce: 0.16841692; accuracy: 0.9488; \n",
      "FastEstimator-Finish: step: 1875; total_time: 5.2 sec; model_lr: 0.001; \n"
     ]
    }
   ],
   "source": [
    "import fastestimator as fe\n",
    "from fastestimator.dataset.data import mnist\n",
    "from fastestimator.trace.metric import Accuracy\n",
    "from fastestimator.op.numpyop.univariate import ExpandDims, Minmax, CoarseDropout\n",
    "from fastestimator.op.tensorop.loss import CrossEntropy\n",
    "from fastestimator.op.tensorop.model import ModelOp, UpdateOp\n",
    "from fastestimator.architecture.tensorflow import LeNet\n",
    "\n",
    "\n",
    "train_data, eval_data = mnist.load_data()\n",
    "test_data = eval_data.split(0.5)\n",
    "model = fe.build(model_fn=LeNet, optimizer_fn=\"adam\")\n",
    "\n",
    "pipeline = fe.Pipeline(train_data=train_data,\n",
    "                       eval_data=eval_data,\n",
    "                       test_data=test_data,\n",
    "                       batch_size=32,\n",
    "                       ops=[ExpandDims(inputs=\"x\", outputs=\"x\"), #default mode=None\n",
    "                            Minmax(inputs=\"x\", outputs=\"x_out\", mode=None),  \n",
    "                            CoarseDropout(inputs=\"x_out\", outputs=\"x_out\", mode=\"train\")])\n",
    "\n",
    "network = fe.Network(ops=[ModelOp(model=model, inputs=\"x_out\", outputs=\"y_pred\"), #default mode=None\n",
    "                          CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\", mode=\"!infer\"),\n",
    "                          UpdateOp(model=model, loss_name=\"ce\", mode=\"train\")])\n",
    "\n",
    "estimator = fe.Estimator(pipeline=pipeline,\n",
    "                         network=network,\n",
    "                         epochs=1,\n",
    "                         traces=Accuracy(true_key=\"y\", pred_key=\"y_pred\")) # default mode=[eval, test]\n",
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a customized print function to showcase our inferencing easier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def print_dict_but_value(data):\n",
    "    for key, value in data.items():\n",
    "        if isinstance(value, np.ndarray):\n",
    "            print(\"{}: ndarray with shape {}\".format(key, value.shape))\n",
    "        \n",
    "        elif isinstance(value, tf.Tensor):\n",
    "            print(\"{}: tf.Tensor with shape {}\".format(key, value.shape))\n",
    "        \n",
    "        else:\n",
    "            print(\"{}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following figure shows the complete execution graph (consisting `Pipeline` and `Network`) for the \"infer\" mode: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../resources/t09_infer_mode.PNG\" alt=\"drawing\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to provide an input image \"x\" and get the prediction result \"y_pred\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='t09pipeline'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline.transform\n",
    "The `Pipeline` object has a `transform` method that runs the pipeline graph (\"x\" to \"x_out\") when inference data (a dictionary of keys and values like {\"x\":image}), is inserted. The returned output will be the dictionary of computed results after applying all `Pipeline` Ops, where the dictionary values will be Numpy arrays.\n",
    "\n",
    "<img src=\"../resources/t09_infer_mode2.PNG\" alt=\"drawing\" width=\"700\"/>\n",
    "\n",
    "Here we take eval_data's first image, package it into a dictionary, and then call `pipeline.transform`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: ndarray with shape (28, 28)\n"
     ]
    }
   ],
   "source": [
    "import copy \n",
    "\n",
    "infer_data = {\"x\": copy.deepcopy(eval_data[0][\"x\"])}\n",
    "print_dict_but_value(infer_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: ndarray with shape (1, 28, 28, 1)\n",
      "x_out: ndarray with shape (1, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "infer_data = pipeline.transform(infer_data, mode=\"infer\")\n",
    "print_dict_but_value(infer_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='t09network'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network.transform\n",
    "\n",
    "We then use the network object to call the `transform` method that runs the network graph (\"x_out\" to \"y_pred\"). Much like with `pipeline.transform`, it will return it's Op outputs, though this time in the form of a dictionary of Tensors. The data type of the returned values depends on the backend of the network. It is `tf.Tensor` when using the TensorFlow backend and `torch.Tensor` with PyTorch. Please check out [Tutorial 6](./t06_network.ipynb) for more details about `Network` backends). \n",
    "\n",
    "<img src=\"../resources/t09_infer_mode3.PNG\" alt=\"drawing\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tf.Tensor with shape (1, 28, 28, 1)\n",
      "x_out: tf.Tensor with shape (1, 28, 28, 1)\n",
      "y_pred: tf.Tensor with shape (1, 10)\n"
     ]
    }
   ],
   "source": [
    "infer_data = network.transform(infer_data, mode=\"infer\")\n",
    "print_dict_but_value(infer_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can visualize the input image and compare with its prediction class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class is 2\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x240 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Predicted class is {}\".format(np.argmax(infer_data[\"y_pred\"])))\n",
    "img = fe.util.ImgData(x=infer_data[\"x\"])\n",
    "fig = img.paint_figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='t09apphub'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apphub Examples\n",
    "You can find some practical examples of the concepts described here in the following FastEstimator Apphubs:\n",
    "\n",
    "* [MNIST](../../apphub/image_classification/mnist/mnist.ipynb)\n",
    "* [IMDB](../../apphub/NLP/imdb/imdb.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
