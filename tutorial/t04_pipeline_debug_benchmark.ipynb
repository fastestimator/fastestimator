{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 4: Pipeline debugging and benchmarking\n",
    "\n",
    "When creating your own end-to-end deep learning workflow, things may go wrong anywhere along the way, it's very important to access the data after the pipeline and examine the correctness of pipeline operations. Furthermore, pipeline may be too slow to keep gpu busy, users may need to benchmark the pipeline speed to decide whether to move some operation to gpu.\n",
    "\n",
    "In this tutorial, we are going to show you how to access pipeline results and do pipeline benchmarking. We will use the same pipeline as tutorial 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define the pipeline (same as tutorial 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image data is generated in /var/folders/5g/d_ny7h211cj3zqkzrtq01s480000gn/T/.fe/Mnist\n"
     ]
    }
   ],
   "source": [
    "from fastestimator.pipeline.augmentation import Augmentation2D\n",
    "from fastestimator.util.op import TensorOp\n",
    "from fastestimator.dataset.mnist import load_data\n",
    "from fastestimator.util.op import NumpyOp\n",
    "from fastestimator.record.preprocess import ImageReader\n",
    "import fastestimator as fe\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "class Rescale(NumpyOp):\n",
    "    def forward(self, data, state):\n",
    "        data = (data - 127.5) / 127.5\n",
    "        return data\n",
    "\n",
    "class Resize(TensorOp):\n",
    "    def __init__(self, inputs, outputs, size):\n",
    "        super().__init__(inputs=inputs, outputs=outputs)\n",
    "        self.size = size\n",
    "    \n",
    "    def forward(self, data, state):\n",
    "        data = tf.image.resize(data, self.size)\n",
    "        return data\n",
    "\n",
    "train_csv, eval_csv, data_path = load_data()\n",
    "\n",
    "print(\"image data is generated in {}\".format(data_path))\n",
    "\n",
    "writer = fe.RecordWriter(save_dir=os.path.join(data_path, \"FEdata\"),\n",
    "                         train_data=train_csv,\n",
    "                         validation_data=eval_csv,\n",
    "                         ops=[ImageReader(inputs=\"x\", parent_path=data_path, grey_scale=True), \n",
    "                              Rescale(outputs=\"x\")])\n",
    "\n",
    "pipeline = fe.Pipeline(data=writer,\n",
    "                       batch_size=32,\n",
    "                       ops=[Resize(inputs=\"x\", size=(30, 30), outputs=\"x\"),\n",
    "                            Augmentation2D(outputs=\"x\", mode=\"train\", rotation_range=15)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## access the pipeline results\n",
    "\n",
    "`pipeline.show_results` is built for accessing the pipeline data, for example, if users want to access single batch of pipeline data on epoch 0 during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastEstimator: Reading non-empty directory: /var/folders/5g/d_ny7h211cj3zqkzrtq01s480000gn/T/.fe/Mnist/FEdata\n",
      "FastEstimator: Found 60000 examples for train in /var/folders/5g/d_ny7h211cj3zqkzrtq01s480000gn/T/.fe/Mnist/FEdata/train_summary0.json\n",
      "FastEstimator: Found 10000 examples for eval in /var/folders/5g/d_ny7h211cj3zqkzrtq01s480000gn/T/.fe/Mnist/FEdata/eval_summary0.json\n",
      "shape of feature x is (32, 30, 30, 1)\n",
      "shape of feature y is (32,)\n"
     ]
    }
   ],
   "source": [
    "result = pipeline.show_results(current_epoch=0, mode=\"train\", num_steps=1)\n",
    "\n",
    "x = result[0][\"x\"]\n",
    "y = result[0][\"y\"]\n",
    "print(\"shape of feature x is {}\".format(x.shape))\n",
    "print(\"shape of feature y is {}\".format(y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground truth label is 3\n",
      "ground truth label is 5\n",
      "ground truth label is 5\n",
      "ground truth label is 6\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 4)\n",
    "for i in range(4):\n",
    "    axes[i].axis('off')\n",
    "    axes[i].imshow(np.squeeze(x[i]), cmap='gray')\n",
    "    print(\"ground truth label is {}\".format(y[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## benchmark pipeline speed\n",
    "\n",
    "`pipeline.benchmark` can be used to benchmark the pipeline speed, for example, if users want to benchmark on epoch 0 during training for 2000 steps (batches)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastEstimator: Reading non-empty directory: /var/folders/5g/d_ny7h211cj3zqkzrtq01s480000gn/T/.fe/Mnist/FEdata\n",
      "FastEstimator: Found 60000 examples for train in /var/folders/5g/d_ny7h211cj3zqkzrtq01s480000gn/T/.fe/Mnist/FEdata/train_summary0.json\n",
      "FastEstimator: Found 10000 examples for eval in /var/folders/5g/d_ny7h211cj3zqkzrtq01s480000gn/T/.fe/Mnist/FEdata/eval_summary0.json\n",
      "FastEstimator: Step: 100, Epoch: 0, Batch Size 32, Example/sec 21609.71\n",
      "FastEstimator: Step: 200, Epoch: 0, Batch Size 32, Example/sec 20687.99\n",
      "FastEstimator: Step: 300, Epoch: 0, Batch Size 32, Example/sec 20598.51\n",
      "FastEstimator: Step: 400, Epoch: 0, Batch Size 32, Example/sec 20761.09\n",
      "FastEstimator: Step: 500, Epoch: 0, Batch Size 32, Example/sec 21411.01\n",
      "FastEstimator: Step: 600, Epoch: 0, Batch Size 32, Example/sec 21501.51\n",
      "FastEstimator: Step: 700, Epoch: 0, Batch Size 32, Example/sec 19981.62\n",
      "FastEstimator: Step: 800, Epoch: 0, Batch Size 32, Example/sec 21540.19\n",
      "FastEstimator: Step: 900, Epoch: 0, Batch Size 32, Example/sec 21277.43\n",
      "FastEstimator: Step: 1000, Epoch: 0, Batch Size 32, Example/sec 21895.33\n",
      "FastEstimator: Step: 1100, Epoch: 0, Batch Size 32, Example/sec 20462.84\n",
      "FastEstimator: Step: 1200, Epoch: 0, Batch Size 32, Example/sec 21390.37\n",
      "FastEstimator: Step: 1300, Epoch: 0, Batch Size 32, Example/sec 21021.64\n",
      "FastEstimator: Step: 1400, Epoch: 0, Batch Size 32, Example/sec 17913.87\n",
      "FastEstimator: Step: 1500, Epoch: 0, Batch Size 32, Example/sec 19595.48\n"
     ]
    }
   ],
   "source": [
    "pipeline.benchmark(current_epoch=0,mode=\"train\", num_steps=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
