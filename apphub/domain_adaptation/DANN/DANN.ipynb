{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Domain Adaptation by Backpropagation\n",
    "In this notebook, we will demonstrate how to implement Domain Adversarial Neural Network (DANN) as proposed in this [paper](https://arxiv.org/abs/1409.7495). In this notebook, we will adapt a digit classifier trained on MNIST digit dataset to USPS digit dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import fastestimator as fe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Defining Pipeline\n",
    "We will download the two datasets and then define ``Pipeline`` objects accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /root/fastestimator_data/USPS/zip.train.gz\n",
      "Extracting /root/fastestimator_data/USPS/zip.test.gz\n"
     ]
    }
   ],
   "source": [
    "from fastestimator.dataset import mnist, usps\n",
    "from fastestimator.op.numpyop import ImageReader\n",
    "from fastestimator import RecordWriter\n",
    "\n",
    "usps_train_csv, usps_eval_csv, usps_parent_dir = usps.load_data()\n",
    "mnist_train_csv, mnist_eval_csv, mnist_parent_dir = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "batch_size = 128\n",
    "epochs = 100 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset api creates a train csv file with each row containing a relative path to a image and the class label. Two train csv files will have the same column names. We need to change these column names to unique name for our purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(mnist_train_csv)\n",
    "df.columns = ['source_img', 'source_label']\n",
    "df.to_csv(mnist_train_csv, index=False)\n",
    "\n",
    "df = pd.read_csv(usps_train_csv)\n",
    "df.columns = ['target_img', 'target_label']\n",
    "df.to_csv(usps_train_csv, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the modified csv files, we can now create an input data pipeline that returns a batch from the MNIST dataset and the USPS dataset.\n",
    "\n",
    "#### Note that the input data pipeline created here is an unpaired dataset of the MNIST and the USPS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastestimator.op.tensorop import Resize, Minmax\n",
    "\n",
    "writer = RecordWriter(save_dir=os.path.join(os.path.dirname(mnist_parent_dir), 'dann', 'tfr'),\n",
    "                      train_data=(usps_train_csv, mnist_train_csv),\n",
    "                      ops=(\n",
    "                          [ImageReader(inputs=\"target_img\", outputs=\"target_img\", parent_path=usps_parent_dir, grey_scale=True)], # first tuple element\n",
    "                          [ImageReader(inputs=\"source_img\", outputs=\"source_img\", parent_path=mnist_parent_dir, grey_scale=True)])) # second tuple element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the following preprocessing to both datasets:\n",
    "\n",
    "* Resize of images to $28\\times28$\n",
    "* Minmax pixel value normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = fe.Pipeline(\n",
    "    batch_size=batch_size,\n",
    "    data=writer,\n",
    "    ops=[\n",
    "        Resize(inputs=\"target_img\", outputs=\"target_img\", size=(28, 28)),\n",
    "        Resize(inputs=\"source_img\", outputs=\"source_img\", size=(28, 28)),\n",
    "        Minmax(inputs=\"target_img\", outputs=\"target_img\"),\n",
    "        Minmax(inputs=\"source_img\", outputs=\"source_img\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Defining Network\n",
    "![DANN](./GRL.png)\n",
    "*Image Credit: [DANN Paper](https://arxiv.org/abs/1409.7495)*\n",
    "\n",
    "With ``Pipeline`` defined, we define the network architecture.\n",
    "The digit classification model is composed of the feature extraction network and the classifier network.\n",
    "In addition, the domain discriminator is attached to the output of the feature extraction network.\n",
    "\n",
    "The main idea is to train the feature extraction network to extract features that are invariant to domain shift.\n",
    "MNIST samples are used to train both classification branch (upper branch) and domain classification branch (lower branch). \n",
    "USPS samples are only used to train domain classification branch.\n",
    "Note that there is a gradient reversal layer between feature extraction network and the domain classification network. This layer helps the feature extraction network to be domain invariant by updating its parameters in the reverse direction of the gradient of domain classification loss.\n",
    "\n",
    "For stable training, the gradient of domain classification $\\frac{\\partial L_{d}}{\\partial \\theta_{d}}$ is scaled by a constant $\\lambda$ which smoothyl changes from 0 to 1 throughout the training. In our example, we define a tensor variable named ``alpha`` for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, Model\n",
    "from fastestimator.layers import GradReversal\n",
    "alpha = tf.Variable(0.0, dtype=tf.float32, trainable=False)\n",
    "img_shape=(28, 28, 1)\n",
    "feat_dim = 7 * 7 * 48\n",
    "\n",
    "def build_feature_extractor(img_shape=(28, 28, 1)):\n",
    "    x0 = layers.Input(shape=img_shape)\n",
    "    x = layers.Conv2D(32, 5, activation=\"relu\", padding=\"same\")(x0)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    x = layers.Conv2D(48, 5, activation=\"relu\", padding=\"same\")(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    feat_map = layers.Flatten()(x)\n",
    "    return Model(inputs=x0, outputs=feat_map)\n",
    "\n",
    "\n",
    "def build_label_predictor(feat_dim):\n",
    "    x0 = layers.Input(shape=(feat_dim,))\n",
    "    x = layers.Dense(100, activation=\"relu\")(x0)\n",
    "    x = layers.Dense(100, activation=\"relu\")(x)\n",
    "    return Model(inputs=x0, outputs=x)\n",
    "\n",
    "def build_domain_predictor(feat_dim):\n",
    "    x0 = layers.Input(shape=(feat_dim,))\n",
    "    x = GradReversal(l=alpha)(x0)\n",
    "    x = layers.Dense(100, activation=\"relu\")(x)\n",
    "    x = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    return Model(inputs=x0, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = fe.build(\n",
    "    model_def=lambda: build_feature_extractor(img_shape),\n",
    "    model_name=\"feature_extractor\",\n",
    "    loss_name=\"fe_loss\",\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-4)\n",
    ")\n",
    "\n",
    "label_predictor = fe.build(\n",
    "    model_def=lambda: build_label_predictor(feat_dim),\n",
    "    model_name=\"label_predictor\",\n",
    "    loss_name=\"fe_loss\",\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-4)\n",
    ")\n",
    "\n",
    "domain_predictor = fe.build(\n",
    "    model_def=lambda: build_domain_predictor(feat_dim),\n",
    "    model_name=\"domain_predictor\",\n",
    "    loss_name=\"fe_loss\",\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-4)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the loss for feature extraction network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastestimator.op.tensorop.loss import Loss, BinaryCrossentropy, SparseCategoricalCrossentropy\n",
    "from tensorflow.keras import losses\n",
    "\n",
    "class FELoss(Loss):\n",
    "    def __init__(self, inputs, outputs=None, mode=None):\n",
    "        super().__init__(inputs=inputs, outputs=outputs, mode=mode)        \n",
    "        self.label_loss_obj = losses.SparseCategoricalCrossentropy(reduction=losses.Reduction.NONE)\n",
    "        self.domain_loss_obj = losses.BinaryCrossentropy(reduction=losses.Reduction.NONE)        \n",
    "        \n",
    "    def forward(self, data, state):\n",
    "        src_c_logit, src_c_label, src_d_logit, tgt_d_logit = data\n",
    "        c_loss = self.label_loss_obj(y_true=src_c_label, y_pred=src_c_logit)\n",
    "        src_d_loss = self.domain_loss_obj(y_true=tf.zeros_like(src_d_logit), y_pred=src_d_logit) \n",
    "        tgt_d_loss = self.domain_loss_obj(y_true=tf.ones_like(tgt_d_logit), y_pred=tgt_d_logit)\n",
    "        return c_loss + src_d_loss + tgt_d_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the overall forward pass of the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastestimator.op.tensorop.model import ModelOp\n",
    "network = fe.Network(ops=[\n",
    "    ModelOp(inputs=\"source_img\", outputs=\"src_feat\", model=feature_extractor),\n",
    "    ModelOp(inputs=\"target_img\", outputs=\"tgt_feat\", model=feature_extractor),\n",
    "    ModelOp(inputs=\"src_feat\", outputs=\"src_c_logit\", model=label_predictor),\n",
    "    ModelOp(inputs=\"src_feat\", outputs=\"src_d_logit\", model=domain_predictor),\n",
    "    ModelOp(inputs=\"tgt_feat\", outputs=\"tgt_d_logit\", model=domain_predictor),\n",
    "    FELoss(inputs=(\"src_c_logit\",\"source_label\", \"src_d_logit\", \"tgt_d_logit\"), outputs=\"fe_loss\")    \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the [paper](https://arxiv.org/abs/1409.7495), the magnitude of the reversed gradient is smoothly changed from [0, 1] for stable training. \n",
    "We accomplish this by defining a trace to control the value of ``alpha`` defined previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastestimator.trace import Trace\n",
    "from tensorflow.python.keras import backend\n",
    "\n",
    "class GRLWeightController(Trace):\n",
    "    def __init__(self, alpha):\n",
    "        super().__init__(inputs=None, outputs=None, mode=\"train\")\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def on_begin(self, state):\n",
    "        self.total_steps = state['total_train_steps']\n",
    "        \n",
    "    def on_batch_begin(self, state):\n",
    "        p = state['train_step'] / self.total_steps\n",
    "        current_alpha = float(2.0 / (1.0 + np.exp(-10.0 * p)) - 1.0)\n",
    "        backend.set_value(self.alpha, current_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = [GRLWeightController(alpha=alpha)]\n",
    "\n",
    "estimator = fe.Estimator(\n",
    "    pipeline= pipeline, \n",
    "    network=network,\n",
    "    traces = traces,\n",
    "    epochs = epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Defining Estimator\n",
    "We put everything together in ``Estimator`` and start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ______           __  ______     __  _                 __            \n",
      "   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n",
      "  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n",
      " / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n",
      "/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n",
      "                                                                        \n",
      "\n",
      "FastEstimator: Reading non-empty directory: /root/fastestimator_data/dann/tfr\n",
      "FastEstimator: Found 60000 examples for train in /root/fastestimator_data/dann/tfr/train_summary1.json\n",
      "FastEstimator: Found 7291 examples for train in /root/fastestimator_data/dann/tfr/train_summary0.json\n",
      "FastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\n",
      "FastEstimator-Start: step: 0; total_train_steps: 5600; feature_extractor_lr: 1e-04; label_predictor_lr: 1e-04; domain_predictor_lr: 1e-04; \n",
      "FastEstimator-Train: step: 0; fe_loss: 9.094802; \n",
      "FastEstimator-Train: step: 100; fe_loss: 2.1193106; examples/sec: 6244.0; progress: 1.8%; \n",
      "FastEstimator-Train: step: 200; fe_loss: 1.310716; examples/sec: 5856.3; progress: 3.6%; \n",
      "FastEstimator-Train: step: 300; fe_loss: 4.0315123; examples/sec: 5953.2; progress: 5.4%; \n",
      "FastEstimator-Train: step: 400; fe_loss: 4.8160152; examples/sec: 5957.6; progress: 7.1%; \n",
      "FastEstimator-Train: step: 500; fe_loss: 4.3182516; examples/sec: 3154.3; progress: 8.9%; \n",
      "FastEstimator-Train: step: 600; fe_loss: 1.101692; examples/sec: 5987.9; progress: 10.7%; \n",
      "FastEstimator-Train: step: 700; fe_loss: 0.7090615; examples/sec: 5935.9; progress: 12.5%; \n",
      "FastEstimator-Train: step: 800; fe_loss: 1.9779288; examples/sec: 5957.6; progress: 14.3%; \n",
      "FastEstimator-Train: step: 900; fe_loss: 0.5050313; examples/sec: 6708.5; progress: 16.1%; \n",
      "FastEstimator-Train: step: 1000; fe_loss: 1.8742278; examples/sec: 2952.5; progress: 17.9%; \n",
      "FastEstimator-Train: step: 1100; fe_loss: 1.6311429; examples/sec: 5997.3; progress: 19.6%; \n",
      "FastEstimator-Train: step: 1200; fe_loss: 1.1044035; examples/sec: 5954.5; progress: 21.4%; \n",
      "FastEstimator-Train: step: 1300; fe_loss: 1.1994965; examples/sec: 6668.5; progress: 23.2%; \n",
      "FastEstimator-Train: step: 1400; fe_loss: 2.8527522; examples/sec: 5973.4; progress: 25.0%; \n",
      "FastEstimator-Train: step: 1500; fe_loss: 1.3480295; examples/sec: 2900.1; progress: 26.8%; \n",
      "FastEstimator-Train: step: 1600; fe_loss: 0.7022261; examples/sec: 5967.9; progress: 28.6%; \n",
      "FastEstimator-Train: step: 1700; fe_loss: 1.6039928; examples/sec: 6697.7; progress: 30.4%; \n",
      "FastEstimator-Train: step: 1800; fe_loss: 2.7394977; examples/sec: 5959.6; progress: 32.1%; \n",
      "FastEstimator-Train: step: 1900; fe_loss: 3.9869611; examples/sec: 2917.8; progress: 33.9%; \n",
      "FastEstimator-Train: step: 2000; fe_loss: 3.3304324; examples/sec: 5965.4; progress: 35.7%; \n",
      "FastEstimator-Train: step: 2100; fe_loss: 1.3063635; examples/sec: 6703.3; progress: 37.5%; \n",
      "FastEstimator-Train: step: 2200; fe_loss: 1.0937166; examples/sec: 5942.2; progress: 39.3%; \n",
      "FastEstimator-Train: step: 2300; fe_loss: 0.685876; examples/sec: 5973.6; progress: 41.1%; \n",
      "FastEstimator-Train: step: 2400; fe_loss: 0.7924838; examples/sec: 2984.2; progress: 42.9%; \n",
      "FastEstimator-Train: step: 2500; fe_loss: 0.8337267; examples/sec: 6632.3; progress: 44.6%; \n",
      "FastEstimator-Train: step: 2600; fe_loss: 1.270983; examples/sec: 5910.6; progress: 46.4%; \n",
      "FastEstimator-Train: step: 2700; fe_loss: 0.8676546; examples/sec: 5849.3; progress: 48.2%; \n",
      "FastEstimator-Train: step: 2800; fe_loss: 0.622426; examples/sec: 5923.0; progress: 50.0%; \n",
      "FastEstimator-Train: step: 2900; fe_loss: 0.6419143; examples/sec: 3060.3; progress: 51.8%; \n",
      "FastEstimator-Train: step: 3000; fe_loss: 0.4665622; examples/sec: 5997.9; progress: 53.6%; \n",
      "FastEstimator-Train: step: 3100; fe_loss: 0.5444981; examples/sec: 5896.7; progress: 55.4%; \n",
      "FastEstimator-Train: step: 3200; fe_loss: 1.4021776; examples/sec: 5945.7; progress: 57.1%; \n",
      "FastEstimator-Train: step: 3300; fe_loss: 1.8765414; examples/sec: 3119.7; progress: 58.9%; \n",
      "FastEstimator-Train: step: 3400; fe_loss: 1.1711186; examples/sec: 5994.0; progress: 60.7%; \n",
      "FastEstimator-Train: step: 3500; fe_loss: 0.6967485; examples/sec: 5957.7; progress: 62.5%; \n",
      "FastEstimator-Train: step: 3600; fe_loss: 1.6623905; examples/sec: 5975.7; progress: 64.3%; \n",
      "FastEstimator-Train: step: 3700; fe_loss: 1.829246; examples/sec: 6737.4; progress: 66.1%; \n",
      "FastEstimator-Train: step: 3800; fe_loss: 4.67502; examples/sec: 2578.8; progress: 67.9%; \n",
      "FastEstimator-Train: step: 3900; fe_loss: 1.9056684; examples/sec: 5922.9; progress: 69.6%; \n",
      "FastEstimator-Train: step: 4000; fe_loss: 0.6706764; examples/sec: 5931.7; progress: 71.4%; \n",
      "FastEstimator-Train: step: 4100; fe_loss: 0.5586882; examples/sec: 5864.6; progress: 73.2%; \n",
      "FastEstimator-Train: step: 4200; fe_loss: 0.3784848; examples/sec: 6618.0; progress: 75.0%; \n",
      "FastEstimator-Train: step: 4300; fe_loss: 0.7109174; examples/sec: 2955.0; progress: 76.8%; \n",
      "FastEstimator-Train: step: 4400; fe_loss: 1.2730641; examples/sec: 5956.8; progress: 78.6%; \n",
      "FastEstimator-Train: step: 4500; fe_loss: 1.1836125; examples/sec: 5958.2; progress: 80.4%; \n",
      "FastEstimator-Train: step: 4600; fe_loss: 0.4930265; examples/sec: 6680.5; progress: 82.1%; \n",
      "FastEstimator-Train: step: 4700; fe_loss: 0.654687; examples/sec: 2957.3; progress: 83.9%; \n",
      "FastEstimator-Train: step: 4800; fe_loss: 0.5251867; examples/sec: 5941.5; progress: 85.7%; \n",
      "FastEstimator-Train: step: 4900; fe_loss: 0.7086602; examples/sec: 5943.8; progress: 87.5%; \n",
      "FastEstimator-Train: step: 5000; fe_loss: 1.9798768; examples/sec: 6761.8; progress: 89.3%; \n",
      "FastEstimator-Train: step: 5100; fe_loss: 1.4147911; examples/sec: 5882.2; progress: 91.1%; \n",
      "FastEstimator-Train: step: 5200; fe_loss: 1.5251054; examples/sec: 2955.5; progress: 92.9%; \n",
      "FastEstimator-Train: step: 5300; fe_loss: 1.5515467; examples/sec: 5985.9; progress: 94.6%; \n",
      "FastEstimator-Train: step: 5400; fe_loss: 1.0991585; examples/sec: 6634.2; progress: 96.4%; \n",
      "FastEstimator-Train: step: 5500; fe_loss: 1.0645909; examples/sec: 5668.7; progress: 98.2%; \n",
      "FastEstimator-Finish: step: 5600; total_time: 144.02 sec; feature_extractor_lr: 1e-04; label_predictor_lr: 1e-04; domain_predictor_lr: 1e-04; \n"
     ]
    }
   ],
   "source": [
    "estimator.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
