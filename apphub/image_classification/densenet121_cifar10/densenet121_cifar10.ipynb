{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cifar10 Image Classification Using DenseNet-121\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import fastestimator as fe\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1: Create FastEstimator `Pipeline`\n",
    "### Load Dataset \n",
    "First, we load the training and evaluation dataset into memory use keras API.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train image shape is (50000, 32, 32, 3)\n",
      "train label shape is (50000, 1)\n",
      "eval image shape is (10000, 32, 32, 3)\n",
      "eval label shape is (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_eval, y_eval) = tf.keras.datasets.cifar10.load_data()\n",
    "print(\"train image shape is {}\".format(x_train.shape))\n",
    "print(\"train label shape is {}\".format(y_train.shape))\n",
    "print(\"eval image shape is {}\".format(x_eval.shape))\n",
    "print(\"eval label shape is {}\".format(y_eval.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define `Pipeline`\n",
    "`Pipeline` is the object that define how the training and evaulation data being ingested to the network.\n",
    "It has three basic arguments: \n",
    "* **batch_size**: (int) The batch size \n",
    "* **data**: (dict) the data source. It should be the nested dictionary like {\"mode1\": {\"feature1\": numpy_array, \"feature2\": numpy_array, ...}, ...} \n",
    "* **ops**: (list, obj) The list of pipeline processs block. For this example, we only use Minmax, so it can be the object.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastestimator.pipeline.processing import Minmax\n",
    "batch_size = 64\n",
    "data = {\"train\": {\"x\": x_train, \n",
    "                  \"y\": y_train}, \n",
    "        \"eval\": {\"x\": x_eval, \n",
    "                 \"y\": y_eval}}\n",
    "\n",
    "pipeline = fe.Pipeline(batch_size=batch_size, data=data, ops=Minmax(inputs=\"x\", outputs=\"x2\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate The Input Pipeline\n",
    "Once the pipeline was created, it is better to validate it with pipeline method, **show_results**, which will return a sample batch of pipeline data that give you a clue of how it works.  \n",
    "\n",
    "Because the pipeline has two different modes, \"train\" and \"eval\", we can take a looks of both examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of train image batch is (64, 32, 32, 3)\n",
      "the shape of train label batch is (64, 1)\n",
      "the shape of eval image batch is (64, 32, 32, 3)\n",
      "the shape of eval label batch is (64, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,2)\n",
    "train_sample = pipeline.show_results(mode=\"train\")\n",
    "print(\"the shape of train image batch is {}\".format(train_sample[0][\"x\"].numpy().shape))\n",
    "print(\"the shape of train label batch is {}\".format(train_sample[0][\"y\"].numpy().shape))\n",
    "ax[0].imshow(train_sample[0][\"x\"].numpy()[0])\n",
    "ax[0].set_title(\"the first image in train batch\")\n",
    "                \n",
    "eval_sample = pipeline.show_results(mode=\"eval\")\n",
    "print(\"the shape of eval image batch is {}\".format(eval_sample[0][\"x\"].numpy().shape))\n",
    "print(\"the shape of eval label batch is {}\".format(eval_sample[0][\"y\"].numpy().shape))\n",
    "ax[1].imshow(eval_sample[0][\"x\"].numpy()[0])\n",
    "ax[1].set_title(\"the first image in eval batch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate The Pipeline Output\n",
    "There are totally three keys in the pipeline \n",
    "1. \"y\": the label\n",
    "2. \"x\": the input image \n",
    "3. \"x2\": the processed output image.\n",
    "In the previous example we only validate the input image. We still need to validate the processed output image, since it will be the actual input data source for the network after all. <br/>\n",
    "\n",
    "The image process chain only has Minmax operation, which will map the minimum pixel value to 0 and maximum to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In train_sample[\"x\"] the max is 255, the min is 0\n",
      "In train_sample[\"x2\"] the max is 1.0, the min is 0.0\n",
      "In eval_sample[\"x\"] the max is 255, the min is 0\n",
      "In eval_sample[\"x2\"] the max is 1.0, the min is 0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"In train_sample[\\\"x\\\"] the max is {}, the min is {}\".format(np.max(train_sample[0][\"x\"].numpy()), np.min(train_sample[0][\"x\"].numpy())))\n",
    "print(\"In train_sample[\\\"x2\\\"] the max is {}, the min is {}\".format(np.max(train_sample[0][\"x2\"].numpy()), np.min(train_sample[0][\"x2\"].numpy())))\n",
    "print(\"In eval_sample[\\\"x\\\"] the max is {}, the min is {}\".format(np.max(eval_sample[0][\"x\"].numpy()), np.min(eval_sample[0][\"x\"].numpy())))\n",
    "print(\"In eval_sample[\\\"x2\\\"] the max is {}, the min is {}\".format(np.max(eval_sample[0][\"x2\"].numpy()), np.min(eval_sample[0][\"x2\"].numpy())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2: Create FastEstimator `Network` \n",
    "`Network` is the object that define the whole logic of neural network, including models, loss functions, optimizers ... etc. \n",
    "A Network can have several different models and loss funcitons (like GAN), but in this case, we are going to build a single model network.   \n",
    "\n",
    "### Define Keras Model Function\n",
    "The Model architecture of Fastestimator is defined by Tensorflow API (Keras). Here we used the pre-defined Keras function for building DensNet-121, and follow it by the custom layer to make it fit the Cifar10 dataset.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.densenet import DenseNet121 \n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "\n",
    "def DenseNet121_cifar10():\n",
    "    inputs = Input((32,32,3))\n",
    "    x = DenseNet121(weights=None, input_shape=(32,32,3), include_top=False, pooling='avg')(inputs)\n",
    "    outputs = Dense(10, activation='softmax')(x)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define `FEModel` from Keras\n",
    "Here We build `FEModel` from Keras model function. It has four arguments:\n",
    "* **model_def**: The model definition function.\n",
    "* **model_name**: The name of the model. It will be used when storing the model. \n",
    "* **optimizer**: The optimizer. It can either be str or tf.optimizers object. \n",
    "* **loss_name**: The name of the loss. Pleas be aware it is the dictionary key name and will be used in `Network` definition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastestimator.network.model import FEModel, ModelOp\n",
    "from fastestimator.network.loss import SparseCategoricalCrossentropy\n",
    "\n",
    "model = FEModel(model_def=DenseNet121_cifar10, \n",
    "                model_name=\"densenet121\", \n",
    "                optimizer=tf.optimizers.Adam(lr=0.1), \n",
    "                loss_name=\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define `Network` from `FEMode`\n",
    "So far we already have `FEmodel` and `Pipeline`, but how those networks connect to each other is still not defined yet.\n",
    "`Network` API is created for this reason. Its input argument is a list of operations each have IO \"keys\". By sharing the keys, those operations can connect in the way you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = fe.Network(ops=[\n",
    "    ModelOp(inputs=\"x2\", model=model, outputs=\"y_pred\"), \n",
    "    SparseCategoricalCrossentropy(y_true=\"y\", y_pred=\"y_pred\", outputs=\"loss\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network will connect like the following graph \n",
    "<img src=\"Capture.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create `Estimator` \n",
    "`Estimator` is the APi that wrap up the `Pipeline`, `Network` and other training metadata together.\n",
    "The `Estimator` basically have 4 arguments:\n",
    "* **pipeline**: the pipeline\n",
    "* **network** the network\n",
    "* **epoch** the epoch number of training\n",
    "* **traces** the list of `trace` object. They are pretty like the callbacks of Keras. The trace object will be called on specific timing during the training. Here we used **Accuracy** for getting model accuracy, **ModelSaver** for saving the best model checkpoint, and **LRController** for adapting learning rate. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastestimator.estimator.trace import Accuracy, ModelSaver, LRController, TensorBoard\n",
    "\n",
    "estimator = fe.Estimator(\n",
    "    network=network,\n",
    "    pipeline=pipeline,\n",
    "    epochs=50,\n",
    "    traces=[\n",
    "        Accuracy(true_key=\"y\", pred_key=\"y_pred\"),\n",
    "        ModelSaver(model_name=\"densenet121\", save_dir=\"./\", save_best=True),\n",
    "        LRController(model_name=\"densenet121\", reduce_on_eval=True)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training\n",
    "We use `Estimator` method **fit** to train the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ______           __  ______     __  _                 __            \n",
      "   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n",
      "  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n",
      " / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n",
      "/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n",
      "                                                                        \n",
      "\n",
      "FastEstimator-Start: step: 0; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 0; loss: 2.6384468; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 100; loss: 2.0457804; examples/sec: 299.76; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 200; loss: 2.1404605; examples/sec: 484.56; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 300; loss: 1.7883763; examples/sec: 484.34; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 400; loss: 1.9247652; examples/sec: 484.27; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 500; loss: 1.5740898; examples/sec: 484.33; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 600; loss: 1.5983531; examples/sec: 482.99; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 700; loss: 1.7560372; examples/sec: 480.66; densenet121_lr: 0.1; \n",
      "FastEstimator-ModelSaver: Saving model to ./densenet121_best_loss.h5\n",
      "FastEstimator-Eval: step: 781; epoch: 0; loss: 2.7472656; min_loss: 2.7472656; since_best_loss: 0; accuracy: 0.2642227564102564; \n",
      "FastEstimator-Train: step: 800; loss: 1.6253914; examples/sec: 481.09; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 900; loss: 1.5980914; examples/sec: 480.2; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 1000; loss: 1.4075153; examples/sec: 480.19; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 1100; loss: 1.5857433; examples/sec: 479.98; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 1200; loss: 1.5476265; examples/sec: 479.89; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 1300; loss: 1.4101198; examples/sec: 479.93; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 1400; loss: 1.3169982; examples/sec: 479.82; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 1500; loss: 1.3748422; examples/sec: 479.97; densenet121_lr: 0.1; \n",
      "FastEstimator-ModelSaver: Saving model to ./densenet121_best_loss.h5\n",
      "FastEstimator-Eval: step: 1562; epoch: 1; loss: 1.5730592; min_loss: 1.5730592; since_best_loss: 0; accuracy: 0.4684495192307692; \n",
      "FastEstimator-Train: step: 1600; loss: 1.4030312; examples/sec: 481.02; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 1700; loss: 1.2848508; examples/sec: 479.92; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 1800; loss: 1.1723138; examples/sec: 479.81; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 1900; loss: 1.0287985; examples/sec: 480.09; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 2000; loss: 1.3314971; examples/sec: 480.03; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 2100; loss: 1.1244555; examples/sec: 480.01; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 2200; loss: 1.0906973; examples/sec: 480.01; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 2300; loss: 1.1635484; examples/sec: 480.0; densenet121_lr: 0.1; \n",
      "FastEstimator-ModelSaver: Saving model to ./densenet121_best_loss.h5\n",
      "FastEstimator-Eval: step: 2343; epoch: 2; loss: 1.5615916; min_loss: 1.5615916; since_best_loss: 0; accuracy: 0.4558293269230769; \n",
      "FastEstimator-Train: step: 2400; loss: 1.0879171; examples/sec: 480.9; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 2500; loss: 1.1926311; examples/sec: 479.89; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 2600; loss: 0.96447664; examples/sec: 479.95; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 2700; loss: 1.1013131; examples/sec: 479.76; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 2800; loss: 0.9886874; examples/sec: 479.81; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 2900; loss: 1.2491612; examples/sec: 479.9; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 3000; loss: 1.3668263; examples/sec: 479.75; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 3100; loss: 1.2384803; examples/sec: 479.87; densenet121_lr: 0.1; \n",
      "FastEstimator-Eval: step: 3124; epoch: 3; loss: 5.6362586; min_loss: 1.5615916; since_best_loss: 1; accuracy: 0.3278245192307692; \n",
      "FastEstimator-Train: step: 3200; loss: 1.0805286; examples/sec: 480.93; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 3300; loss: 0.7691003; examples/sec: 479.23; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 3400; loss: 0.947104; examples/sec: 479.87; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 3500; loss: 1.0084258; examples/sec: 479.75; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 3600; loss: 1.1244035; examples/sec: 479.87; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 3700; loss: 0.97967; examples/sec: 479.8; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 3800; loss: 1.3613846; examples/sec: 479.61; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 3900; loss: 1.1249496; examples/sec: 479.58; densenet121_lr: 0.1; \n",
      "FastEstimator-Eval: step: 3905; epoch: 4; loss: 1.8262599; min_loss: 1.5615916; since_best_loss: 2; accuracy: 0.46083733974358976; \n",
      "FastEstimator-Train: step: 4000; loss: 0.86349237; examples/sec: 480.94; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 4100; loss: 1.2271874; examples/sec: 479.7; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 4200; loss: 1.3158662; examples/sec: 479.69; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 4300; loss: 1.0689255; examples/sec: 479.69; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 4400; loss: 0.98766613; examples/sec: 479.69; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 4500; loss: 0.8101882; examples/sec: 479.11; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 4600; loss: 0.946991; examples/sec: 479.42; densenet121_lr: 0.1; \n",
      "FastEstimator-ModelSaver: Saving model to ./densenet121_best_loss.h5\n",
      "FastEstimator-Eval: step: 4686; epoch: 5; loss: 1.4090807; min_loss: 1.4090807; since_best_loss: 0; accuracy: 0.5662059294871795; \n",
      "FastEstimator-Train: step: 4700; loss: 1.1347101; examples/sec: 480.75; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 4800; loss: 0.7943791; examples/sec: 479.73; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 4900; loss: 0.76035; examples/sec: 479.63; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 5000; loss: 0.87154436; examples/sec: 479.81; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 5100; loss: 0.7735913; examples/sec: 479.75; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 5200; loss: 1.271446; examples/sec: 479.55; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 5300; loss: 1.0884212; examples/sec: 479.56; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 5400; loss: 0.9528035; examples/sec: 479.61; densenet121_lr: 0.1; \n",
      "FastEstimator-ModelSaver: Saving model to ./densenet121_best_loss.h5\n",
      "FastEstimator-Eval: step: 5467; epoch: 6; loss: 1.0981696; min_loss: 1.0981696; since_best_loss: 0; accuracy: 0.6259014423076923; \n",
      "FastEstimator-Train: step: 5500; loss: 0.8799625; examples/sec: 480.97; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 5600; loss: 0.69600856; examples/sec: 479.77; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 5700; loss: 1.0476885; examples/sec: 479.57; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 5800; loss: 0.76492107; examples/sec: 479.66; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 5900; loss: 0.81845915; examples/sec: 479.67; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 6000; loss: 0.8298842; examples/sec: 479.45; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 6100; loss: 0.7821038; examples/sec: 479.66; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 6200; loss: 1.0393853; examples/sec: 479.63; densenet121_lr: 0.1; \n",
      "FastEstimator-Eval: step: 6248; epoch: 7; loss: 1.1600372; min_loss: 1.0981696; since_best_loss: 1; accuracy: 0.635917467948718; \n",
      "FastEstimator-Train: step: 6300; loss: 0.9653058; examples/sec: 480.7; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 6400; loss: 0.81487787; examples/sec: 479.49; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 6500; loss: 0.7175354; examples/sec: 479.49; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 6600; loss: 0.54200643; examples/sec: 479.63; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 6700; loss: 0.47776884; examples/sec: 479.46; densenet121_lr: 0.1; \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastEstimator-Train: step: 6800; loss: 0.91149163; examples/sec: 479.59; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 6900; loss: 0.96352965; examples/sec: 479.63; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 7000; loss: 0.54122734; examples/sec: 479.53; densenet121_lr: 0.1; \n",
      "FastEstimator-ModelSaver: Saving model to ./densenet121_best_loss.h5\n",
      "FastEstimator-Eval: step: 7029; epoch: 8; loss: 1.0158402; min_loss: 1.0158402; since_best_loss: 0; accuracy: 0.6761818910256411; \n",
      "FastEstimator-Train: step: 7100; loss: 0.50456727; examples/sec: 480.45; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 7200; loss: 0.62531084; examples/sec: 479.34; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 7300; loss: 0.7420913; examples/sec: 479.34; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 7400; loss: 0.7009349; examples/sec: 479.43; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 7500; loss: 0.8380206; examples/sec: 479.51; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 7600; loss: 0.8128431; examples/sec: 479.49; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 7700; loss: 0.7645513; examples/sec: 479.45; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 7800; loss: 0.62689865; examples/sec: 479.52; densenet121_lr: 0.1; \n",
      "FastEstimator-Eval: step: 7810; epoch: 9; loss: 1.2486163; min_loss: 1.0158402; since_best_loss: 1; accuracy: 0.6110777243589743; \n",
      "FastEstimator-Train: step: 7900; loss: 0.5275134; examples/sec: 480.67; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 8000; loss: 0.64331365; examples/sec: 479.5; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 8100; loss: 0.6958475; examples/sec: 479.41; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 8200; loss: 0.82315147; examples/sec: 479.68; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 8300; loss: 0.73965; examples/sec: 479.73; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 8400; loss: 0.44875503; examples/sec: 479.65; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 8500; loss: 0.82706624; examples/sec: 479.57; densenet121_lr: 0.1; \n",
      "FastEstimator-Eval: step: 8591; epoch: 10; loss: 1.31894; min_loss: 1.0158402; since_best_loss: 2; accuracy: 0.6370192307692307; \n",
      "FastEstimator-Train: step: 8600; loss: 0.5798354; examples/sec: 480.72; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 8700; loss: 0.53158754; examples/sec: 479.69; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 8800; loss: 0.8765549; examples/sec: 479.59; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 8900; loss: 0.50747496; examples/sec: 479.53; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 9000; loss: 0.54068625; examples/sec: 479.47; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 9100; loss: 0.63252777; examples/sec: 479.47; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 9200; loss: 0.786335; examples/sec: 479.63; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 9300; loss: 0.7185761; examples/sec: 479.55; densenet121_lr: 0.1; \n",
      "FastEstimator-ModelSaver: Saving model to ./densenet121_best_loss.h5\n",
      "FastEstimator-Eval: step: 9372; epoch: 11; loss: 0.9776936; min_loss: 0.9776936; since_best_loss: 0; accuracy: 0.6723758012820513; \n",
      "FastEstimator-Train: step: 9400; loss: 0.47382376; examples/sec: 480.88; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 9500; loss: 0.4102968; examples/sec: 479.65; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 9600; loss: 0.45774573; examples/sec: 479.55; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 9700; loss: 0.4479028; examples/sec: 479.67; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 9800; loss: 0.4970061; examples/sec: 479.65; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 9900; loss: 0.75718355; examples/sec: 479.62; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 10000; loss: 0.5131601; examples/sec: 479.64; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 10100; loss: 0.6933514; examples/sec: 479.59; densenet121_lr: 0.1; \n",
      "FastEstimator-ModelSaver: Saving model to ./densenet121_best_loss.h5\n",
      "FastEstimator-Eval: step: 10153; epoch: 12; loss: 0.95174855; min_loss: 0.95174855; since_best_loss: 0; accuracy: 0.715645032051282; \n",
      "FastEstimator-Train: step: 10200; loss: 0.6357097; examples/sec: 480.75; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 10300; loss: 0.6537467; examples/sec: 479.6; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 10400; loss: 0.69898015; examples/sec: 479.72; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 10500; loss: 0.5365771; examples/sec: 479.54; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 10600; loss: 0.64285314; examples/sec: 479.65; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 10700; loss: 0.4659403; examples/sec: 479.7; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 10800; loss: 0.6866198; examples/sec: 479.63; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 10900; loss: 0.78233206; examples/sec: 479.61; densenet121_lr: 0.1; \n",
      "FastEstimator-ModelSaver: Saving model to ./densenet121_best_loss.h5\n",
      "FastEstimator-Eval: step: 10934; epoch: 13; loss: 0.80561167; min_loss: 0.80561167; since_best_loss: 0; accuracy: 0.7512019230769231; \n",
      "FastEstimator-Train: step: 11000; loss: 0.30204958; examples/sec: 480.65; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 11100; loss: 0.5698671; examples/sec: 479.64; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 11200; loss: 0.4661543; examples/sec: 479.49; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 11300; loss: 0.5905831; examples/sec: 479.33; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 11400; loss: 0.85640734; examples/sec: 479.65; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 11500; loss: 0.8836372; examples/sec: 479.58; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 11600; loss: 0.8815084; examples/sec: 479.64; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 11700; loss: 0.6054491; examples/sec: 479.48; densenet121_lr: 0.1; \n",
      "FastEstimator-Eval: step: 11715; epoch: 14; loss: 3.6700184; min_loss: 0.80561167; since_best_loss: 1; accuracy: 0.5598958333333334; \n",
      "FastEstimator-Train: step: 11800; loss: 0.6583222; examples/sec: 480.65; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 11900; loss: 0.3972354; examples/sec: 479.71; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 12000; loss: 0.6146903; examples/sec: 479.61; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 12100; loss: 0.5313769; examples/sec: 479.71; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 12200; loss: 0.53103596; examples/sec: 479.67; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 12300; loss: 0.49105558; examples/sec: 479.74; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 12400; loss: 0.33615148; examples/sec: 479.18; densenet121_lr: 0.1; \n",
      "FastEstimator-Eval: step: 12496; epoch: 15; loss: 1.0987549; min_loss: 0.80561167; since_best_loss: 2; accuracy: 0.6838942307692307; \n",
      "FastEstimator-Train: step: 12500; loss: 0.40994036; examples/sec: 480.45; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 12600; loss: 0.4392055; examples/sec: 479.87; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 12700; loss: 0.2962895; examples/sec: 479.81; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 12800; loss: 0.36236888; examples/sec: 479.69; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 12900; loss: 0.4680061; examples/sec: 479.78; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 13000; loss: 0.48824662; examples/sec: 479.72; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 13100; loss: 0.5162629; examples/sec: 479.75; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 13200; loss: 0.5499758; examples/sec: 479.54; densenet121_lr: 0.1; \n",
      "FastEstimator-Eval: step: 13277; epoch: 16; loss: 1.0188146; min_loss: 0.80561167; since_best_loss: 3; accuracy: 0.6899038461538461; \n",
      "FastEstimator-Train: step: 13300; loss: 0.21783628; examples/sec: 480.89; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 13400; loss: 0.58859384; examples/sec: 479.73; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 13500; loss: 0.5863471; examples/sec: 479.67; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 13600; loss: 0.5193701; examples/sec: 479.76; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 13700; loss: 0.20826384; examples/sec: 479.91; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 13800; loss: 0.5317817; examples/sec: 479.84; densenet121_lr: 0.1; \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastEstimator-Train: step: 13900; loss: 0.38152578; examples/sec: 479.79; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 14000; loss: 0.57427394; examples/sec: 479.66; densenet121_lr: 0.1; \n",
      "FastEstimator-Eval: step: 14058; epoch: 17; loss: 1.0412898; min_loss: 0.80561167; since_best_loss: 4; accuracy: 0.6893028846153846; \n",
      "FastEstimator-Train: step: 14100; loss: 0.38318282; examples/sec: 480.9; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 14200; loss: 0.2567243; examples/sec: 479.8; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 14300; loss: 0.7241281; examples/sec: 479.8; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 14400; loss: 0.30224153; examples/sec: 479.62; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 14500; loss: 0.36791041; examples/sec: 479.61; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 14600; loss: 0.4310701; examples/sec: 479.71; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 14700; loss: 0.34545335; examples/sec: 479.69; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 14800; loss: 0.47383863; examples/sec: 479.8; densenet121_lr: 0.1; \n",
      "FastEstimator-Eval: step: 14839; epoch: 18; loss: 0.84784883; min_loss: 0.80561167; since_best_loss: 5; accuracy: 0.7514022435897436; \n",
      "FastEstimator-Train: step: 14900; loss: 0.20792498; examples/sec: 480.9; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 15000; loss: 0.3507716; examples/sec: 479.74; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 15100; loss: 0.4185434; examples/sec: 479.7; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 15200; loss: 0.54236937; examples/sec: 479.77; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 15300; loss: 0.547595; examples/sec: 479.72; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 15400; loss: 0.26143086; examples/sec: 479.73; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 15500; loss: 0.46928692; examples/sec: 479.75; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 15600; loss: 0.44399148; examples/sec: 479.84; densenet121_lr: 0.1; \n",
      "FastEstimator-Eval: step: 15620; epoch: 19; loss: 1.1539068; min_loss: 0.80561167; since_best_loss: 6; accuracy: 0.6998197115384616; \n",
      "FastEstimator-Train: step: 15700; loss: 0.3642046; examples/sec: 481.01; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 15800; loss: 0.35416976; examples/sec: 479.59; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 15900; loss: 0.44398662; examples/sec: 479.52; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 16000; loss: 0.5877776; examples/sec: 479.77; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 16100; loss: 0.48866075; examples/sec: 479.7; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 16200; loss: 0.20359674; examples/sec: 479.71; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 16300; loss: 0.28181925; examples/sec: 479.58; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 16400; loss: 0.51037776; examples/sec: 479.5; densenet121_lr: 0.1; \n",
      "FastEstimator-Eval: step: 16401; epoch: 20; loss: 1.1580251; min_loss: 0.80561167; since_best_loss: 7; accuracy: 0.7087339743589743; \n",
      "FastEstimator-Train: step: 16500; loss: 0.5829655; examples/sec: 479.25; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 16600; loss: 0.2335669; examples/sec: 479.15; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 16700; loss: 0.46507916; examples/sec: 479.24; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 16800; loss: 0.24412334; examples/sec: 479.19; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 16900; loss: 0.36468711; examples/sec: 479.3; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 17000; loss: 0.48712415; examples/sec: 479.85; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 17100; loss: 0.43866506; examples/sec: 480.03; densenet121_lr: 0.1; \n",
      "FastEstimator-Eval: step: 17182; epoch: 21; loss: 1.0183794; min_loss: 0.80561167; since_best_loss: 8; accuracy: 0.7403846153846154; \n",
      "FastEstimator-Train: step: 17200; loss: 0.08677483; examples/sec: 481.07; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 17300; loss: 0.33332717; examples/sec: 479.63; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 17400; loss: 0.24711493; examples/sec: 479.48; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 17500; loss: 0.30331075; examples/sec: 480.07; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 17600; loss: 0.30061197; examples/sec: 479.83; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 17700; loss: 0.18340155; examples/sec: 479.46; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 17800; loss: 0.5234656; examples/sec: 479.64; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 17900; loss: 0.2668103; examples/sec: 479.84; densenet121_lr: 0.1; \n",
      "FastEstimator-Eval: step: 17963; epoch: 22; loss: 1.0331002; min_loss: 0.80561167; since_best_loss: 9; accuracy: 0.7369791666666666; \n",
      "FastEstimator-Train: step: 18000; loss: 0.31449112; examples/sec: 480.73; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 18100; loss: 0.3809492; examples/sec: 479.66; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 18200; loss: 0.0871848; examples/sec: 479.12; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 18300; loss: 0.31541517; examples/sec: 479.6; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 18400; loss: 0.24880476; examples/sec: 479.49; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 18500; loss: 0.37014848; examples/sec: 479.38; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 18600; loss: 0.17091902; examples/sec: 479.55; densenet121_lr: 0.1; \n",
      "FastEstimator-Train: step: 18700; loss: 0.45668197; examples/sec: 479.46; densenet121_lr: 0.1; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 18744; epoch: 23; loss: 1.0609483; min_loss: 0.80561167; since_best_loss: 10; accuracy: 0.7158453525641025; \n",
      "FastEstimator-Train: step: 18800; loss: 0.1281841; examples/sec: 480.6; densenet121_lr: 0.01; \n",
      "FastEstimator-Train: step: 18900; loss: 0.1840939; examples/sec: 479.57; densenet121_lr: 0.01; \n",
      "FastEstimator-Train: step: 19000; loss: 0.030625343; examples/sec: 479.51; densenet121_lr: 0.01; \n",
      "FastEstimator-Train: step: 19100; loss: 0.15554391; examples/sec: 479.45; densenet121_lr: 0.01; \n",
      "FastEstimator-Train: step: 19200; loss: 0.22403772; examples/sec: 479.53; densenet121_lr: 0.01; \n",
      "FastEstimator-Train: step: 19300; loss: 0.26363885; examples/sec: 479.54; densenet121_lr: 0.01; \n",
      "FastEstimator-Train: step: 19400; loss: 0.096794106; examples/sec: 479.0; densenet121_lr: 0.01; \n",
      "FastEstimator-Train: step: 19500; loss: 0.10450378; examples/sec: 478.73; densenet121_lr: 0.01; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 19525; epoch: 24; loss: 0.80688286; min_loss: 0.80561167; since_best_loss: 11; accuracy: 0.7982772435897436; \n",
      "FastEstimator-Train: step: 19600; loss: 0.061826587; examples/sec: 480.66; densenet121_lr: 0.001; \n",
      "FastEstimator-Train: step: 19700; loss: 0.113582015; examples/sec: 479.97; densenet121_lr: 0.001; \n",
      "FastEstimator-Train: step: 19800; loss: 0.02079852; examples/sec: 479.98; densenet121_lr: 0.001; \n",
      "FastEstimator-Train: step: 19900; loss: 0.046183847; examples/sec: 479.96; densenet121_lr: 0.001; \n",
      "FastEstimator-Train: step: 20000; loss: 0.2253827; examples/sec: 479.87; densenet121_lr: 0.001; \n",
      "FastEstimator-Train: step: 20100; loss: 0.06731348; examples/sec: 479.73; densenet121_lr: 0.001; \n",
      "FastEstimator-Train: step: 20200; loss: 0.10147763; examples/sec: 479.68; densenet121_lr: 0.001; \n",
      "FastEstimator-Train: step: 20300; loss: 0.108633704; examples/sec: 479.66; densenet121_lr: 0.001; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 20306; epoch: 25; loss: 0.82493496; min_loss: 0.80561167; since_best_loss: 12; accuracy: 0.7995793269230769; \n",
      "FastEstimator-Train: step: 20400; loss: 0.041979797; examples/sec: 481.1; densenet121_lr: 0.0001; \n",
      "FastEstimator-Train: step: 20500; loss: 0.0681745; examples/sec: 480.06; densenet121_lr: 0.0001; \n",
      "FastEstimator-Train: step: 20600; loss: 0.03196047; examples/sec: 479.93; densenet121_lr: 0.0001; \n",
      "FastEstimator-Train: step: 20700; loss: 0.016953945; examples/sec: 479.9; densenet121_lr: 0.0001; \n",
      "FastEstimator-Train: step: 20800; loss: 0.05020147; examples/sec: 479.8; densenet121_lr: 0.0001; \n",
      "FastEstimator-Train: step: 20900; loss: 0.026455965; examples/sec: 479.85; densenet121_lr: 0.0001; \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastEstimator-Train: step: 21000; loss: 0.023690222; examples/sec: 480.07; densenet121_lr: 0.0001; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 21087; epoch: 26; loss: 0.8345152; min_loss: 0.80561167; since_best_loss: 13; accuracy: 0.7996794871794872; \n",
      "FastEstimator-Train: step: 21100; loss: 0.15004686; examples/sec: 480.86; densenet121_lr: 1e-05; \n",
      "FastEstimator-Train: step: 21200; loss: 0.103584334; examples/sec: 479.74; densenet121_lr: 1e-05; \n",
      "FastEstimator-Train: step: 21300; loss: 0.030101815; examples/sec: 479.77; densenet121_lr: 1e-05; \n",
      "FastEstimator-Train: step: 21400; loss: 0.032056473; examples/sec: 479.88; densenet121_lr: 1e-05; \n",
      "FastEstimator-Train: step: 21500; loss: 0.17481083; examples/sec: 479.26; densenet121_lr: 1e-05; \n",
      "FastEstimator-Train: step: 21600; loss: 0.16798462; examples/sec: 479.82; densenet121_lr: 1e-05; \n",
      "FastEstimator-Train: step: 21700; loss: 0.11887287; examples/sec: 479.9; densenet121_lr: 1e-05; \n",
      "FastEstimator-Train: step: 21800; loss: 0.12781353; examples/sec: 479.84; densenet121_lr: 1e-05; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 21868; epoch: 27; loss: 0.8255368; min_loss: 0.80561167; since_best_loss: 14; accuracy: 0.8009815705128205; \n",
      "FastEstimator-Train: step: 21900; loss: 0.044315085; examples/sec: 480.94; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 22000; loss: 0.1359852; examples/sec: 479.8; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 22100; loss: 0.039628174; examples/sec: 479.85; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 22200; loss: 0.04402913; examples/sec: 479.86; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 22300; loss: 0.028211392; examples/sec: 479.96; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 22400; loss: 0.10297183; examples/sec: 479.89; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 22500; loss: 0.1277656; examples/sec: 480.01; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 22600; loss: 0.017516926; examples/sec: 479.83; densenet121_lr: 1e-06; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 22649; epoch: 28; loss: 0.8168064; min_loss: 0.80561167; since_best_loss: 15; accuracy: 0.8000801282051282; \n",
      "FastEstimator-Train: step: 22700; loss: 0.099011265; examples/sec: 480.87; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 22800; loss: 0.067407675; examples/sec: 479.94; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 22900; loss: 0.09831661; examples/sec: 479.92; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 23000; loss: 0.10931054; examples/sec: 480.09; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 23100; loss: 0.054510206; examples/sec: 479.92; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 23200; loss: 0.096128814; examples/sec: 479.72; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 23300; loss: 0.08763911; examples/sec: 479.85; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 23400; loss: 0.17356558; examples/sec: 479.98; densenet121_lr: 1e-06; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 23430; epoch: 29; loss: 0.8331159; min_loss: 0.80561167; since_best_loss: 16; accuracy: 0.7987780448717948; \n",
      "FastEstimator-Train: step: 23500; loss: 0.07141274; examples/sec: 481.1; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 23600; loss: 0.11900911; examples/sec: 479.94; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 23700; loss: 0.07838875; examples/sec: 479.94; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 23800; loss: 0.11471763; examples/sec: 480.02; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 23900; loss: 0.036268514; examples/sec: 480.04; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 24000; loss: 0.11476267; examples/sec: 480.32; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 24100; loss: 0.1368064; examples/sec: 480.22; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 24200; loss: 0.12221098; examples/sec: 479.53; densenet121_lr: 1e-06; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 24211; epoch: 30; loss: 0.8340764; min_loss: 0.80561167; since_best_loss: 17; accuracy: 0.7981770833333334; \n",
      "FastEstimator-Train: step: 24300; loss: 0.10665418; examples/sec: 480.59; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 24400; loss: 0.039753966; examples/sec: 479.5; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 24500; loss: 0.0987713; examples/sec: 479.88; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 24600; loss: 0.056945764; examples/sec: 479.87; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 24700; loss: 0.039676953; examples/sec: 479.85; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 24800; loss: 0.064659424; examples/sec: 479.91; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 24900; loss: 0.06891568; examples/sec: 480.05; densenet121_lr: 1e-06; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 24992; epoch: 31; loss: 0.8289789; min_loss: 0.80561167; since_best_loss: 18; accuracy: 0.7996794871794872; \n",
      "FastEstimator-Train: step: 25000; loss: 0.10039259; examples/sec: 480.95; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 25100; loss: 0.073335506; examples/sec: 479.91; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 25200; loss: 0.04698392; examples/sec: 480.01; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 25300; loss: 0.06150177; examples/sec: 479.8; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 25400; loss: 0.016139532; examples/sec: 480.11; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 25500; loss: 0.03422602; examples/sec: 480.26; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 25600; loss: 0.06675476; examples/sec: 480.08; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 25700; loss: 0.06252436; examples/sec: 480.12; densenet121_lr: 1e-06; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 25773; epoch: 32; loss: 0.8351396; min_loss: 0.80561167; since_best_loss: 19; accuracy: 0.7980769230769231; \n",
      "FastEstimator-Train: step: 25800; loss: 0.047444195; examples/sec: 480.81; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 25900; loss: 0.13923952; examples/sec: 480.1; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 26000; loss: 0.08354418; examples/sec: 480.29; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 26100; loss: 0.060797; examples/sec: 480.2; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 26200; loss: 0.040465273; examples/sec: 480.24; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 26300; loss: 0.06088505; examples/sec: 480.33; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 26400; loss: 0.104599595; examples/sec: 479.87; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 26500; loss: 0.06578925; examples/sec: 479.73; densenet121_lr: 1e-06; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 26554; epoch: 33; loss: 0.8319974; min_loss: 0.80561167; since_best_loss: 20; accuracy: 0.8004807692307693; \n",
      "FastEstimator-Train: step: 26600; loss: 0.08788578; examples/sec: 480.57; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 26700; loss: 0.09941806; examples/sec: 479.74; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 26800; loss: 0.13603939; examples/sec: 479.78; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 26900; loss: 0.038917035; examples/sec: 479.6; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 27000; loss: 0.16515715; examples/sec: 479.73; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 27100; loss: 0.055594906; examples/sec: 479.7; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 27200; loss: 0.13560484; examples/sec: 479.74; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 27300; loss: 0.08119285; examples/sec: 479.77; densenet121_lr: 1e-06; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 27335; epoch: 34; loss: 0.8279419; min_loss: 0.80561167; since_best_loss: 21; accuracy: 0.7991786858974359; \n",
      "FastEstimator-Train: step: 27400; loss: 0.070663415; examples/sec: 480.81; densenet121_lr: 1e-06; \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastEstimator-Train: step: 27500; loss: 0.029532574; examples/sec: 479.8; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 27600; loss: 0.03168583; examples/sec: 479.65; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 27700; loss: 0.09797203; examples/sec: 479.94; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 27800; loss: 0.29340243; examples/sec: 479.86; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 27900; loss: 0.071743816; examples/sec: 479.88; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 28000; loss: 0.122900486; examples/sec: 479.89; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 28100; loss: 0.020515941; examples/sec: 479.71; densenet121_lr: 1e-06; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 28116; epoch: 35; loss: 0.82110703; min_loss: 0.80561167; since_best_loss: 22; accuracy: 0.8008814102564102; \n",
      "FastEstimator-Train: step: 28200; loss: 0.02759188; examples/sec: 480.82; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 28300; loss: 0.19965929; examples/sec: 479.77; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 28400; loss: 0.0425519; examples/sec: 479.74; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 28500; loss: 0.14489909; examples/sec: 479.79; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 28600; loss: 0.103878334; examples/sec: 479.53; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 28700; loss: 0.07015334; examples/sec: 479.29; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 28800; loss: 0.06476031; examples/sec: 479.77; densenet121_lr: 1e-06; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 28897; epoch: 36; loss: 0.83967346; min_loss: 0.80561167; since_best_loss: 23; accuracy: 0.7970753205128205; \n",
      "FastEstimator-Train: step: 28900; loss: 0.042699464; examples/sec: 480.94; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 29000; loss: 0.07236235; examples/sec: 479.85; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 29100; loss: 0.09955698; examples/sec: 479.74; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 29200; loss: 0.12156765; examples/sec: 479.84; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 29300; loss: 0.084004015; examples/sec: 479.8; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 29400; loss: 0.02060385; examples/sec: 479.78; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 29500; loss: 0.1793328; examples/sec: 479.86; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 29600; loss: 0.03767272; examples/sec: 479.74; densenet121_lr: 1e-06; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 29678; epoch: 37; loss: 0.82823485; min_loss: 0.80561167; since_best_loss: 24; accuracy: 0.7989783653846154; \n",
      "FastEstimator-Train: step: 29700; loss: 0.1897915; examples/sec: 480.72; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 29800; loss: 0.045536503; examples/sec: 479.8; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 29900; loss: 0.053405583; examples/sec: 479.81; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 30000; loss: 0.1555985; examples/sec: 479.84; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 30100; loss: 0.11638557; examples/sec: 479.73; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 30200; loss: 0.04874228; examples/sec: 479.89; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 30300; loss: 0.12095846; examples/sec: 480.01; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 30400; loss: 0.1409027; examples/sec: 479.85; densenet121_lr: 1e-06; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 30459; epoch: 38; loss: 0.82953626; min_loss: 0.80561167; since_best_loss: 25; accuracy: 0.8009815705128205; \n",
      "FastEstimator-Train: step: 30500; loss: 0.022157028; examples/sec: 480.86; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 30600; loss: 0.0440033; examples/sec: 479.89; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 30700; loss: 0.09802669; examples/sec: 479.74; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 30800; loss: 0.10327478; examples/sec: 479.82; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 30900; loss: 0.050125677; examples/sec: 479.96; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 31000; loss: 0.070754774; examples/sec: 479.83; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 31100; loss: 0.05068633; examples/sec: 480.01; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 31200; loss: 0.018590141; examples/sec: 479.81; densenet121_lr: 1e-06; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 31240; epoch: 39; loss: 0.82051975; min_loss: 0.80561167; since_best_loss: 26; accuracy: 0.7989783653846154; \n",
      "FastEstimator-Train: step: 31300; loss: 0.15145807; examples/sec: 480.12; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 31400; loss: 0.04999852; examples/sec: 479.28; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 31500; loss: 0.11128628; examples/sec: 479.52; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 31600; loss: 0.049058635; examples/sec: 479.56; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 31700; loss: 0.16335739; examples/sec: 478.91; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 31800; loss: 0.14008984; examples/sec: 479.52; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 31900; loss: 0.13188583; examples/sec: 479.61; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 32000; loss: 0.05310629; examples/sec: 479.82; densenet121_lr: 1e-06; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 32021; epoch: 40; loss: 0.82793957; min_loss: 0.80561167; since_best_loss: 27; accuracy: 0.7991786858974359; \n",
      "FastEstimator-Train: step: 32100; loss: 0.06466413; examples/sec: 480.94; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 32200; loss: 0.058363967; examples/sec: 479.8; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 32300; loss: 0.07425217; examples/sec: 480.17; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 32400; loss: 0.11473041; examples/sec: 480.23; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 32500; loss: 0.10222764; examples/sec: 479.99; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 32600; loss: 0.12283237; examples/sec: 479.78; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 32700; loss: 0.028140899; examples/sec: 479.88; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 32800; loss: 0.047903143; examples/sec: 479.71; densenet121_lr: 1e-06; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 32802; epoch: 41; loss: 0.8356259; min_loss: 0.80561167; since_best_loss: 28; accuracy: 0.7977764423076923; \n",
      "FastEstimator-Train: step: 32900; loss: 0.16081828; examples/sec: 480.96; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 33000; loss: 0.075936645; examples/sec: 479.9; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 33100; loss: 0.044510968; examples/sec: 479.89; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 33200; loss: 0.011048832; examples/sec: 480.02; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 33300; loss: 0.05433338; examples/sec: 479.96; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 33400; loss: 0.09886911; examples/sec: 479.93; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 33500; loss: 0.10045679; examples/sec: 479.88; densenet121_lr: 1e-06; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 33583; epoch: 42; loss: 0.84219146; min_loss: 0.80561167; since_best_loss: 29; accuracy: 0.7970753205128205; \n",
      "FastEstimator-Train: step: 33600; loss: 0.1700025; examples/sec: 481.03; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 33700; loss: 0.060427055; examples/sec: 479.91; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 33800; loss: 0.1614262; examples/sec: 480.12; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 33900; loss: 0.023201074; examples/sec: 480.14; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 34000; loss: 0.10646626; examples/sec: 480.2; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 34100; loss: 0.124089174; examples/sec: 480.18; densenet121_lr: 1e-06; \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastEstimator-Train: step: 34200; loss: 0.08373022; examples/sec: 480.2; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 34300; loss: 0.08295175; examples/sec: 479.98; densenet121_lr: 1e-06; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 34364; epoch: 43; loss: 0.82523817; min_loss: 0.80561167; since_best_loss: 30; accuracy: 0.8011818910256411; \n",
      "FastEstimator-Train: step: 34400; loss: 0.057077996; examples/sec: 480.8; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 34500; loss: 0.14795354; examples/sec: 480.02; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 34600; loss: 0.040078465; examples/sec: 480.09; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 34700; loss: 0.08570035; examples/sec: 479.92; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 34800; loss: 0.1514775; examples/sec: 479.47; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 34900; loss: 0.14707658; examples/sec: 479.56; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 35000; loss: 0.13808712; examples/sec: 479.75; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 35100; loss: 0.13879314; examples/sec: 479.8; densenet121_lr: 1e-06; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 35145; epoch: 44; loss: 0.81593335; min_loss: 0.80561167; since_best_loss: 31; accuracy: 0.8000801282051282; \n",
      "FastEstimator-Train: step: 35200; loss: 0.1366249; examples/sec: 480.77; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 35300; loss: 0.052897558; examples/sec: 479.65; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 35400; loss: 0.016840639; examples/sec: 479.68; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 35500; loss: 0.15603004; examples/sec: 479.79; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 35600; loss: 0.03288239; examples/sec: 479.73; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 35700; loss: 0.1534592; examples/sec: 479.86; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 35800; loss: 0.09301464; examples/sec: 479.81; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 35900; loss: 0.17149413; examples/sec: 479.76; densenet121_lr: 1e-06; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 35926; epoch: 45; loss: 0.835396; min_loss: 0.80561167; since_best_loss: 32; accuracy: 0.8000801282051282; \n",
      "FastEstimator-Train: step: 36000; loss: 0.087416485; examples/sec: 480.78; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 36100; loss: 0.037890263; examples/sec: 479.76; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 36200; loss: 0.028710686; examples/sec: 479.93; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 36300; loss: 0.115602; examples/sec: 479.89; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 36400; loss: 0.1477013; examples/sec: 479.69; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 36500; loss: 0.09318696; examples/sec: 479.93; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 36600; loss: 0.052137494; examples/sec: 479.67; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 36700; loss: 0.034297083; examples/sec: 479.78; densenet121_lr: 1e-06; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 36707; epoch: 46; loss: 0.8369121; min_loss: 0.80561167; since_best_loss: 33; accuracy: 0.796875; \n",
      "FastEstimator-Train: step: 36800; loss: 0.02946137; examples/sec: 480.91; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 36900; loss: 0.06606113; examples/sec: 479.44; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 37000; loss: 0.0228867; examples/sec: 479.61; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 37100; loss: 0.07998386; examples/sec: 479.69; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 37200; loss: 0.06895779; examples/sec: 479.67; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 37300; loss: 0.05293674; examples/sec: 479.91; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 37400; loss: 0.07757172; examples/sec: 479.3; densenet121_lr: 1e-06; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 37488; epoch: 47; loss: 0.82953906; min_loss: 0.80561167; since_best_loss: 34; accuracy: 0.8005809294871795; \n",
      "FastEstimator-Train: step: 37500; loss: 0.09501857; examples/sec: 480.49; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 37600; loss: 0.059387553; examples/sec: 479.62; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 37700; loss: 0.031745158; examples/sec: 479.69; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 37800; loss: 0.13525687; examples/sec: 479.22; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 37900; loss: 0.13197201; examples/sec: 479.29; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 38000; loss: 0.06702788; examples/sec: 479.58; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 38100; loss: 0.12115511; examples/sec: 479.75; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 38200; loss: 0.0961501; examples/sec: 479.97; densenet121_lr: 1e-06; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 38269; epoch: 48; loss: 0.8332439; min_loss: 0.80561167; since_best_loss: 35; accuracy: 0.7993790064102564; \n",
      "FastEstimator-Train: step: 38300; loss: 0.109389484; examples/sec: 480.59; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 38400; loss: 0.02951738; examples/sec: 479.89; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 38500; loss: 0.04388304; examples/sec: 480.03; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 38600; loss: 0.024860252; examples/sec: 480.06; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 38700; loss: 0.024129301; examples/sec: 479.98; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 38800; loss: 0.06439555; examples/sec: 479.86; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 38900; loss: 0.051749457; examples/sec: 479.97; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 39000; loss: 0.10151175; examples/sec: 479.93; densenet121_lr: 1e-06; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 39050; epoch: 49; loss: 0.8347072; min_loss: 0.80561167; since_best_loss: 36; accuracy: 0.7988782051282052; \n",
      "FastEstimator-Train: step: 39100; loss: 0.20080724; examples/sec: 481.03; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 39200; loss: 0.078811035; examples/sec: 479.9; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 39300; loss: 0.018808879; examples/sec: 479.85; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 39400; loss: 0.13734879; examples/sec: 479.99; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 39500; loss: 0.07240154; examples/sec: 479.87; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 39600; loss: 0.12791005; examples/sec: 479.87; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 39700; loss: 0.10020809; examples/sec: 479.94; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 39800; loss: 0.030074557; examples/sec: 479.84; densenet121_lr: 1e-06; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 39831; epoch: 50; loss: 0.82397527; min_loss: 0.80561167; since_best_loss: 37; accuracy: 0.7996794871794872; \n",
      "FastEstimator-Train: step: 39900; loss: 0.10301467; examples/sec: 481.02; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 40000; loss: 0.09538902; examples/sec: 479.88; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 40100; loss: 0.08635545; examples/sec: 479.68; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 40200; loss: 0.031733803; examples/sec: 479.16; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 40300; loss: 0.057907432; examples/sec: 479.89; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 40400; loss: 0.1606677; examples/sec: 479.93; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 40500; loss: 0.055677254; examples/sec: 479.87; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 40600; loss: 0.048216376; examples/sec: 479.99; densenet121_lr: 1e-06; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 40612; epoch: 51; loss: 0.8327281; min_loss: 0.80561167; since_best_loss: 38; accuracy: 0.799979967948718; \n",
      "FastEstimator-Train: step: 40700; loss: 0.073308304; examples/sec: 480.96; densenet121_lr: 1e-06; \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastEstimator-Train: step: 40800; loss: 0.11249444; examples/sec: 479.83; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 40900; loss: 0.059206348; examples/sec: 479.71; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 41000; loss: 0.1229921; examples/sec: 479.54; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 41100; loss: 0.13271764; examples/sec: 479.35; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 41200; loss: 0.17915091; examples/sec: 479.29; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 41300; loss: 0.07844268; examples/sec: 479.49; densenet121_lr: 1e-06; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 41393; epoch: 52; loss: 0.8312372; min_loss: 0.80561167; since_best_loss: 39; accuracy: 0.797676282051282; \n",
      "FastEstimator-Train: step: 41400; loss: 0.17970242; examples/sec: 480.48; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 41500; loss: 0.048979986; examples/sec: 479.68; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 41600; loss: 0.044163883; examples/sec: 479.8; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 41700; loss: 0.047349; examples/sec: 479.68; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 41800; loss: 0.0796637; examples/sec: 479.6; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 41900; loss: 0.113332726; examples/sec: 479.73; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 42000; loss: 0.09790136; examples/sec: 479.55; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 42100; loss: 0.19702779; examples/sec: 479.83; densenet121_lr: 1e-06; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 42174; epoch: 53; loss: 0.84219617; min_loss: 0.80561167; since_best_loss: 40; accuracy: 0.799979967948718; \n",
      "FastEstimator-Train: step: 42200; loss: 0.05265092; examples/sec: 480.78; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 42300; loss: 0.102497965; examples/sec: 479.62; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 42400; loss: 0.10382214; examples/sec: 479.72; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 42500; loss: 0.13532165; examples/sec: 479.77; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 42600; loss: 0.041131712; examples/sec: 479.75; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 42700; loss: 0.18398854; examples/sec: 479.7; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 42800; loss: 0.09101735; examples/sec: 479.61; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 42900; loss: 0.019460114; examples/sec: 479.7; densenet121_lr: 1e-06; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 42955; epoch: 54; loss: 0.81542355; min_loss: 0.80561167; since_best_loss: 41; accuracy: 0.8001802884615384; \n",
      "FastEstimator-Train: step: 43000; loss: 0.06913818; examples/sec: 480.81; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 43100; loss: 0.09025133; examples/sec: 479.8; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 43200; loss: 0.057437044; examples/sec: 479.7; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 43300; loss: 0.062049016; examples/sec: 479.78; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 43400; loss: 0.04531814; examples/sec: 479.8; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 43500; loss: 0.08702671; examples/sec: 479.77; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 43600; loss: 0.12899709; examples/sec: 479.39; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 43700; loss: 0.11754186; examples/sec: 479.22; densenet121_lr: 1e-06; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 43736; epoch: 55; loss: 0.8370326; min_loss: 0.80561167; since_best_loss: 42; accuracy: 0.8001802884615384; \n",
      "FastEstimator-Train: step: 43800; loss: 0.23036394; examples/sec: 480.84; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 43900; loss: 0.10117244; examples/sec: 479.77; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 44000; loss: 0.050189402; examples/sec: 479.72; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 44100; loss: 0.06441792; examples/sec: 479.71; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 44200; loss: 0.14697343; examples/sec: 479.7; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 44300; loss: 0.059593495; examples/sec: 479.77; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 44400; loss: 0.27445716; examples/sec: 479.71; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 44500; loss: 0.038784467; examples/sec: 479.76; densenet121_lr: 1e-06; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 44517; epoch: 56; loss: 0.826899; min_loss: 0.80561167; since_best_loss: 43; accuracy: 0.7995793269230769; \n",
      "FastEstimator-Train: step: 44600; loss: 0.030334197; examples/sec: 480.58; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 44700; loss: 0.063941374; examples/sec: 479.78; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 44800; loss: 0.099558115; examples/sec: 479.39; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 44900; loss: 0.05816964; examples/sec: 478.86; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 45000; loss: 0.10414685; examples/sec: 479.72; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 45100; loss: 0.15620479; examples/sec: 479.66; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 45200; loss: 0.12911566; examples/sec: 479.65; densenet121_lr: 1e-06; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 45298; epoch: 57; loss: 0.83751816; min_loss: 0.80561167; since_best_loss: 44; accuracy: 0.7974759615384616; \n",
      "FastEstimator-Train: step: 45300; loss: 0.16635999; examples/sec: 480.59; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 45400; loss: 0.13730367; examples/sec: 479.56; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 45500; loss: 0.21126029; examples/sec: 479.49; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 45600; loss: 0.051366273; examples/sec: 479.53; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 45700; loss: 0.024301138; examples/sec: 479.54; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 45800; loss: 0.015286388; examples/sec: 479.3; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 45900; loss: 0.017832706; examples/sec: 479.71; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 46000; loss: 0.04982757; examples/sec: 479.72; densenet121_lr: 1e-06; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 46079; epoch: 58; loss: 0.83027804; min_loss: 0.80561167; since_best_loss: 45; accuracy: 0.7990785256410257; \n",
      "FastEstimator-Train: step: 46100; loss: 0.18665637; examples/sec: 480.74; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 46200; loss: 0.053879283; examples/sec: 479.62; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 46300; loss: 0.058406435; examples/sec: 479.8; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 46400; loss: 0.05474761; examples/sec: 479.58; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 46500; loss: 0.10427478; examples/sec: 479.55; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 46600; loss: 0.07561822; examples/sec: 479.54; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 46700; loss: 0.037511468; examples/sec: 479.67; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 46800; loss: 0.062363263; examples/sec: 479.62; densenet121_lr: 1e-06; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 46860; epoch: 59; loss: 0.81947315; min_loss: 0.80561167; since_best_loss: 46; accuracy: 0.8013822115384616; \n",
      "FastEstimator-Train: step: 46900; loss: 0.22336327; examples/sec: 480.6; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 47000; loss: 0.1963785; examples/sec: 479.45; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 47100; loss: 0.06800881; examples/sec: 479.51; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 47200; loss: 0.07974425; examples/sec: 479.57; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 47300; loss: 0.06286056; examples/sec: 479.48; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 47400; loss: 0.08143704; examples/sec: 479.49; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 47500; loss: 0.025179092; examples/sec: 479.22; densenet121_lr: 1e-06; \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastEstimator-Train: step: 47600; loss: 0.13369416; examples/sec: 479.08; densenet121_lr: 1e-06; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 47641; epoch: 60; loss: 0.8399075; min_loss: 0.80561167; since_best_loss: 47; accuracy: 0.7971754807692307; \n",
      "FastEstimator-Train: step: 47700; loss: 0.01459052; examples/sec: 480.23; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 47800; loss: 0.119249225; examples/sec: 478.94; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 47900; loss: 0.10699761; examples/sec: 479.33; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 48000; loss: 0.041301657; examples/sec: 479.37; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 48100; loss: 0.06503202; examples/sec: 479.4; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 48200; loss: 0.12967314; examples/sec: 479.29; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 48300; loss: 0.14921948; examples/sec: 479.32; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 48400; loss: 0.08226717; examples/sec: 479.26; densenet121_lr: 1e-06; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 48422; epoch: 61; loss: 0.8334558; min_loss: 0.80561167; since_best_loss: 48; accuracy: 0.7992788461538461; \n",
      "FastEstimator-Train: step: 48500; loss: 0.2734901; examples/sec: 480.37; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 48600; loss: 0.0436935; examples/sec: 479.27; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 48700; loss: 0.040373493; examples/sec: 479.17; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 48800; loss: 0.21656492; examples/sec: 479.26; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 48900; loss: 0.11226594; examples/sec: 479.11; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 49000; loss: 0.0738741; examples/sec: 479.02; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 49100; loss: 0.10443306; examples/sec: 479.1; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 49200; loss: 0.039211586; examples/sec: 479.12; densenet121_lr: 1e-06; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 49203; epoch: 62; loss: 0.82302624; min_loss: 0.80561167; since_best_loss: 49; accuracy: 0.8000801282051282; \n",
      "FastEstimator-Train: step: 49300; loss: 0.11133564; examples/sec: 480.14; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 49400; loss: 0.03416788; examples/sec: 479.07; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 49500; loss: 0.029823331; examples/sec: 478.99; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 49600; loss: 0.045867085; examples/sec: 478.89; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 49700; loss: 0.030492887; examples/sec: 478.96; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 49800; loss: 0.054496102; examples/sec: 478.94; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 49900; loss: 0.009699377; examples/sec: 478.8; densenet121_lr: 1e-06; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 49984; epoch: 63; loss: 0.8410022; min_loss: 0.80561167; since_best_loss: 50; accuracy: 0.7979767628205128; \n",
      "FastEstimator-Train: step: 50000; loss: 0.12459032; examples/sec: 479.84; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 50100; loss: 0.113994434; examples/sec: 478.74; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 50200; loss: 0.13371687; examples/sec: 478.58; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 50300; loss: 0.087045535; examples/sec: 478.59; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 50400; loss: 0.17259821; examples/sec: 478.57; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 50500; loss: 0.12500584; examples/sec: 477.95; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 50600; loss: 0.0150641175; examples/sec: 478.36; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 50700; loss: 0.02096802; examples/sec: 478.53; densenet121_lr: 1e-06; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 50765; epoch: 64; loss: 0.8331332; min_loss: 0.80561167; since_best_loss: 51; accuracy: 0.7975761217948718; \n",
      "FastEstimator-Train: step: 50800; loss: 0.067374736; examples/sec: 479.53; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 50900; loss: 0.17101121; examples/sec: 478.34; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 51000; loss: 0.11850756; examples/sec: 478.35; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 51100; loss: 0.065681785; examples/sec: 478.37; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 51200; loss: 0.10387266; examples/sec: 478.19; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 51300; loss: 0.07500157; examples/sec: 477.7; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 51400; loss: 0.11061505; examples/sec: 477.9; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 51500; loss: 0.037488338; examples/sec: 478.23; densenet121_lr: 1e-06; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 51546; epoch: 65; loss: 0.81996906; min_loss: 0.80561167; since_best_loss: 52; accuracy: 0.8022836538461539; \n",
      "FastEstimator-Train: step: 51600; loss: 0.22878891; examples/sec: 479.09; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 51700; loss: 0.091809675; examples/sec: 477.59; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 51800; loss: 0.1414083; examples/sec: 477.85; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 51900; loss: 0.036360558; examples/sec: 477.81; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 52000; loss: 0.16396476; examples/sec: 477.64; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 52100; loss: 0.027118286; examples/sec: 477.36; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 52200; loss: 0.13280928; examples/sec: 477.16; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 52300; loss: 0.09440096; examples/sec: 477.82; densenet121_lr: 1e-06; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 52327; epoch: 66; loss: 0.8299095; min_loss: 0.80561167; since_best_loss: 53; accuracy: 0.797676282051282; \n",
      "FastEstimator-Train: step: 52400; loss: 0.080730766; examples/sec: 478.93; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 52500; loss: 0.08429435; examples/sec: 477.21; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 52600; loss: 0.13425991; examples/sec: 477.4; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 52700; loss: 0.04230259; examples/sec: 477.39; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 52800; loss: 0.03080453; examples/sec: 477.32; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 52900; loss: 0.116333604; examples/sec: 477.26; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 53000; loss: 0.027693179; examples/sec: 477.0; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 53100; loss: 0.01557637; examples/sec: 477.35; densenet121_lr: 1e-06; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 53108; epoch: 67; loss: 0.8426231; min_loss: 0.80561167; since_best_loss: 54; accuracy: 0.7995793269230769; \n",
      "FastEstimator-Train: step: 53200; loss: 0.08180533; examples/sec: 478.84; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 53300; loss: 0.022910144; examples/sec: 477.52; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 53400; loss: 0.047398396; examples/sec: 477.43; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 53500; loss: 0.10149118; examples/sec: 477.63; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 53600; loss: 0.10090165; examples/sec: 477.42; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 53700; loss: 0.021727815; examples/sec: 477.32; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 53800; loss: 0.064243935; examples/sec: 477.49; densenet121_lr: 1e-06; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 53889; epoch: 68; loss: 0.8187706; min_loss: 0.80561167; since_best_loss: 55; accuracy: 0.8014823717948718; \n",
      "FastEstimator-Train: step: 53900; loss: 0.1113656; examples/sec: 478.5; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 54000; loss: 0.07837881; examples/sec: 477.31; densenet121_lr: 1e-06; \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastEstimator-Train: step: 54100; loss: 0.041206226; examples/sec: 477.43; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 54200; loss: 0.12408933; examples/sec: 477.43; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 54300; loss: 0.15357207; examples/sec: 477.23; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 54400; loss: 0.04524383; examples/sec: 477.48; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 54500; loss: 0.09232055; examples/sec: 477.36; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 54600; loss: 0.087635025; examples/sec: 477.43; densenet121_lr: 1e-06; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 54670; epoch: 69; loss: 0.82910603; min_loss: 0.80561167; since_best_loss: 56; accuracy: 0.8001802884615384; \n",
      "FastEstimator-Train: step: 54700; loss: 0.06664575; examples/sec: 478.34; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 54800; loss: 0.061680175; examples/sec: 477.2; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 54900; loss: 0.042883206; examples/sec: 477.25; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 55000; loss: 0.102905504; examples/sec: 477.32; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 55100; loss: 0.040605806; examples/sec: 477.26; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 55200; loss: 0.1303308; examples/sec: 477.12; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 55300; loss: 0.08254415; examples/sec: 477.09; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 55400; loss: 0.17409138; examples/sec: 477.51; densenet121_lr: 1e-06; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 55451; epoch: 70; loss: 0.8435973; min_loss: 0.80561167; since_best_loss: 57; accuracy: 0.7951722756410257; \n",
      "FastEstimator-Train: step: 55500; loss: 0.08175896; examples/sec: 477.85; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 55600; loss: 0.069739684; examples/sec: 477.14; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 55700; loss: 0.09158772; examples/sec: 477.68; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 55800; loss: 0.114497736; examples/sec: 477.29; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 55900; loss: 0.086772226; examples/sec: 477.55; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 56000; loss: 0.037767332; examples/sec: 476.92; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 56100; loss: 0.059885986; examples/sec: 477.66; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 56200; loss: 0.08149854; examples/sec: 476.91; densenet121_lr: 1e-06; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 56232; epoch: 71; loss: 0.82733387; min_loss: 0.80561167; since_best_loss: 58; accuracy: 0.7986778846153846; \n",
      "FastEstimator-Train: step: 56300; loss: 0.018672537; examples/sec: 477.99; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 56400; loss: 0.059377722; examples/sec: 477.46; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 56500; loss: 0.08270857; examples/sec: 477.48; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 56600; loss: 0.09760778; examples/sec: 477.73; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 56700; loss: 0.17287028; examples/sec: 477.2; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 56800; loss: 0.050755315; examples/sec: 477.76; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 56900; loss: 0.16356617; examples/sec: 477.46; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 57000; loss: 0.047855336; examples/sec: 477.3; densenet121_lr: 1e-06; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 57013; epoch: 72; loss: 0.82516766; min_loss: 0.80561167; since_best_loss: 59; accuracy: 0.8013822115384616; \n",
      "FastEstimator-Train: step: 57100; loss: 0.06491198; examples/sec: 478.39; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 57200; loss: 0.028996892; examples/sec: 477.18; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 57300; loss: 0.05602482; examples/sec: 476.99; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 57400; loss: 0.20176986; examples/sec: 477.44; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 57500; loss: 0.042482674; examples/sec: 477.28; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 57600; loss: 0.06382194; examples/sec: 477.41; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 57700; loss: 0.13049974; examples/sec: 477.08; densenet121_lr: 1e-06; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 57794; epoch: 73; loss: 0.83615756; min_loss: 0.80561167; since_best_loss: 60; accuracy: 0.7988782051282052; \n",
      "FastEstimator-Train: step: 57800; loss: 0.04846467; examples/sec: 478.74; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 57900; loss: 0.06272792; examples/sec: 477.07; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 58000; loss: 0.048145905; examples/sec: 477.0; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 58100; loss: 0.1306756; examples/sec: 477.19; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 58200; loss: 0.13269249; examples/sec: 477.33; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 58300; loss: 0.08511378; examples/sec: 477.74; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 58400; loss: 0.16917941; examples/sec: 477.12; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 58500; loss: 0.01746406; examples/sec: 477.49; densenet121_lr: 1e-06; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 58575; epoch: 74; loss: 0.8202516; min_loss: 0.80561167; since_best_loss: 61; accuracy: 0.8024839743589743; \n",
      "FastEstimator-Train: step: 58600; loss: 0.047174953; examples/sec: 478.83; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 58700; loss: 0.024165142; examples/sec: 477.46; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 58800; loss: 0.113254264; examples/sec: 477.69; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 58900; loss: 0.03774754; examples/sec: 477.76; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 59000; loss: 0.08056788; examples/sec: 477.64; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 59100; loss: 0.09986633; examples/sec: 477.42; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 59200; loss: 0.14377235; examples/sec: 477.6; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 59300; loss: 0.18567786; examples/sec: 477.05; densenet121_lr: 1e-06; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 59356; epoch: 75; loss: 0.83584225; min_loss: 0.80561167; since_best_loss: 62; accuracy: 0.7987780448717948; \n",
      "FastEstimator-Train: step: 59400; loss: 0.2030769; examples/sec: 478.72; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 59500; loss: 0.08383748; examples/sec: 477.46; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 59600; loss: 0.03788665; examples/sec: 477.62; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 59700; loss: 0.06950968; examples/sec: 477.46; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 59800; loss: 0.104328305; examples/sec: 477.7; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 59900; loss: 0.110154435; examples/sec: 477.55; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 60000; loss: 0.17284775; examples/sec: 477.75; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 60100; loss: 0.111019686; examples/sec: 477.42; densenet121_lr: 1e-06; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 60137; epoch: 76; loss: 0.83242434; min_loss: 0.80561167; since_best_loss: 63; accuracy: 0.7984775641025641; \n",
      "FastEstimator-Train: step: 60200; loss: 0.10142567; examples/sec: 478.16; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 60300; loss: 0.107715204; examples/sec: 477.64; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 60400; loss: 0.02721014; examples/sec: 477.31; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 60500; loss: 0.10145547; examples/sec: 476.85; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 60600; loss: 0.0828657; examples/sec: 477.11; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 60700; loss: 0.113477565; examples/sec: 477.58; densenet121_lr: 1e-06; \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastEstimator-Train: step: 60800; loss: 0.1551293; examples/sec: 477.45; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 60900; loss: 0.045775816; examples/sec: 477.85; densenet121_lr: 1e-06; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 60918; epoch: 77; loss: 0.8324583; min_loss: 0.80561167; since_best_loss: 64; accuracy: 0.8004807692307693; \n",
      "FastEstimator-Train: step: 61000; loss: 0.14862458; examples/sec: 478.65; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 61100; loss: 0.07917966; examples/sec: 477.65; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 61200; loss: 0.045302276; examples/sec: 477.59; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 61300; loss: 0.28696972; examples/sec: 477.83; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 61400; loss: 0.06003861; examples/sec: 477.72; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 61500; loss: 0.073388204; examples/sec: 477.58; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 61600; loss: 0.24598946; examples/sec: 477.4; densenet121_lr: 1e-06; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 61699; epoch: 78; loss: 0.83762985; min_loss: 0.80561167; since_best_loss: 65; accuracy: 0.7982772435897436; \n",
      "FastEstimator-Train: step: 61700; loss: 0.04880947; examples/sec: 478.81; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 61800; loss: 0.010163124; examples/sec: 477.42; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 61900; loss: 0.041090656; examples/sec: 477.64; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 62000; loss: 0.10844965; examples/sec: 477.46; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 62100; loss: 0.0533512; examples/sec: 477.41; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 62200; loss: 0.13459618; examples/sec: 477.67; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 62300; loss: 0.10201654; examples/sec: 477.73; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 62400; loss: 0.052625887; examples/sec: 477.45; densenet121_lr: 1e-06; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 62480; epoch: 79; loss: 0.82526946; min_loss: 0.80561167; since_best_loss: 66; accuracy: 0.8009815705128205; \n",
      "FastEstimator-Train: step: 62500; loss: 0.10776623; examples/sec: 478.4; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 62600; loss: 0.062161952; examples/sec: 477.52; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 62700; loss: 0.12810224; examples/sec: 477.63; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 62800; loss: 0.07522751; examples/sec: 477.59; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 62900; loss: 0.052487202; examples/sec: 477.39; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 63000; loss: 0.06024359; examples/sec: 477.5; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 63100; loss: 0.13125017; examples/sec: 477.58; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 63200; loss: 0.017966378; examples/sec: 477.58; densenet121_lr: 1e-06; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 63261; epoch: 80; loss: 0.83961713; min_loss: 0.80561167; since_best_loss: 67; accuracy: 0.7993790064102564; \n",
      "FastEstimator-Train: step: 63300; loss: 0.07874285; examples/sec: 478.52; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 63400; loss: 0.07440717; examples/sec: 477.47; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 63500; loss: 0.0148188025; examples/sec: 477.49; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 63600; loss: 0.020811096; examples/sec: 477.53; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 63700; loss: 0.02457411; examples/sec: 477.67; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 63800; loss: 0.10997613; examples/sec: 477.52; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 63900; loss: 0.021843571; examples/sec: 477.41; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 64000; loss: 0.052802745; examples/sec: 476.96; densenet121_lr: 1e-06; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 64042; epoch: 81; loss: 0.8270348; min_loss: 0.80561167; since_best_loss: 68; accuracy: 0.7984775641025641; \n",
      "FastEstimator-Train: step: 64100; loss: 0.058924578; examples/sec: 477.87; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 64200; loss: 0.1047832; examples/sec: 477.45; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 64300; loss: 0.03196108; examples/sec: 477.73; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 64400; loss: 0.015828898; examples/sec: 477.61; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 64500; loss: 0.09860914; examples/sec: 477.53; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 64600; loss: 0.046430703; examples/sec: 477.46; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 64700; loss: 0.13570125; examples/sec: 477.19; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 64800; loss: 0.019522574; examples/sec: 477.24; densenet121_lr: 1e-06; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 64823; epoch: 82; loss: 0.8216881; min_loss: 0.80561167; since_best_loss: 69; accuracy: 0.8024839743589743; \n",
      "FastEstimator-Train: step: 64900; loss: 0.0116169285; examples/sec: 478.51; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 65000; loss: 0.07668665; examples/sec: 477.66; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 65100; loss: 0.13092792; examples/sec: 477.29; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 65200; loss: 0.09550965; examples/sec: 477.44; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 65300; loss: 0.18431963; examples/sec: 477.23; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 65400; loss: 0.13479635; examples/sec: 477.02; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 65500; loss: 0.03311309; examples/sec: 476.98; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 65600; loss: 0.11293585; examples/sec: 476.55; densenet121_lr: 1e-06; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 65604; epoch: 83; loss: 0.8365265; min_loss: 0.80561167; since_best_loss: 70; accuracy: 0.7951722756410257; \n",
      "FastEstimator-Train: step: 65700; loss: 0.0639799; examples/sec: 478.1; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 65800; loss: 0.092035845; examples/sec: 477.35; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 65900; loss: 0.23605613; examples/sec: 476.8; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 66000; loss: 0.06694021; examples/sec: 477.21; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 66100; loss: 0.09171147; examples/sec: 476.87; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 66200; loss: 0.09323762; examples/sec: 476.6; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 66300; loss: 0.05370827; examples/sec: 477.15; densenet121_lr: 1e-06; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 66385; epoch: 84; loss: 0.83184755; min_loss: 0.80561167; since_best_loss: 71; accuracy: 0.801582532051282; \n",
      "FastEstimator-Train: step: 66400; loss: 0.184595; examples/sec: 478.08; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 66500; loss: 0.042761486; examples/sec: 477.39; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 66600; loss: 0.03667021; examples/sec: 477.6; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 66700; loss: 0.03195192; examples/sec: 477.41; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 66800; loss: 0.09611376; examples/sec: 477.37; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 66900; loss: 0.1781826; examples/sec: 477.29; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 67000; loss: 0.08584866; examples/sec: 477.44; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 67100; loss: 0.029206682; examples/sec: 477.5; densenet121_lr: 1e-06; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 67166; epoch: 85; loss: 0.8385208; min_loss: 0.80561167; since_best_loss: 72; accuracy: 0.7979767628205128; \n",
      "FastEstimator-Train: step: 67200; loss: 0.19410124; examples/sec: 478.58; densenet121_lr: 1e-06; \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastEstimator-Train: step: 67300; loss: 0.042020146; examples/sec: 477.32; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 67400; loss: 0.058309674; examples/sec: 477.48; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 67500; loss: 0.088257454; examples/sec: 477.39; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 67600; loss: 0.10592627; examples/sec: 477.05; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 67700; loss: 0.024674645; examples/sec: 476.98; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 67800; loss: 0.062594526; examples/sec: 477.47; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 67900; loss: 0.05506504; examples/sec: 477.37; densenet121_lr: 1e-06; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 67947; epoch: 86; loss: 0.8240808; min_loss: 0.80561167; since_best_loss: 73; accuracy: 0.801582532051282; \n",
      "FastEstimator-Train: step: 68000; loss: 0.11855619; examples/sec: 478.44; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 68100; loss: 0.1796736; examples/sec: 477.69; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 68200; loss: 0.048560463; examples/sec: 477.09; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 68300; loss: 0.16303581; examples/sec: 477.0; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 68400; loss: 0.03480692; examples/sec: 477.14; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 68500; loss: 0.038870946; examples/sec: 476.6; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 68600; loss: 0.09781602; examples/sec: 477.62; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 68700; loss: 0.068597466; examples/sec: 477.53; densenet121_lr: 1e-06; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 68728; epoch: 87; loss: 0.83551747; min_loss: 0.80561167; since_best_loss: 74; accuracy: 0.7995793269230769; \n",
      "FastEstimator-Train: step: 68800; loss: 0.07599482; examples/sec: 478.55; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 68900; loss: 0.1436432; examples/sec: 477.59; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 69000; loss: 0.021811018; examples/sec: 477.4; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 69100; loss: 0.027481625; examples/sec: 477.5; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 69200; loss: 0.13079895; examples/sec: 477.64; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 69300; loss: 0.17519228; examples/sec: 477.32; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 69400; loss: 0.20015524; examples/sec: 477.42; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 69500; loss: 0.134004; examples/sec: 477.58; densenet121_lr: 1e-06; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 69509; epoch: 88; loss: 0.8319082; min_loss: 0.80561167; since_best_loss: 75; accuracy: 0.8008814102564102; \n",
      "FastEstimator-Train: step: 69600; loss: 0.049926266; examples/sec: 478.79; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 69700; loss: 0.013112414; examples/sec: 477.5; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 69800; loss: 0.052905265; examples/sec: 477.46; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 69900; loss: 0.05625566; examples/sec: 477.46; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 70000; loss: 0.08443791; examples/sec: 477.45; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 70100; loss: 0.04374191; examples/sec: 477.59; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 70200; loss: 0.10374459; examples/sec: 477.55; densenet121_lr: 1e-06; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 70290; epoch: 89; loss: 0.8253447; min_loss: 0.80561167; since_best_loss: 76; accuracy: 0.7977764423076923; \n",
      "FastEstimator-Train: step: 70300; loss: 0.023844995; examples/sec: 478.47; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 70400; loss: 0.07064551; examples/sec: 477.52; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 70500; loss: 0.07747418; examples/sec: 477.44; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 70600; loss: 0.04914337; examples/sec: 477.41; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 70700; loss: 0.023263875; examples/sec: 477.47; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 70800; loss: 0.065586604; examples/sec: 477.45; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 70900; loss: 0.12493597; examples/sec: 477.51; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 71000; loss: 0.08942243; examples/sec: 477.55; densenet121_lr: 1e-06; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 71071; epoch: 90; loss: 0.840762; min_loss: 0.80561167; since_best_loss: 77; accuracy: 0.8003806089743589; \n",
      "FastEstimator-Train: step: 71100; loss: 0.08432238; examples/sec: 478.53; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 71200; loss: 0.021541435; examples/sec: 477.02; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 71300; loss: 0.1981619; examples/sec: 476.37; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 71400; loss: 0.07842607; examples/sec: 476.68; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 71500; loss: 0.058678508; examples/sec: 477.53; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 71600; loss: 0.07059926; examples/sec: 477.55; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 71700; loss: 0.036718465; examples/sec: 477.28; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 71800; loss: 0.08508014; examples/sec: 477.49; densenet121_lr: 1e-06; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 71852; epoch: 91; loss: 0.8362366; min_loss: 0.80561167; since_best_loss: 78; accuracy: 0.7983774038461539; \n",
      "FastEstimator-Train: step: 71900; loss: 0.027965909; examples/sec: 478.44; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 72000; loss: 0.034990694; examples/sec: 477.15; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 72100; loss: 0.074362084; examples/sec: 476.99; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 72200; loss: 0.14766204; examples/sec: 477.17; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 72300; loss: 0.05780673; examples/sec: 477.1; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 72400; loss: 0.07776028; examples/sec: 477.1; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 72500; loss: 0.07390002; examples/sec: 477.55; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 72600; loss: 0.050688207; examples/sec: 476.77; densenet121_lr: 1e-06; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 72633; epoch: 92; loss: 0.8323323; min_loss: 0.80561167; since_best_loss: 79; accuracy: 0.7984775641025641; \n",
      "FastEstimator-Train: step: 72700; loss: 0.0698088; examples/sec: 478.65; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 72800; loss: 0.08477635; examples/sec: 477.55; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 72900; loss: 0.104124784; examples/sec: 477.23; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 73000; loss: 0.050714053; examples/sec: 476.76; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 73100; loss: 0.05853127; examples/sec: 476.91; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 73200; loss: 0.028826945; examples/sec: 477.06; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 73300; loss: 0.10188471; examples/sec: 477.44; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 73400; loss: 0.061880175; examples/sec: 476.64; densenet121_lr: 1e-06; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 73414; epoch: 93; loss: 0.8144445; min_loss: 0.80561167; since_best_loss: 80; accuracy: 0.799979967948718; \n",
      "FastEstimator-Train: step: 73500; loss: 0.122054994; examples/sec: 477.98; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 73600; loss: 0.057571292; examples/sec: 477.33; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 73700; loss: 0.029316934; examples/sec: 476.83; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 73800; loss: 0.028558623; examples/sec: 477.15; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 73900; loss: 0.06259504; examples/sec: 477.35; densenet121_lr: 1e-06; \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastEstimator-Train: step: 74000; loss: 0.09431377; examples/sec: 477.49; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 74100; loss: 0.033630583; examples/sec: 476.9; densenet121_lr: 1e-06; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 74195; epoch: 94; loss: 0.8306216; min_loss: 0.80561167; since_best_loss: 81; accuracy: 0.8022836538461539; \n",
      "FastEstimator-Train: step: 74200; loss: 0.15188096; examples/sec: 478.34; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 74300; loss: 0.040334612; examples/sec: 477.64; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 74400; loss: 0.047368713; examples/sec: 477.56; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 74500; loss: 0.044137344; examples/sec: 477.75; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 74600; loss: 0.14507648; examples/sec: 477.92; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 74700; loss: 0.07336719; examples/sec: 477.71; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 74800; loss: 0.041084334; examples/sec: 477.79; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 74900; loss: 0.08739099; examples/sec: 477.65; densenet121_lr: 1e-06; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 74976; epoch: 95; loss: 0.84816897; min_loss: 0.80561167; since_best_loss: 82; accuracy: 0.7963741987179487; \n",
      "FastEstimator-Train: step: 75000; loss: 0.025433317; examples/sec: 478.87; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 75100; loss: 0.13385633; examples/sec: 477.65; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 75200; loss: 0.045226797; examples/sec: 477.67; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 75300; loss: 0.013343745; examples/sec: 477.71; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 75400; loss: 0.13392614; examples/sec: 477.61; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 75500; loss: 0.120577544; examples/sec: 477.62; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 75600; loss: 0.021699576; examples/sec: 477.39; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 75700; loss: 0.08334631; examples/sec: 477.47; densenet121_lr: 1e-06; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 75757; epoch: 96; loss: 0.8416972; min_loss: 0.80561167; since_best_loss: 83; accuracy: 0.7969751602564102; \n",
      "FastEstimator-Train: step: 75800; loss: 0.033454385; examples/sec: 478.38; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 75900; loss: 0.05288597; examples/sec: 477.39; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 76000; loss: 0.058074858; examples/sec: 476.98; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 76100; loss: 0.27422774; examples/sec: 477.24; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 76200; loss: 0.047296807; examples/sec: 476.5; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 76300; loss: 0.07101974; examples/sec: 476.54; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 76400; loss: 0.084911; examples/sec: 477.38; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 76500; loss: 0.06527689; examples/sec: 477.22; densenet121_lr: 1e-06; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 76538; epoch: 97; loss: 0.82294464; min_loss: 0.80561167; since_best_loss: 84; accuracy: 0.8018830128205128; \n",
      "FastEstimator-Train: step: 76600; loss: 0.08134085; examples/sec: 478.47; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 76700; loss: 0.029883368; examples/sec: 477.06; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 76800; loss: 0.055570375; examples/sec: 476.41; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 76900; loss: 0.07605739; examples/sec: 476.82; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 77000; loss: 0.109579876; examples/sec: 477.18; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 77100; loss: 0.02749157; examples/sec: 477.03; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 77200; loss: 0.034059294; examples/sec: 476.83; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 77300; loss: 0.12071203; examples/sec: 477.22; densenet121_lr: 1e-06; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 77319; epoch: 98; loss: 0.8258794; min_loss: 0.80561167; since_best_loss: 85; accuracy: 0.8003806089743589; \n",
      "FastEstimator-Train: step: 77400; loss: 0.07866405; examples/sec: 478.39; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 77500; loss: 0.075476885; examples/sec: 477.58; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 77600; loss: 0.096294895; examples/sec: 477.74; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 77700; loss: 0.25596967; examples/sec: 477.44; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 77800; loss: 0.20583399; examples/sec: 477.46; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 77900; loss: 0.12970708; examples/sec: 477.81; densenet121_lr: 1e-06; \n",
      "FastEstimator-Train: step: 78000; loss: 0.009589294; examples/sec: 477.43; densenet121_lr: 1e-06; \n",
      "FastEstimator-LRController: learning rate reduced by factor of 0.1\n",
      "FastEstimator-Eval: step: 78100; epoch: 99; loss: 0.82076794; min_loss: 0.80561167; since_best_loss: 86; accuracy: 0.8016826923076923; \n",
      "FastEstimator-Finish: step: 78100; total_time: 11113.56 sec; densenet121_lr: 1e-06; \n"
     ]
    }
   ],
   "source": [
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Model\n",
    "After we trained the model, we might want to validate the model by running inference on evaluation datasets. Because FE so far doesn't support doing inference using estimator, We ues Keras API. \n",
    "\n",
    "First load the keras model (storing by **ModelSaver**)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = tf.keras.models.load_model(\"densenet121_best_loss.h5\", compile=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the keras model doesn't include the data preprocessing pipeline, we cannot ingest the raw dataset to the model. Instead, we need to create the same pipeline again with batch size equal to whole evaluation dataset and feed the processed to the keras model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the evaluation accuracy is 0.7518\n"
     ]
    }
   ],
   "source": [
    "pipeline = fe.Pipeline(batch_size=10000, data=data, ops=Minmax(inputs=\"x\", outputs=\"x2\"))\n",
    "eval_sample = pipeline.show_results(mode=\"eval\")\n",
    "x_input = eval_sample[0][\"x2\"].numpy()\n",
    "y_input = eval_sample[0][\"y\"].numpy()\n",
    "y_output = trained_model.predict(x_input)\n",
    "y_predict = np.argmax(y_output, axis=1).reshape(10000,1)\n",
    "print(\"the evaluation accuracy is {}\".format(np.count_nonzero((y_input == y_predict))/10000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look as a random inference sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the ground truth label is [1], and the prediction is [1]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rand_int = np.random.randint(10000)\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(x_input[rand_int])\n",
    "ax.set_title(\"the input image\")\n",
    "print(\"the ground truth label is {}, and the prediction is {}\".format(y_input[rand_int], y_predict[rand_int]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}