{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Image Classification Using LeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we are going to walk through the logic in `lenet_mnist.py` shown below and provide step-by-step instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Copyright 2019 The FastEstimator Authors. All Rights Reserved.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#     http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "# ==============================================================================\n",
      "import tempfile\n",
      "\n",
      "import numpy as np\n",
      "import tensorflow as tf\n",
      "\n",
      "import fastestimator as fe\n",
      "from fastestimator.trace import Accuracy, ModelSaver\n",
      "from fastestimator.architecture import LeNet\n",
      "from fastestimator.op.tensorop import Minmax, ModelOp, SparseCategoricalCrossentropy\n",
      "\n",
      "\n",
      "def get_estimator(epochs=2, batch_size=32, model_dir=tempfile.mkdtemp()):\n",
      "    # step 1. prepare data\n",
      "    (x_train, y_train), (x_eval, y_eval) = tf.keras.datasets.mnist.load_data()\n",
      "    train_data = {\"x\": np.expand_dims(x_train, -1), \"y\": y_train}\n",
      "    eval_data = {\"x\": np.expand_dims(x_eval, -1), \"y\": y_eval}\n",
      "    data = {\"train\": train_data, \"eval\": eval_data}\n",
      "    pipeline = fe.Pipeline(batch_size=batch_size, data=data, ops=Minmax(inputs=\"x\", outputs=\"x\"))\n",
      "\n",
      "    # step 2. prepare model\n",
      "    model = fe.FEModel(model_def=LeNet, model_name=\"lenet\", optimizer=\"adam\")\n",
      "    network = fe.Network(\n",
      "        ops=[ModelOp(inputs=\"x\", model=model, outputs=\"y_pred\"), SparseCategoricalCrossentropy(inputs=(\"y\", \"y_pred\"))])\n",
      "\n",
      "    # step 3.prepare estimator\n",
      "    traces = [\n",
      "        Accuracy(true_key=\"y\", pred_key=\"y_pred\", output_name='acc'),\n",
      "        ModelSaver(model_name=\"lenet\", save_dir=model_dir, save_best=True)\n",
      "    ]\n",
      "    estimator = fe.Estimator(network=network, pipeline=pipeline, epochs=epochs, traces=traces)\n",
      "    return estimator\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    est = get_estimator()\n",
      "    est.fit()\n"
     ]
    }
   ],
   "source": [
    "!cat lenet_mnist.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Prepare training and evaluation dataset, create FastEstimator `Pipeline`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Pipeline` can take both data in memory and data in disk. In this example, we are going to use data in memory by loading data with `tf.keras.datasets.mnist`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train image shape is (60000, 28, 28)\n",
      "train label shape is (60000,)\n",
      "eval image shape is (10000, 28, 28)\n",
      "eval label shape is (10000,)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "(x_train, y_train), (x_eval, y_eval) = tf.keras.datasets.mnist.load_data()\n",
    "print(\"train image shape is {}\".format(x_train.shape))\n",
    "print(\"train label shape is {}\".format(y_train.shape))\n",
    "print(\"eval image shape is {}\".format(x_eval.shape))\n",
    "print(\"eval label shape is {}\".format(y_eval.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convolution layer requires channel dimension (batch, height, width, channel), therefore, we need to expand the training image and evaluation image by one dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train image shape is (60000, 28, 28, 1)\n",
      "eval image shape is (10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_eval = np.expand_dims(x_eval, -1)\n",
    "print(\"train image shape is {}\".format(x_train.shape))\n",
    "print(\"eval image shape is {}\".format(x_eval.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For in-memory data in `Pipeline`, the data format should be a nested dictionary like: {\"mode1\": {\"feature1\": numpy_array, \"feature2\": numpy_array, ...}, ...}. Each `mode` can be either `train` or `eval`, in our case, we have both `train` and `eval`.  `feature` is the feature name, in our case, we have `x` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"train\": {\"x\": x_train, \"y\": y_train}, \"eval\": {\"x\": x_eval, \"y\": y_eval}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "#Parameters\n",
    "epochs = 2\n",
    "batch_size = 32\n",
    "steps_per_epoch = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to define `Pipeline`, we want to apply a `Minmax` online preprocessing to the image feature `x` for both training and evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastestimator as fe\n",
    "from fastestimator.op.tensorop import Minmax\n",
    "pipeline = fe.Pipeline(batch_size=batch_size, data=data, ops=Minmax(inputs=\"x\", outputs=\"x\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Prepare model, create FastEstimator `Network`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we have to define the network architecture in `tf.keras.Model` or `tf.keras.Sequential`, for a popular architecture like LeNet, FastEstimator has it implemented already in [fastestimator.architecture.lenet](https://github.com/fastestimator/fastestimator/blob/master/fastestimator/architecture/lenet.py).  After defining the architecture, users are expected to feed the architecture definition and its associated model name, optimizer and loss name (default to be 'loss') to `FEModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastestimator.architecture import LeNet\n",
    "model = fe.build(model_def=LeNet, model_name=\"lenet\", optimizer=\"adam\", loss_name=\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to define the `Network`: given with a batch data with key `x` and `y`, we have to work our way to `loss` with series of operators.  `ModelOp` is an operator that contains a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastestimator.op.tensorop import ModelOp, SparseCategoricalCrossentropy\n",
    "network = fe.Network(ops=[ModelOp(inputs=\"x\", model=model, outputs=\"y_pred\"), \n",
    "                          SparseCategoricalCrossentropy(y_pred=\"y_pred\", y_true=\"y\", outputs=\"loss\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Configure training, create `Estimator`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the training loop, we want to: 1) measure accuracy for data data 2) save the model with lowest valdiation loss. `Trace` class is used for anything related to training loop, we will need to import the `Accuracy` and `ModelSaver` trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "from fastestimator.trace import Accuracy, ModelSaver\n",
    "save_dir = tempfile.mkdtemp()\n",
    "traces = [Accuracy(true_key=\"y\", pred_key=\"y_pred\", output_name='acc'),\n",
    "          ModelSaver(model_name=\"lenet\", save_dir=save_dir, save_best=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define the `Estimator` and specify the training configuation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = fe.Estimator(network=network, pipeline=pipeline, epochs=epochs, \n",
    "                         steps_per_epoch=steps_per_epoch, traces=traces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training\n",
    "training time takes ~1min on CPU of Mac Book Pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ______           __  ______     __  _                 __            \n",
      "   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n",
      "  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n",
      " / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n",
      "/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n",
      "                                                                        \n",
      "\n",
      "FastEstimator-Start: step: 0; total_train_steps: 3750; lenet_lr: 0.001; \n",
      "FastEstimator-Train: step: 0; loss: 2.2939115; \n",
      "FastEstimator-Train: step: 100; loss: 0.3307221; examples/sec: 1999.1; progress: 2.7%; \n",
      "FastEstimator-Train: step: 200; loss: 0.0966952; examples/sec: 2024.1; progress: 5.3%; \n",
      "FastEstimator-Train: step: 300; loss: 0.0663533; examples/sec: 2054.5; progress: 8.0%; \n",
      "FastEstimator-Train: step: 400; loss: 0.3214094; examples/sec: 2040.8; progress: 10.7%; \n",
      "FastEstimator-Train: step: 500; loss: 0.094881; examples/sec: 2036.7; progress: 13.3%; \n",
      "FastEstimator-Train: step: 600; loss: 0.105669; examples/sec: 2012.1; progress: 16.0%; \n",
      "FastEstimator-Train: step: 700; loss: 0.0671014; examples/sec: 1997.2; progress: 18.7%; \n",
      "FastEstimator-Train: step: 800; loss: 0.0074315; examples/sec: 2038.9; progress: 21.3%; \n",
      "FastEstimator-Train: step: 900; loss: 0.1444476; examples/sec: 1907.2; progress: 24.0%; \n",
      "FastEstimator-Train: step: 1000; loss: 0.0217456; examples/sec: 1841.8; progress: 26.7%; \n",
      "FastEstimator-Train: step: 1100; loss: 0.0066336; examples/sec: 1867.2; progress: 29.3%; \n",
      "FastEstimator-Train: step: 1200; loss: 0.1368273; examples/sec: 1947.0; progress: 32.0%; \n",
      "FastEstimator-Train: step: 1300; loss: 0.0055385; examples/sec: 1876.7; progress: 34.7%; \n",
      "FastEstimator-Train: step: 1400; loss: 0.0103834; examples/sec: 1824.9; progress: 37.3%; \n",
      "FastEstimator-Train: step: 1500; loss: 0.0384815; examples/sec: 1706.7; progress: 40.0%; \n",
      "FastEstimator-Train: step: 1600; loss: 0.0244098; examples/sec: 1645.0; progress: 42.7%; \n",
      "FastEstimator-Train: step: 1700; loss: 0.023585; examples/sec: 1730.0; progress: 45.3%; \n",
      "FastEstimator-Train: step: 1800; loss: 0.0234456; examples/sec: 1748.2; progress: 48.0%; \n",
      "FastEstimator-ModelSaver: Saving model to /var/folders/sg/pck0wj4d27j2rwzg88n70trj5ysk83/T/tmppqc833no/lenet_best_loss.h5\n",
      "FastEstimator-Eval: step: 1875; epoch: 0; loss: 0.0418167; min_loss: 0.04181668; since_best_loss: 0; acc: 0.9868790064102564; \n",
      "FastEstimator-Train: step: 1900; loss: 0.0034348; examples/sec: 1614.7; progress: 50.7%; \n",
      "FastEstimator-Train: step: 2000; loss: 0.0102888; examples/sec: 1826.9; progress: 53.3%; \n",
      "FastEstimator-Train: step: 2100; loss: 0.0652192; examples/sec: 1850.4; progress: 56.0%; \n",
      "FastEstimator-Train: step: 2200; loss: 0.0034029; examples/sec: 1848.2; progress: 58.7%; \n",
      "FastEstimator-Train: step: 2300; loss: 0.0094607; examples/sec: 1799.9; progress: 61.3%; \n",
      "FastEstimator-Train: step: 2400; loss: 0.0203875; examples/sec: 1828.7; progress: 64.0%; \n",
      "FastEstimator-Train: step: 2500; loss: 0.1798029; examples/sec: 1922.2; progress: 66.7%; \n",
      "FastEstimator-Train: step: 2600; loss: 0.1138548; examples/sec: 1883.4; progress: 69.3%; \n",
      "FastEstimator-Train: step: 2700; loss: 0.0313583; examples/sec: 1859.7; progress: 72.0%; \n",
      "FastEstimator-Train: step: 2800; loss: 0.0135262; examples/sec: 1924.9; progress: 74.7%; \n",
      "FastEstimator-Train: step: 2900; loss: 0.0261103; examples/sec: 1666.0; progress: 77.3%; \n",
      "FastEstimator-Train: step: 3000; loss: 0.0026995; examples/sec: 1817.5; progress: 80.0%; \n",
      "FastEstimator-Train: step: 3100; loss: 0.0119558; examples/sec: 1816.0; progress: 82.7%; \n",
      "FastEstimator-Train: step: 3200; loss: 0.0848036; examples/sec: 1823.7; progress: 85.3%; \n",
      "FastEstimator-Train: step: 3300; loss: 0.0070706; examples/sec: 1789.6; progress: 88.0%; \n",
      "FastEstimator-Train: step: 3400; loss: 0.0774212; examples/sec: 1741.3; progress: 90.7%; \n",
      "FastEstimator-Train: step: 3500; loss: 0.026064; examples/sec: 1753.5; progress: 93.3%; \n",
      "FastEstimator-Train: step: 3600; loss: 0.0017203; examples/sec: 1708.7; progress: 96.0%; \n",
      "FastEstimator-Train: step: 3700; loss: 0.0020871; examples/sec: 1696.0; progress: 98.7%; \n",
      "FastEstimator-Eval: step: 3750; epoch: 1; loss: 0.0536151; min_loss: 0.04181668; since_best_loss: 1; acc: 0.9822716346153846; \n",
      "FastEstimator-Finish: step: 3750; total_time: 69.29 sec; lenet_lr: 0.001; \n"
     ]
    }
   ],
   "source": [
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferencing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, the model is saved to a temporary folder. we can load the model from file and do inferencing on a sample image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "model_path = os.path.join(save_dir, 'lenet_best_loss.h5')\n",
    "trained_model = tf.keras.models.load_model(model_path, compile=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly get one image from validation set and compare the ground truth with model prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test image idx 6547, ground truth: 4\n",
      "model predicted class is 4\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from fastestimator.interpretation import show_image\n",
    "\n",
    "selected_idx = np.random.randint(10000)\n",
    "print(\"test image idx {}, ground truth: {}\".format(selected_idx, y_eval[selected_idx]))\n",
    "show_image(x_eval[selected_idx])\n",
    "\n",
    "test_image = x_eval[selected_idx]\n",
    "test_image = np.expand_dims(test_image, 0)\n",
    "prediction_score = trained_model.predict(test_image)\n",
    "print(\"model predicted class is {}\".format(np.argmax(prediction_score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
