{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from albumentations import BboxParams\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.python.keras import layers, models, regularizers\n",
    "\n",
    "import fastestimator as fe\n",
    "from fastestimator.backend.to_number import to_number\n",
    "from fastestimator.dataset.data import mscoco\n",
    "from fastestimator.op.numpyop import NumpyOp\n",
    "from fastestimator.op.numpyop.meta import Sometimes\n",
    "from fastestimator.op.numpyop.multivariate import HorizontalFlip, LongestMaxSize, PadIfNeeded\n",
    "from fastestimator.op.numpyop.univariate import Normalize, ReadImage, ToArray\n",
    "from fastestimator.op.tensorop import TensorOp\n",
    "from fastestimator.op.tensorop.model import ModelOp, UpdateOp\n",
    "from fastestimator.trace.adapt import LRScheduler\n",
    "from fastestimator.trace.io import BestModelSaver\n",
    "from fastestimator.trace.metric import MeanAveragePrecision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "epochs = 7 # 12 is the number used in paper\n",
    "max_steps_per_epoch = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('class.json', 'r') as f:\n",
    "    class_map = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading train2017.zip to /home/zymurge/fastestimator_data/MSCOCO2017\n",
      "100% [..............................................]    19336.86 / 19336.86 MB\n",
      "Extracting train2017.zip\n",
      "Downloading val2017.zip to /home/zymurge/fastestimator_data/MSCOCO2017\n",
      "100% [..................................................]    815.59 / 815.59 MB\n",
      "Extracting val2017.zip\n",
      "Downloading annotations_trainval2017.zip to /home/zymurge/fastestimator_data/MSCOCO2017\n",
      "100% [..................................................]    252.91 / 252.91 MB\n",
      "Extracting annotations_trainval2017.zip\n"
     ]
    }
   ],
   "source": [
    "train_ds, eval_ds = mscoco.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(train_ds) == 118287\n",
    "assert len(eval_ds) == 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anchor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_fpn_anchor_box(width: int, height: int):\n",
    "    assert height % 32 == 0 and width % 32 == 0\n",
    "    shapes = [(int(height / 8), int(width / 8))]  # P3\n",
    "    num_pixel = [np.prod(shapes)]\n",
    "    anchor_lengths = [32, 64, 128, 256, 512]\n",
    "    for _ in range(4):  # P4 through P7\n",
    "        shapes.append((int(np.ceil(shapes[-1][0] / 2)), int(np.ceil(shapes[-1][1] / 2))))\n",
    "        num_pixel.append(np.prod(shapes[-1]))\n",
    "    total_num_pixels = np.sum(num_pixel)\n",
    "    anchorbox = np.zeros((9 * total_num_pixels, 4))\n",
    "    anchor_length_multipliers = [2**(0.0), 2**(1 / 3), 2**(2 / 3)]\n",
    "    aspect_ratios = [1.0, 2.0, 0.5]  #x:y\n",
    "    anchor_idx = 0\n",
    "    for shape, anchor_length in zip(shapes, anchor_lengths):\n",
    "        p_h, p_w = shape\n",
    "        base_y = 2**np.ceil(np.log2(height / p_h))\n",
    "        base_x = 2**np.ceil(np.log2(width / p_w))\n",
    "        for i in range(p_h):\n",
    "            center_y = (i + 1 / 2) * base_y\n",
    "            for j in range(p_w):\n",
    "                center_x = (j + 1 / 2) * base_x\n",
    "                for anchor_length_multiplier in anchor_length_multipliers:\n",
    "                    area = (anchor_length * anchor_length_multiplier)**2\n",
    "                    for aspect_ratio in aspect_ratios:\n",
    "                        x1 = center_x - np.sqrt(area * aspect_ratio) / 2\n",
    "                        y1 = center_y - np.sqrt(area / aspect_ratio) / 2\n",
    "                        x2 = center_x + np.sqrt(area * aspect_ratio) / 2\n",
    "                        y2 = center_y + np.sqrt(area / aspect_ratio) / 2\n",
    "                        anchorbox[anchor_idx, 0] = x1\n",
    "                        anchorbox[anchor_idx, 1] = y1\n",
    "                        anchorbox[anchor_idx, 2] = x2 - x1\n",
    "                        anchorbox[anchor_idx, 3] = y2 - y1\n",
    "                        anchor_idx += 1\n",
    "        if p_h == 1 and p_w == 1:  # the next level of 1x1 feature map is still 1x1, therefore ignore\n",
    "            break\n",
    "    return np.float32(anchorbox), np.int32(num_pixel) * 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnchorBox(NumpyOp):\n",
    "    def __init__(self, width, height, inputs, outputs, mode=None):\n",
    "        super().__init__(inputs=inputs, outputs=outputs, mode=mode)\n",
    "        self.anchorbox, _ = _get_fpn_anchor_box(width, height)  # anchorbox is #num_anchor x 4\n",
    "\n",
    "    def forward(self, data, state):\n",
    "        target = self._generate_target(data)  # bbox is #obj x 5\n",
    "        return np.float32(target)\n",
    "\n",
    "    def _generate_target(self, bbox):\n",
    "        object_boxes = bbox[:, :-1]  # num_obj x 4\n",
    "        label = bbox[:, -1]  # num_obj x 1\n",
    "        ious = self._get_iou(object_boxes, self.anchorbox)  # num_obj x num_anchor\n",
    "        #now for each object in image, assign the anchor box with highest iou to them\n",
    "        anchorbox_best_iou_idx = np.argmax(ious, axis=1)\n",
    "        num_obj = ious.shape[0]\n",
    "        for row in range(num_obj):\n",
    "            ious[row, anchorbox_best_iou_idx[row]] = 0.99\n",
    "        #next, begin the anchor box assignment based on iou\n",
    "        anchor_to_obj_idx = np.argmax(ious, axis=0)  # num_anchor x 1\n",
    "        anchor_best_iou = np.max(ious, axis=0)  # num_anchor x 1\n",
    "        cls_gt = np.int32([label[idx] for idx in anchor_to_obj_idx])  # num_anchor x 1\n",
    "        cls_gt[np.where(anchor_best_iou <= 0.4)] = -1  #background class\n",
    "        cls_gt[np.where(np.logical_and(anchor_best_iou > 0.4, anchor_best_iou <= 0.5))] = -2  # ignore these examples\n",
    "        #finally, calculate localization target\n",
    "        single_loc_gt = object_boxes[anchor_to_obj_idx]  # num_anchor x 4\n",
    "        gt_x1, gt_y1, gt_width, gt_height = np.split(single_loc_gt, 4, axis=1)\n",
    "        ac_x1, ac_y1, ac_width, ac_height = np.split(self.anchorbox, 4, axis=1)\n",
    "        dx1 = np.squeeze((gt_x1 - ac_x1) / ac_width)\n",
    "        dy1 = np.squeeze((gt_y1 - ac_y1) / ac_height)\n",
    "        dwidth = np.squeeze(np.log(gt_width / ac_width))\n",
    "        dheight = np.squeeze(np.log(gt_height / ac_height))\n",
    "        return np.array([dx1, dy1, dwidth, dheight, cls_gt]).T  # num_anchor x 5\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_iou(boxes1, boxes2):\n",
    "        \"\"\"Computes the value of intersection over union (IoU) of two array of boxes.\n",
    "        Args:\n",
    "            box1 (array): first boxes in N x 4\n",
    "            box2 (array): second box in M x 4\n",
    "        Returns:\n",
    "            float: IoU value in N x M\n",
    "        \"\"\"\n",
    "        x11, y11, w1, h1 = np.split(boxes1, 4, axis=1)\n",
    "        x21, y21, w2, h2 = np.split(boxes2, 4, axis=1)\n",
    "        x12 = x11 + w1\n",
    "        y12 = y11 + h1\n",
    "        x22 = x21 + w2\n",
    "        y22 = y21 + h2\n",
    "        xmin = np.maximum(x11, np.transpose(x21))\n",
    "        ymin = np.maximum(y11, np.transpose(y21))\n",
    "        xmax = np.minimum(x12, np.transpose(x22))\n",
    "        ymax = np.minimum(y12, np.transpose(y22))\n",
    "        inter_area = np.maximum((xmax - xmin + 1), 0) * np.maximum((ymax - ymin + 1), 0)\n",
    "        area1 = (w1 + 1) * (h1 + 1)\n",
    "        area2 = (w2 + 1) * (h2 + 1)\n",
    "        iou = inter_area / (area1 + area2.T - inter_area)\n",
    "        return iou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = fe.Pipeline(\n",
    "    train_data=train_ds,\n",
    "    eval_data=eval_ds.split(0.01),\n",
    "    batch_size=batch_size,\n",
    "    ops=[\n",
    "        ReadImage(inputs=\"image\", outputs=\"image\"),\n",
    "        LongestMaxSize(512,\n",
    "                       image_in=\"image\",\n",
    "                       image_out=\"image\",\n",
    "                       bbox_in=\"bbox\",\n",
    "                       bbox_out=\"bbox\",\n",
    "                       bbox_params=BboxParams(\"coco\", min_area=1.0)),\n",
    "        PadIfNeeded(\n",
    "            512,\n",
    "            512,\n",
    "            border_mode=cv2.BORDER_CONSTANT,\n",
    "            image_in=\"image\",\n",
    "            image_out=\"image\",\n",
    "            bbox_in=\"bbox\",\n",
    "            bbox_out=\"bbox\",\n",
    "            bbox_params=BboxParams(\"coco\", min_area=1.0),\n",
    "        ),\n",
    "        Sometimes(\n",
    "            HorizontalFlip(mode=\"train\",\n",
    "                           image_in=\"image\",\n",
    "                           image_out=\"image\",\n",
    "                           bbox_in=\"bbox\",\n",
    "                           bbox_out=\"bbox\",\n",
    "                           bbox_params='coco')),\n",
    "        Normalize(inputs=\"image\", outputs=\"image\", mean=1.0, std=1.0, max_pixel_value=127.5),\n",
    "        ToArray(inputs=\"bbox\", outputs=\"bbox\", dtype=\"float32\"),\n",
    "        AnchorBox(inputs=\"bbox\", outputs=\"anchorbox\", width=512, height=512)\n",
    "    ],\n",
    "    pad_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_data = pipeline.get_results(mode='train', num_steps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_index = 6\n",
    "# step_index = 1\n",
    "\n",
    "# img = batch_data[step_index]['image'][batch_index].numpy()\n",
    "# img = ((img + 1)/2 * 255).astype(np.uint8)\n",
    "\n",
    "# keep = batch_data[step_index]['bbox'][batch_index].numpy()[..., -1] > 0\n",
    "# x1, y1, w, h, label = batch_data[step_index]['bbox'][batch_index].numpy()[keep].T\n",
    "# x2 = x1 + w\n",
    "# y2 = y1 + h\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(10, 10))\n",
    "# for j in range(len(x1)):\n",
    "#     cv2.rectangle(img, (x1[j], y1[j]), (x2[j], y2[j]), (0, 0, 255), 2)\n",
    "#     ax.text(x1[j] + 3, y1[j] + 12, class_map[str(int(label[j]))], color=(0, 0, 1), fontsize=14, fontweight='bold')\n",
    "\n",
    "# ax.imshow(img)\n",
    "# print(\"id = {}\".format(batch_data[step_index]['image_id'][batch_index].numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _classification_sub_net(num_classes, num_anchor=9):\n",
    "    model = models.Sequential()\n",
    "    model.add(\n",
    "        layers.Conv2D(256,\n",
    "                      kernel_size=3,\n",
    "                      strides=1,\n",
    "                      padding='same',\n",
    "                      activation='relu',\n",
    "                      kernel_regularizer=regularizers.l2(0.0001),\n",
    "                      kernel_initializer=tf.random_normal_initializer(stddev=0.01)))\n",
    "    model.add(\n",
    "        layers.Conv2D(256,\n",
    "                      kernel_size=3,\n",
    "                      strides=1,\n",
    "                      padding='same',\n",
    "                      activation='relu',\n",
    "                      kernel_regularizer=regularizers.l2(0.0001),\n",
    "                      kernel_initializer=tf.random_normal_initializer(stddev=0.01)))\n",
    "    model.add(\n",
    "        layers.Conv2D(256,\n",
    "                      kernel_size=3,\n",
    "                      strides=1,\n",
    "                      padding='same',\n",
    "                      activation='relu',\n",
    "                      kernel_regularizer=regularizers.l2(0.0001),\n",
    "                      kernel_initializer=tf.random_normal_initializer(stddev=0.01)))\n",
    "    model.add(\n",
    "        layers.Conv2D(256,\n",
    "                      kernel_size=3,\n",
    "                      strides=1,\n",
    "                      padding='same',\n",
    "                      activation='relu',\n",
    "                      kernel_regularizer=regularizers.l2(0.0001),\n",
    "                      kernel_initializer=tf.random_normal_initializer(stddev=0.01)))\n",
    "    model.add(\n",
    "        layers.Conv2D(num_classes * num_anchor,\n",
    "                      kernel_size=3,\n",
    "                      strides=1,\n",
    "                      padding='same',\n",
    "                      activation='sigmoid',\n",
    "                      kernel_regularizer=regularizers.l2(0.0001),\n",
    "                      kernel_initializer=tf.random_normal_initializer(stddev=0.01),\n",
    "                      bias_initializer=tf.initializers.constant(np.log(1 / 99))))\n",
    "    model.add(layers.Reshape((-1, num_classes)))  # the output dimension is [batch, #anchor, #classes]\n",
    "    return model\n",
    "\n",
    "\n",
    "def _regression_sub_net(num_anchor=9):\n",
    "    model = models.Sequential()\n",
    "    model.add(\n",
    "        layers.Conv2D(256,\n",
    "                      kernel_size=3,\n",
    "                      strides=1,\n",
    "                      padding='same',\n",
    "                      activation='relu',\n",
    "                      kernel_regularizer=regularizers.l2(0.0001),\n",
    "                      kernel_initializer=tf.random_normal_initializer(stddev=0.01)))\n",
    "    model.add(\n",
    "        layers.Conv2D(256,\n",
    "                      kernel_size=3,\n",
    "                      strides=1,\n",
    "                      padding='same',\n",
    "                      activation='relu',\n",
    "                      kernel_regularizer=regularizers.l2(0.0001),\n",
    "                      kernel_initializer=tf.random_normal_initializer(stddev=0.01)))\n",
    "    model.add(\n",
    "        layers.Conv2D(256,\n",
    "                      kernel_size=3,\n",
    "                      strides=1,\n",
    "                      padding='same',\n",
    "                      activation='relu',\n",
    "                      kernel_regularizer=regularizers.l2(0.0001),\n",
    "                      kernel_initializer=tf.random_normal_initializer(stddev=0.01)))\n",
    "    model.add(\n",
    "        layers.Conv2D(256,\n",
    "                      kernel_size=3,\n",
    "                      strides=1,\n",
    "                      padding='same',\n",
    "                      activation='relu',\n",
    "                      kernel_regularizer=regularizers.l2(0.0001),\n",
    "                      kernel_initializer=tf.random_normal_initializer(stddev=0.01)))\n",
    "    model.add(\n",
    "        layers.Conv2D(4 * num_anchor,\n",
    "                      kernel_size=3,\n",
    "                      strides=1,\n",
    "                      padding='same',\n",
    "                      kernel_regularizer=regularizers.l2(0.0001),\n",
    "                      kernel_initializer=tf.random_normal_initializer(stddev=0.01)))\n",
    "    model.add(layers.Reshape((-1, 4)))  # the output dimension is [batch, #anchor, 4]\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RetinaNet(input_shape, num_classes, num_anchor=9):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    # FPN\n",
    "    resnet50 = tf.keras.applications.ResNet50(weights=\"imagenet\", include_top=False, input_tensor=inputs, pooling=None)\n",
    "    assert resnet50.layers[80].name == \"conv3_block4_out\"\n",
    "    C3 = resnet50.layers[80].output\n",
    "    assert resnet50.layers[142].name == \"conv4_block6_out\"\n",
    "    C4 = resnet50.layers[142].output\n",
    "    assert resnet50.layers[-1].name == \"conv5_block3_out\"\n",
    "    C5 = resnet50.layers[-1].output\n",
    "    P5 = layers.Conv2D(256, kernel_size=1, strides=1, padding='same', kernel_regularizer=regularizers.l2(0.0001))(C5)\n",
    "    P5_upsampling = layers.UpSampling2D()(P5)\n",
    "    P4 = layers.Conv2D(256, kernel_size=1, strides=1, padding='same', kernel_regularizer=regularizers.l2(0.0001))(C4)\n",
    "    P4 = layers.Add()([P5_upsampling, P4])\n",
    "    P4_upsampling = layers.UpSampling2D()(P4)\n",
    "    P3 = layers.Conv2D(256, kernel_size=1, strides=1, padding='same', kernel_regularizer=regularizers.l2(0.0001))(C3)\n",
    "    P3 = layers.Add()([P4_upsampling, P3])\n",
    "    P6 = layers.Conv2D(256,\n",
    "                       kernel_size=3,\n",
    "                       strides=2,\n",
    "                       padding='same',\n",
    "                       name=\"P6\",\n",
    "                       kernel_regularizer=regularizers.l2(0.0001))(C5)\n",
    "    P7 = layers.Activation('relu')(P6)\n",
    "    P7 = layers.Conv2D(256,\n",
    "                       kernel_size=3,\n",
    "                       strides=2,\n",
    "                       padding='same',\n",
    "                       name=\"P7\",\n",
    "                       kernel_regularizer=regularizers.l2(0.0001))(P7)\n",
    "    P5 = layers.Conv2D(256,\n",
    "                       kernel_size=3,\n",
    "                       strides=1,\n",
    "                       padding='same',\n",
    "                       name=\"P5\",\n",
    "                       kernel_regularizer=regularizers.l2(0.0001))(P5)\n",
    "    P4 = layers.Conv2D(256,\n",
    "                       kernel_size=3,\n",
    "                       strides=1,\n",
    "                       padding='same',\n",
    "                       name=\"P4\",\n",
    "                       kernel_regularizer=regularizers.l2(0.0001))(P4)\n",
    "    P3 = layers.Conv2D(256,\n",
    "                       kernel_size=3,\n",
    "                       strides=1,\n",
    "                       padding='same',\n",
    "                       name=\"P3\",\n",
    "                       kernel_regularizer=regularizers.l2(0.0001))(P3)\n",
    "    # classification subnet\n",
    "    cls_subnet = _classification_sub_net(num_classes=num_classes, num_anchor=num_anchor)\n",
    "    P3_cls = cls_subnet(P3)\n",
    "    P4_cls = cls_subnet(P4)\n",
    "    P5_cls = cls_subnet(P5)\n",
    "    P6_cls = cls_subnet(P6)\n",
    "    P7_cls = cls_subnet(P7)\n",
    "    cls_output = layers.Concatenate(axis=-2)([P3_cls, P4_cls, P5_cls, P6_cls, P7_cls])\n",
    "    # localization subnet\n",
    "    loc_subnet = _regression_sub_net(num_anchor=num_anchor)\n",
    "    P3_loc = loc_subnet(P3)\n",
    "    P4_loc = loc_subnet(P4)\n",
    "    P5_loc = loc_subnet(P5)\n",
    "    P6_loc = loc_subnet(P6)\n",
    "    P7_loc = loc_subnet(P7)\n",
    "    loc_output = layers.Concatenate(axis=-2)([P3_loc, P4_loc, P5_loc, P6_loc, P7_loc])\n",
    "    return tf.keras.Model(inputs=inputs, outputs=[cls_output, loc_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetinaLoss(TensorOp):\n",
    "    def forward(self, data, state):\n",
    "        anchorbox, cls_pred, loc_pred = data\n",
    "        batch_size = anchorbox.shape[0]\n",
    "        focal_loss, l1_loss, total_loss = [], [], []\n",
    "        for idx in range(batch_size):\n",
    "            single_loc_gt, single_cls_gt = anchorbox[idx][:, :-1], tf.cast(anchorbox[idx][:, -1], tf.int32)\n",
    "            single_loc_pred, single_cls_pred = loc_pred[idx], cls_pred[idx]\n",
    "            single_focal_loss, anchor_obj_idx = self.focal_loss(single_cls_gt, single_cls_pred)\n",
    "            single_l1_loss = self.smooth_l1(single_loc_gt, single_loc_pred, anchor_obj_idx)\n",
    "            focal_loss.append(single_focal_loss)\n",
    "            l1_loss.append(single_l1_loss)\n",
    "        focal_loss, l1_loss = tf.reduce_mean(focal_loss), tf.reduce_mean(l1_loss)\n",
    "        total_loss = focal_loss + l1_loss\n",
    "        return total_loss, focal_loss, l1_loss\n",
    "\n",
    "    def focal_loss(self, single_cls_gt, single_cls_pred, alpha=0.25, gamma=2.0):\n",
    "        # single_cls_gt shape: [num_anchor], single_cls_pred shape: [num_anchor, num_class]\n",
    "        num_classes = single_cls_pred.shape[-1]\n",
    "        # gather the objects and background, discard the rest\n",
    "        anchor_obj_idx = tf.where(tf.greater_equal(single_cls_gt, 0))\n",
    "        anchor_obj_bg_idx = tf.where(tf.greater_equal(single_cls_gt, -1))\n",
    "        anchor_obj_count = tf.cast(tf.shape(anchor_obj_idx)[0], tf.float32)\n",
    "        single_cls_gt = tf.one_hot(single_cls_gt, num_classes)\n",
    "        single_cls_gt = tf.gather_nd(single_cls_gt, anchor_obj_bg_idx)\n",
    "        single_cls_pred = tf.gather_nd(single_cls_pred, anchor_obj_bg_idx)\n",
    "        single_cls_gt = tf.reshape(single_cls_gt, (-1, 1))\n",
    "        single_cls_pred = tf.reshape(single_cls_pred, (-1, 1))\n",
    "        # compute the focal weight on each selected anchor box\n",
    "        alpha_factor = tf.ones_like(single_cls_gt) * alpha\n",
    "        alpha_factor = tf.where(tf.equal(single_cls_gt, 1), alpha_factor, 1 - alpha_factor)\n",
    "        focal_weight = tf.where(tf.equal(single_cls_gt, 1), 1 - single_cls_pred, single_cls_pred)\n",
    "        focal_weight = alpha_factor * focal_weight**gamma / anchor_obj_count\n",
    "        cls_loss = tf.losses.BinaryCrossentropy(reduction='sum')(single_cls_gt,\n",
    "                                                                 single_cls_pred,\n",
    "                                                                 sample_weight=focal_weight)\n",
    "        return cls_loss, anchor_obj_idx\n",
    "\n",
    "    def smooth_l1(self, single_loc_gt, single_loc_pred, anchor_obj_idx, beta=0.1):\n",
    "        # single_loc_gt shape: [num_anchor x 4], anchor_obj_idx shape:  [num_anchor x 4]\n",
    "        single_loc_pred = tf.gather_nd(single_loc_pred, anchor_obj_idx)  #anchor_obj_count x 4\n",
    "        single_loc_gt = tf.gather_nd(single_loc_gt, anchor_obj_idx)  #anchor_obj_count x 4\n",
    "        anchor_obj_count = tf.cast(tf.shape(single_loc_pred)[0], tf.float32)\n",
    "        single_loc_gt = tf.reshape(single_loc_gt, (-1, 1))\n",
    "        single_loc_pred = tf.reshape(single_loc_pred, (-1, 1))\n",
    "        loc_diff = tf.abs(single_loc_gt - single_loc_pred)\n",
    "        cond = tf.less(loc_diff, beta)\n",
    "        loc_loss = tf.where(cond, 0.5 * loc_diff**2 / beta, loc_diff - 0.5 * beta)\n",
    "        loc_loss = tf.reduce_sum(loc_loss) / anchor_obj_count\n",
    "        return loc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_fn(step):\n",
    "    if step < 2000:\n",
    "        lr = (0.01 - 0.0002) / 2000 * step + 0.0002\n",
    "    elif step < 120000:\n",
    "        lr = 0.01\n",
    "    elif step < 160000:\n",
    "        lr = 0.001\n",
    "    else:\n",
    "        lr = 0.0001\n",
    "    return lr / 2  # original batch_size 16, for 512 we have batch_size 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/keras-team/keras-applications/releases/download/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94773248/94765736 [==============================] - 9s 0us/step\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    }
   ],
   "source": [
    "model = fe.build(model_fn=lambda: RetinaNet(input_shape=(512, 512, 3), num_classes=90),\n",
    "                 optimizer_fn=lambda: tf.optimizers.SGD(momentum=0.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictBox(TensorOp):\n",
    "    \"\"\"Convert network output to bounding boxes.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 inputs=None,\n",
    "                 outputs=None,\n",
    "                 mode=None,\n",
    "                 input_shape=(512, 512, 3),\n",
    "                 select_top_k=1000,\n",
    "                 nms_max_outputs=100,\n",
    "                 score_threshold=0.05):\n",
    "        super().__init__(inputs=inputs, outputs=outputs, mode=mode)\n",
    "        self.input_shape = input_shape\n",
    "        self.select_top_k = select_top_k\n",
    "        self.nms_max_outputs = nms_max_outputs\n",
    "        self.score_threshold = score_threshold\n",
    "\n",
    "        all_anchors, num_anchors_per_level = _get_fpn_anchor_box(width=input_shape[1], height=input_shape[0])\n",
    "        self.all_anchors = tf.convert_to_tensor(all_anchors)\n",
    "        self.num_anchors_per_level = num_anchors_per_level\n",
    "\n",
    "    def forward(self, data, state):\n",
    "        pred = []\n",
    "\n",
    "        # extract max score and its class label\n",
    "        cls_pred, deltas, bbox = data\n",
    "        batch_size = bbox.shape[0]\n",
    "        labels = tf.cast(tf.argmax(cls_pred, axis=2), dtype=tf.int32)\n",
    "        scores = tf.reduce_max(cls_pred, axis=2)\n",
    "\n",
    "\n",
    "        # iterate over images\n",
    "        for i in range(batch_size):\n",
    "            # split batch into images\n",
    "            labels_per_image = labels[i]\n",
    "            scores_per_image = scores[i]\n",
    "            deltas_per_image = deltas[i]\n",
    "\n",
    "            selected_deltas_per_image = tf.constant([], shape=(0, 4))\n",
    "            selected_labels_per_image = tf.constant([], dtype=tf.int32)\n",
    "            selected_scores_per_image = tf.constant([])\n",
    "            selected_anchor_indices_per_image = tf.constant([], dtype=tf.int32)\n",
    "\n",
    "            end_index = 0\n",
    "            # iterate over each pyramid level\n",
    "            for j in range(self.num_anchors_per_level.shape[0]):\n",
    "                start_index = end_index\n",
    "                end_index += self.num_anchors_per_level[j]\n",
    "                anchor_indices = tf.range(start_index, end_index, dtype=tf.int32)\n",
    "\n",
    "                level_scores = scores_per_image[start_index:end_index]\n",
    "                level_deltas = deltas_per_image[start_index:end_index]\n",
    "                level_labels = labels_per_image[start_index:end_index]\n",
    "\n",
    "                # select top k\n",
    "                if self.num_anchors_per_level[j] >= self.select_top_k:\n",
    "                    top_k = tf.math.top_k(level_scores, self.select_top_k)\n",
    "                    top_k_indices = top_k.indices\n",
    "                else:\n",
    "                    top_k_indices = tf.subtract(anchor_indices, [start_index])\n",
    "\n",
    "                # combine all pyramid levels\n",
    "                selected_deltas_per_image = tf.concat(\n",
    "                    [selected_deltas_per_image, tf.gather(level_deltas, top_k_indices)], axis=0)\n",
    "                selected_scores_per_image = tf.concat(\n",
    "                    [selected_scores_per_image, tf.gather(level_scores, top_k_indices)], axis=0)\n",
    "                selected_labels_per_image = tf.concat(\n",
    "                    [selected_labels_per_image, tf.gather(level_labels, top_k_indices)], axis=0)\n",
    "                selected_anchor_indices_per_image = tf.concat(\n",
    "                    [selected_anchor_indices_per_image, tf.gather(anchor_indices, top_k_indices)], axis=0)\n",
    "\n",
    "            # delta -> (x1, y1, w, h)\n",
    "            selected_anchors_per_image = tf.gather(self.all_anchors, selected_anchor_indices_per_image)\n",
    "            x1 = (selected_deltas_per_image[:, 0] * selected_anchors_per_image[:, 2]) + selected_anchors_per_image[:, 0]\n",
    "            y1 = (selected_deltas_per_image[:, 1] * selected_anchors_per_image[:, 3]) + selected_anchors_per_image[:, 1]\n",
    "            w = tf.math.exp(selected_deltas_per_image[:, 2]) * selected_anchors_per_image[:, 2]\n",
    "            h = tf.math.exp(selected_deltas_per_image[:, 3]) * selected_anchors_per_image[:, 3]\n",
    "            x2 = x1 + w\n",
    "            y2 = y1 + h\n",
    "\n",
    "            # nms\n",
    "            # filter out low score, and perform nms\n",
    "            boxes_per_image = tf.stack([y1, x1, y2, x2], axis=1)\n",
    "            nms_indices = tf.image.non_max_suppression(boxes_per_image,\n",
    "                                                       selected_scores_per_image,\n",
    "                                                       self.nms_max_outputs,\n",
    "                                                       score_threshold=self.score_threshold)\n",
    "\n",
    "            nms_boxes = tf.gather(boxes_per_image, nms_indices)\n",
    "            final_scores = tf.gather(selected_scores_per_image, nms_indices)\n",
    "            final_labels = tf.cast(tf.gather(selected_labels_per_image, nms_indices), dtype=tf.float32)\n",
    "\n",
    "            # clip bounding boxes to image size\n",
    "            x1 = tf.clip_by_value(nms_boxes[:, 1], clip_value_min=0, clip_value_max=self.input_shape[1])\n",
    "            y1 = tf.clip_by_value(nms_boxes[:, 0], clip_value_min=0, clip_value_max=self.input_shape[0])\n",
    "            w = tf.clip_by_value(nms_boxes[:, 3], clip_value_min=0, clip_value_max=self.input_shape[1]) - x1\n",
    "            h = tf.clip_by_value(nms_boxes[:, 2], clip_value_min=0, clip_value_max=self.input_shape[0]) - y1\n",
    "\n",
    "            image_results = tf.stack([x1, y1, w, h, final_labels, final_scores], axis=1)\n",
    "            pred.append(image_results)\n",
    "            \n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = fe.Network(ops=[\n",
    "    ModelOp(model=model, inputs=\"image\", outputs=[\"cls_pred\", \"loc_pred\"]),\n",
    "    RetinaLoss(inputs=[\"anchorbox\", \"cls_pred\", \"loc_pred\"], outputs=[\"total_loss\", \"focal_loss\", \"l1_loss\"]),\n",
    "    UpdateOp(model=model, loss_name=\"total_loss\"),\n",
    "    PredictBox(inputs=[\"cls_pred\", \"loc_pred\", \"bbox\"], outputs=\"pred\", mode=\"eval\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = fe.Estimator(\n",
    "    pipeline=pipeline,\n",
    "    network=network,\n",
    "    epochs=epochs,\n",
    "    max_steps_per_epoch=max_steps_per_epoch,\n",
    "    traces=[\n",
    "        LRScheduler(model=model, lr_fn=lr_fn),\n",
    "        BestModelSaver(model=model, save_dir='./', metric='total_loss', save_best_mode=\"min\"),\n",
    "        MeanAveragePrecision(num_classes=90)\n",
    "    ],\n",
    "    monitor_names=[\"l1_loss\", \"focal_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ______           __  ______     __  _                 __            \n",
      "   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n",
      "  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n",
      " / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n",
      "/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n",
      "                                                                        \n",
      "\n",
      "WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `experimental_run_v2` inside a tf.function to get the best performance.\n",
      "WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `experimental_run_v2` inside a tf.function to get the best performance.\n",
      "FastEstimator-Start: step: 1; model_lr: 0.00010245; \n",
      "INFO:tensorflow:batch_all_reduce: 248 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "FastEstimator-Train: step: 1; l1_loss: 1.2321999; focal_loss: 0.9088831; total_loss: 0.9088678; model_lr: 0.00010245; \n",
      "FastEstimator-Train: step: 1; epoch: 1; epoch_time: 19.83 sec; \n",
      "WARNING:tensorflow:5 out of the last 5 calls to functools.partial(<bound method MirroredExtended._call_for_each_replica of <tensorflow.python.distribute.mirrored_strategy.MirroredExtended object at 0x7fe5e4197c90>>, <function TFNetwork._forward_step_static at 0x7fe454073170>) triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to functools.partial(<bound method MirroredExtended._call_for_each_replica of <tensorflow.python.distribute.mirrored_strategy.MirroredExtended object at 0x7fe5e4197c90>>, <function TFNetwork._forward_step_static at 0x7fe454073170>) triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:7 out of the last 7 calls to functools.partial(<bound method MirroredExtended._call_for_each_replica of <tensorflow.python.distribute.mirrored_strategy.MirroredExtended object at 0x7fe5e4197c90>>, <function TFNetwork._forward_step_static at 0x7fe454073170>) triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:8 out of the last 8 calls to functools.partial(<bound method MirroredExtended._call_for_each_replica of <tensorflow.python.distribute.mirrored_strategy.MirroredExtended object at 0x7fe5e4197c90>>, <function TFNetwork._forward_step_static at 0x7fe454073170>) triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "FastEstimator-ModelSaver: saved model to ./model_best_total_loss.h5\n",
      "FastEstimator-Eval: step: 1; epoch: 1; total_loss: 1.2155781; l1_loss: 1.1893253; focal_loss: 0.8434461; min_total_loss: 1.2155781; since_best: 0; mAP: 0.0; AP50: 0.0; AP75: 0.0; \n",
      "FastEstimator-Train: step: 2; epoch: 2; epoch_time: 4.18 sec; \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-28be9b2c12b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/github/fastestimator/fastestimator/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, summary)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_warmup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/fastestimator/fastestimator/estimator.py\u001b[0m in \u001b[0;36m_start_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    258\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"eval\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_modes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"eval\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mEarlyStop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0;32mpass\u001b[0m  \u001b[0;31m# On early stopping we still want to run the final traces and return results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/fastestimator/fastestimator/estimator.py\u001b[0m in \u001b[0;36m_run_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_traces_on_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_configure_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m                 \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_traces_on_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_idx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_steps_per_epoch\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/fastestimator/fastestimator/network.py\u001b[0m in \u001b[0;36mrun_step\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    417\u001b[0m                     args=(batch_in, self.epoch_state, self.epoch_ops, to_list(self.effective_outputs[mode])))\n\u001b[1;32m    418\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mper_replica_to_global\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m             \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mper_replica_to_global\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"warmup\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/fastestimator/fastestimator/util/util.py\u001b[0m in \u001b[0;36mper_replica_to_global\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m             \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mper_replica_to_global\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/fastestimator/fastestimator/util/util.py\u001b[0m in \u001b[0;36mper_replica_to_global\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDistributedValues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2_pytorch_py37/lib/python3.7/site-packages/tensorflow_core/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2_pytorch_py37/lib/python3.7/site-packages/tensorflow_core/python/ops/math_ops.py\u001b[0m in \u001b[0;36mreduce_mean\u001b[0;34m(input_tensor, axis, keepdims, name)\u001b[0m\n\u001b[1;32m   1909\u001b[0m       \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1910\u001b[0m       gen_math_ops.mean(\n\u001b[0;32m-> 1911\u001b[0;31m           \u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ReductionDims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1912\u001b[0m           name=name))\n\u001b[1;32m   1913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2_pytorch_py37/lib/python3.7/site-packages/tensorflow_core/python/ops/math_ops.py\u001b[0m in \u001b[0;36m_ReductionDims\u001b[0;34m(x, axis, reduction_indices)\u001b[0m\n\u001b[1;32m   1478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1479\u001b[0m     \u001b[0;31m# Otherwise, we rely on Range and Rank to do the right thing at run-time.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1480\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2_pytorch_py37/lib/python3.7/site-packages/tensorflow_core/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2_pytorch_py37/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py\u001b[0m in \u001b[0;36mrank\u001b[0;34m(input, name)\u001b[0m\n\u001b[1;32m    701\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mend_compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m   \"\"\"\n\u001b[0;32m--> 703\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mrank_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2_pytorch_py37/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py\u001b[0m in \u001b[0;36mrank_internal\u001b[0;34m(input, name, optimize)\u001b[0m\n\u001b[1;32m    721\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 723\u001b[0;31m       \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    724\u001b[0m       \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0moptimize\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2_pytorch_py37/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1314\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2_pytorch_py37/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_autopacking_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m   1366\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0minferred_dtype\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_cast_nested_seqs_to_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1368\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_autopacking_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"packed\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2_pytorch_py37/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_autopacking_helper\u001b[0;34m(list_or_tuple, dtype, name)\u001b[0m\n\u001b[1;32m   1272\u001b[0m     \u001b[0;31m# checking.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1273\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dense_tensor_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_or_tuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1274\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_or_tuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1275\u001b[0m   \u001b[0mmust_pack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m   \u001b[0mconverted_elems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2_pytorch_py37/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mpack\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   5682\u001b[0m       _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n\u001b[1;32m   5683\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Pack\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop_callbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5684\u001b[0;31m         values, \"axis\", axis)\n\u001b[0m\u001b[1;32m   5685\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5686\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = './'\n",
    "weights_path = os.path.join(save_dir, \"model1_best_total_loss_20200407.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fe.build(model_fn=lambda: RetinaNet(input_shape=(512, 512, 3), num_classes=90),\n",
    "                 optimizer_fn=None,\n",
    "                 model_names=\"retinanet\",\n",
    "                 weights_path=weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = fe.Network(ops=[\n",
    "    ModelOp(model=model, inputs=\"image\", outputs=[\"cls_pred\", \"loc_pred\"]),\n",
    "    #RetinaLoss(inputs=[\"anchorbox\", \"cls_pred\", \"loc_pred\"], outputs=[\"total_loss\", \"focal_loss\", \"l1_loss\"]),\n",
    "    PredictBox(inputs=[\"cls_pred\", \"loc_pred\", \"bbox\"],\n",
    "               outputs=\"pred\",\n",
    "               mode=\"infer\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = np.random.randint(len(eval_ds), size=batch_size).tolist()\n",
    "print(selected)\n",
    "data = [pipeline.transform(eval_ds[i], mode=\"infer\") for i in selected]\n",
    "im = np.array([item['image'][0] for item in data])\n",
    "pad = max(item['bbox'][0].shape[0] for item in data)\n",
    "bo = np.array([\n",
    "    np.pad(item['bbox'][0], ((0, pad - item['bbox'][0].shape[0]), (0, 0)), mode='constant', constant_values=0)\n",
    "    for item in data\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_out = network.transform({'image': im, 'bbox': bo}, mode=\"infer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# #batch_index = 0\n",
    "# fig, ax = plt.subplots(batch_size, 2, figsize=(20, 60))\n",
    "# for batch_index in range(batch_size):\n",
    "#     img = network_out['image'].numpy()[batch_index, ...]\n",
    "#     img = ((img + 1) / 2 * 255).astype(np.uint8)\n",
    "\n",
    "#     img2 = img.copy()\n",
    "\n",
    "#     keep = network_out['bbox'][batch_index].numpy()[..., -1] > 0\n",
    "#     gt_x1, gt_y1, gt_w, gt_h, gt_label = network_out['bbox'][batch_index].numpy()[keep].T\n",
    "#     gt_x2 = gt_x1 + gt_w\n",
    "#     gt_y2 = gt_y1 + gt_h\n",
    "\n",
    "#     scores = network_out['pred'][batch_index].numpy()[..., -1]\n",
    "#     labels = network_out['pred'][batch_index].numpy()[..., -2]\n",
    "#     keep = scores > 0.5\n",
    "#     x1, y1, w, h, label, _ = network_out['pred'][batch_index].numpy()[keep].T\n",
    "#     x2 = x1 + w\n",
    "#     y2 = y1 + h\n",
    "\n",
    "#     for i in range(len(gt_x1)):\n",
    "#         cv2.rectangle(img, (gt_x1[i], gt_y1[i]), (gt_x2[i], gt_y2[i]), (255, 0, 0), 2)\n",
    "#         ax[batch_index, 0].set_xlabel('GT', fontsize=14, fontweight='bold')\n",
    "#         ax[batch_index, 0].text(gt_x1[i] + 3,\n",
    "#                    gt_y1[i] + 12,\n",
    "#                    class_map[str(int(gt_label[i]))],\n",
    "#                    color=(1, 0, 0),\n",
    "#                    fontsize=14,\n",
    "#                    fontweight='bold')\n",
    "\n",
    "#     for j in range(len(x1)):\n",
    "#         cv2.rectangle(img2, (x1[j], y1[j]), (x2[j], y2[j]), (100, 240, 240), 2)\n",
    "#         ax[batch_index, 1].set_xlabel('Prediction', fontsize=14, fontweight='bold')\n",
    "#         ax[batch_index, 1].text(x1[j] + 3, y1[j] + 12, class_map[str(int(label[j]))], color=(0.5, 0.9, 0.9), fontsize=14, fontweight='bold')\n",
    "\n",
    "#     ax[batch_index, 0].imshow(img)\n",
    "#     ax[batch_index, 1].imshow(img2)\n",
    "# #     print(scores)\n",
    "# #     print(list(map(class_map.get, labels.astype(int).astype(str))))\n",
    "\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_out.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### network_out['bbox']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox = to_number(network_out['bbox'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dimension is `(batch, padded_boxes, [x1, y1, w, h, label])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox[0][:, -1] != 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, item in enumerate(bbox):\n",
    "    assert np.array_equal(bbox[i], bbox.reshape(-1, 5)[bbox.shape[1]*i:bbox.shape[1]*(i+1), ...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_ids = np.expand_dims(np.repeat(range(bbox.shape[0]), bbox.shape[1], axis=None), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _reshape_gt(array: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Reshape ground truth and add local image id within batch.\n",
    "\n",
    "    The ground truth array has shape (batch_size, num_bbox, 5). The 5 is [x1, y1, w, h, label] for each bounding\n",
    "    box. We suppress the padded bounding boxes, and flatten the batch dimwnsion. The output shape is\n",
    "    (batch_size * num_bbox, 6). The 6 is [id_in_batch, x1, y1, w, h, label].\n",
    "\n",
    "    Args:\n",
    "        array: Ground truth with shape (batch_size, num_bbox, 5).\n",
    "\n",
    "    Returns:\n",
    "        Ground truth with shape (batch_size * num_bbox, 6).\n",
    "    \"\"\"\n",
    "    local_ids = np.repeat(range(array.shape[0]), array.shape[1], axis=None)\n",
    "    local_ids = np.expand_dims(local_ids, axis=-1)\n",
    "\n",
    "    gt_with_id = np.concatenate([local_ids, array.reshape(-1, 5)], axis=1)\n",
    "    keep = gt_with_id[..., -1] > 0\n",
    "\n",
    "    return gt_with_id[keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### network_out['pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(network_out['pred'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a list containing `(batch_size)` elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(network_out['pred'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside the list, the dimension is `(up_to_max_det, [x1, y1, w, h, label, score])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = list(map(to_number, network_out['pred']))\n",
    "[item.shape for item in pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_with_id = []\n",
    "for index, item in enumerate(pred):\n",
    "    local_ids = np.repeat([index], item.shape[0], axis=None)\n",
    "    local_ids = np.expand_dims(local_ids, axis=-1)\n",
    "    pred_with_id.append(np.concatenate([local_ids, item], axis=1))\n",
    "    \n",
    "pred_with_id = np.concatenate(pred_with_id, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.concatenate([np.array([[1,2], [5,7]]), np.array([[6,8],[3,4]])], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_with_id[15:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[0][1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _reshape_pred(pred: List[np.ndarray]) -> np.ndarray:\n",
    "    \"\"\"Reshape predicted bounding boxes and add local image id within batch.\n",
    "\n",
    "    The input prediction array is a list of batch_size elements. For each element, it \n",
    "    has shape (num_bbox, 6). The 6 is [x1, y1, w, h, label, score] for each bounding\n",
    "    box. For output we flatten the batch dimension. The output shape is\n",
    "    (batch_size * num_bbox, 6). The 6 is [id_in_batch, x1, y1, w, h, label].\n",
    "\n",
    "    Args:\n",
    "        array: Ground truth with shape (batch_size, num_bbox, 5).\n",
    "\n",
    "    Returns:\n",
    "        Ground truth with shape (batch_size * num_bbox, 6).\n",
    "    \"\"\"\n",
    "    pred_with_id = []\n",
    "    for index, item in enumerate(pred):\n",
    "        local_ids = np.repeat([index], item.shape[0], axis=None)\n",
    "        local_ids = np.expand_dims(local_ids, axis=-1)\n",
    "        pred_with_id.append(np.concatenate([local_ids, item], axis=1))\n",
    "    \n",
    "    pred_with_id = np.concatenate(pred_with_id, axis=0)\n",
    "    return pred_with_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = defaultdict(list)\n",
    "test['a', 'b'].append([30, 40])\n",
    "test['c', 'd'].append(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, item in enumerate(test):\n",
    "    print(i)\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['a', 'b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = {}\n",
    "test2['c', 'd'] = {'x': 5, 'y': 4}\n",
    "test2['a', 'b'] = {'x': 2, 'y': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
