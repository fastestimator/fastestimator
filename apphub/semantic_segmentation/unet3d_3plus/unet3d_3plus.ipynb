{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1bd53eb",
   "metadata": {},
   "source": [
    "# Microscopy Cell Segmentation Using Unet 3D 3plus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0798216",
   "metadata": {},
   "source": [
    "UNet, which is one of deep learning networks with an encoder-decoder architecture, is widely used in medical image segmentation. Combining multi-scale features is one of important factors for accurate segmentation. UNet++ was developed as a modified Unet by designing an architecture with nested and dense skip connections. However, it does not explore sufficient information from full scales and there is still a large room for improvement. \n",
    "\n",
    "[UNET 3plus](https://arxiv.org/abs/2004.08790)  full-scale skip connections convert the inter-connection between the encoder and decoder as well as intra-connection between the decoder sub-networks. Both UNet with plain connections and UNet++ with nested and dense connections are short of exploring sufficient information from full scales, failing to explicitly learn position and boundary of an organ. To remedy the defect in UNet and UNet++, each decoder layer in UNet 3+ incorporates both smaller- and same-scale feature maps from encoder and larger-scale feature maps from decoder, which capturing fine-grained details and coarse-grained semantics in full scales.\n",
    "\n",
    "This example of UNET 3D 3plus to modification of UNET 3plus to segmentation 3D dataset. We are showcasing UNET 3D 3plus to segment [electronic microscopy 3D cell dataset](https://leapmanlab.github.io/dense-cell/). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e81e9d5",
   "metadata": {},
   "source": [
    "## Getting things ready\n",
    "\n",
    "Lets import the necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca50ae75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import fastestimator as fe\n",
    "from fastestimator.dataset.data.em_3d import load_data\n",
    "from fastestimator.op.numpyop import NumpyOp\n",
    "from fastestimator.op.numpyop.meta import Sometimes\n",
    "from fastestimator.op.numpyop.multivariate import HorizontalFlip, Rotate, VerticalFlip\n",
    "from fastestimator.op.numpyop.univariate import ChannelTranspose, Minmax\n",
    "from fastestimator.op.numpyop.univariate.expand_dims import ExpandDims\n",
    "from fastestimator.op.tensorop.loss import CrossEntropy\n",
    "from fastestimator.op.tensorop.model import ModelOp, UpdateOp\n",
    "from fastestimator.op.tensorop.resize3d import Resize3D\n",
    "from fastestimator.trace.adapt import EarlyStopping, ReduceLROnPlateau\n",
    "from fastestimator.trace.io import BestModelSaver\n",
    "from fastestimator.trace.metric import Dice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe98cb17",
   "metadata": {},
   "source": [
    "Next, let's set up some hyperparameters related to the task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a9f09de",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "epochs = 40\n",
    "log_steps = 20\n",
    "height = 256\n",
    "width = 256\n",
    "depth = 24\n",
    "channels = 1\n",
    "num_classes = 7\n",
    "filters = 64\n",
    "learning_rate = 1e-3\n",
    "train_steps_per_epoch = None\n",
    "eval_steps_per_epoch = None\n",
    "save_dir = tempfile.mkdtemp()\n",
    "data_dir = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f45e08f",
   "metadata": {},
   "source": [
    "## Importing Dataset\n",
    "\n",
    "Electronic Microscopy 3D cell dataset, consists of 2 3D images, one 800x800x50 and the other 800x800x24. The 800x800x50 is used as training dataset and 800x800x24 is used for validation. Instead of using the entire 800x800 images, the 800x800x50 is tiled into 256x256x24 tiles with an overlap of 128 producing around 75 training images and similarly the 800x800x24 image is tiled to produce 25 validation images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85bed6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, eval_data = load_data(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a31e67b",
   "metadata": {},
   "source": [
    "Now we have loaded the data and created the samples, let's look at one sample of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92d1ac0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset length is 75\n",
      "evaluation dataset length is 25\n",
      "dataset sample:\n",
      "Image Shape:  (256, 256, 24)\n",
      "Label Shape:  (256, 256, 24)\n"
     ]
    }
   ],
   "source": [
    "print(\"training dataset length is {}\".format(len(train_data)))\n",
    "print(\"evaluation dataset length is {}\".format(len(eval_data)))\n",
    "\n",
    "print(\"dataset sample:\")\n",
    "print(\"Image Shape: \", train_data[0]['image'].shape)\n",
    "print(\"Label Shape: \", train_data[0]['label'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8ab615",
   "metadata": {},
   "source": [
    "The `image`is a 256x256x24 numpy array of uint16.  \n",
    "\n",
    "The `label` is a 256x256x24 encoded(7 classes) numpy array. Semantic label files classify each image voxel into one of seven classes, indexed from 0-6:\n",
    "\n",
    "<table>\n",
    "<tr><td><b>Index</b> </td><td><b>Color </b></td><td><b>Class name</b></td></tr>\n",
    "<tr><td>0 </td><td>None </td><td>Background</td></tr>\n",
    "<tr><td>1 </td><td>Dark Blue </td><td>Cell</td></tr>\n",
    "<tr><td>2 </td><td>Cyan </td><td>Mitochondria</td></tr>\n",
    "<tr><td>3 </td><td>Green </td><td>Alpha granule</td></tr>\n",
    "<tr><td>4 </td><td>Yellow </td><td>Canalicular vessel</td></tr>\n",
    "<tr><td>5 </td><td>Red\tDense </td><td>granule body</td></tr>\n",
    "<tr><td>6 </td><td>Purple </td><td>granule core</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc00fab",
   "metadata": {},
   "source": [
    "## Creating Pipeline\n",
    "\n",
    "Now that both training and validation datasets are created, we use `Pipeline` to define the preprocessing operations:\n",
    "\n",
    "* ClassEncoding is used to one hot encode the encoded label one channel per class(256x256x24x7). \n",
    "\n",
    "\n",
    "* We are using HorizontalFlip, VerticalFlip and Rotate(-10, 10) as our applied data agumentations.\n",
    "\n",
    "\n",
    "* ExpandDims is used to expand the last channel(256x256x24)-> (256x256x24x1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76dcfb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassEncoding(NumpyOp):\n",
    "    \"\"\"\n",
    "    One hot encode the class labels\n",
    "\n",
    "    Args:\n",
    "        inputs: Key(s) of images to be modified.\n",
    "        outputs: Key(s) into which to write the modified images.\n",
    "        no_of_classes: number of classes\n",
    "        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n",
    "            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n",
    "            like \"!infer\" or \"!train\".\n",
    "        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n",
    "            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n",
    "    \"\"\"\n",
    "    def __init__(self, inputs, outputs, no_of_classes: int = 5, mode=None, ds_id=None):\n",
    "        super().__init__(inputs=inputs, outputs=outputs, mode=mode, ds_id=ds_id)\n",
    "        self.no_of_classes = no_of_classes\n",
    "\n",
    "    def forward(self, data, state):\n",
    "        encoded_label = np.zeros(list(data.shape) + [self.no_of_classes])\n",
    "        for i in range(self.no_of_classes):\n",
    "            encoded_label[:, :, :, i] = (data == i).astype(np.uint8)\n",
    "        return np.uint8(encoded_label)\n",
    "\n",
    "pipeline = fe.Pipeline(\n",
    "    train_data=train_data,\n",
    "    eval_data=eval_data,\n",
    "    batch_size=batch_size,\n",
    "    ops=[\n",
    "        Sometimes(numpy_op=HorizontalFlip(image_in=\"image\", mask_in=\"label\", mode='train')),\n",
    "        Sometimes(numpy_op=VerticalFlip(image_in=\"image\", mask_in=\"label\", mode='train')),\n",
    "        Sometimes(numpy_op=Rotate(\n",
    "            image_in=\"image\", mask_in=\"label\", limit=(-10, 10), border_mode=cv2.BORDER_CONSTANT, mode='train')),\n",
    "        ClassEncoding(inputs=\"label\", outputs=\"label\", no_of_classes=num_classes),\n",
    "        Minmax(inputs=\"image\", outputs=\"image\"),\n",
    "        ExpandDims(inputs=\"image\", outputs=\"image\"),\n",
    "        ChannelTranspose(inputs=(\"image\", \"label\"), outputs=(\"image\", \"label\"), axes=(3, 0, 1, 2))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c69d81",
   "metadata": {},
   "source": [
    "# Visualizing Sample Data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001d6779",
   "metadata": {},
   "source": [
    "<img src='https://github.com/fastestimator-util/fastestimator-misc/blob/master/resource/pictures/apphub/unet3d_3plus/sample_input.gif?raw=true' width=\"800\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ece93d",
   "metadata": {},
   "source": [
    "## UNET 3D 3Plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6e16b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.init import kaiming_normal_ as he_normal\n",
    "\n",
    "\n",
    "class StdSingleConvBlock(nn.Module):\n",
    "    \"\"\"A UNet3D StdSingleConvBlock block.\n",
    "\n",
    "    Args:\n",
    "        in_channels: How many channels enter the encoder.\n",
    "        out_channels: How many channels leave the encoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, out_channels: int) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(nn.BatchNorm3d(in_channels),\n",
    "                                    nn.ReLU(inplace=True),\n",
    "                                    nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=\"same\"))\n",
    "\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, nn.Conv3d):\n",
    "                he_normal(layer.weight.data)\n",
    "                layer.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        out = self.layers(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"A UNet3D ConvBlock block.\n",
    "\n",
    "    Args:\n",
    "        in_channels: How many channels enter the encoder.\n",
    "        out_channels: How many channels leave the encoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, out_channels: int) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=\"same\"))\n",
    "\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, nn.Conv3d):\n",
    "                he_normal(layer.weight.data)\n",
    "                layer.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        out = self.layers(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class StdDoubleConvBlock(nn.Module):\n",
    "    \"\"\"A UNet3D StdDoubleConvBlock block.\n",
    "\n",
    "    Args:\n",
    "        in_channels: How many channels enter the encoder.\n",
    "        out_channels: How many channels leave the encoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, out_channels: int) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            StdSingleConvBlock(in_channels, out_channels),\n",
    "            StdSingleConvBlock(out_channels, out_channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        out = self.layers(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class StdConvBlockSkip(nn.Module):\n",
    "    \"\"\"A UNet3D StdConvBlockSkip block skipping batch normalization.\n",
    "\n",
    "    Args:\n",
    "        in_channels: How many channels enter the encoder.\n",
    "        out_channels: How many channels leave the encoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, out_channels: int) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(ConvBlock(in_channels, out_channels),\n",
    "                                    StdSingleConvBlock(out_channels, out_channels))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        out = self.layers(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class UpsampleBlock(nn.Module):\n",
    "    \"\"\"A UNet3D UpsampleBlock block.\n",
    "\n",
    "    Args:\n",
    "        in_channels: How many channels enter the encoder.\n",
    "        out_channels: How many channels leave the encoder.\n",
    "        scale_factor: scale factor to up sample\n",
    "        kernel_size: size of the kernel\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, out_channels: int, scale_factor: int, kernel_size: int = 3) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=scale_factor, mode='trilinear', align_corners=False),\n",
    "            nn.Conv3d(in_channels, out_channels, kernel_size, padding=\"same\"),\n",
    "        )\n",
    "\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, nn.Conv3d):\n",
    "                he_normal(layer.weight.data)\n",
    "                layer.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        out = self.layers(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class DownSampleBlock(nn.Module):\n",
    "    \"\"\"A UNet3D DownSampleBlock block.\n",
    "\n",
    "    Args:\n",
    "        in_channels: How many channels enter the encoder.\n",
    "        out_channels: How many channels leave the encoder.\n",
    "        scale_factor: scale factor to down sample\n",
    "        kernel_size: size of the kernel\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, out_channels: int, scale_factor: int, kernel_size: int = 3) -> None:\n",
    "        super().__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        self.layers = nn.Sequential(nn.Conv3d(in_channels, out_channels, kernel_size=kernel_size, padding=\"same\"))\n",
    "\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, nn.Conv3d):\n",
    "                he_normal(layer.weight.data)\n",
    "                layer.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        out = self.layers(F.max_pool3d(x, self.scale_factor))\n",
    "        return out\n",
    "\n",
    "\n",
    "class UNet3D3Plus(nn.Module):\n",
    "    \"\"\"A Attention UNet3D 3plus implementation in PyTorch.\n",
    "\n",
    "    Args:\n",
    "        input_size: The size of the input tensor (channels, height, width).\n",
    "        output_channel: The number of output channels.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: Length of `input_size` is not 3.\n",
    "        ValueError: `input_size`[1] or `input_size`[2] is not a multiple of 16.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 input_size: Tuple[int, int, int] = (1, 128, 128, 24),\n",
    "                 output_channel: int = 1,\n",
    "                 channels: int = 64) -> None:\n",
    "        UNet3D3Plus._check_input_size(input_size)\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.enc1 = StdConvBlockSkip(in_channels=input_size[0], out_channels=channels)\n",
    "        self.enc2 = StdDoubleConvBlock(in_channels=channels, out_channels=channels * 2)\n",
    "        self.enc3 = StdDoubleConvBlock(in_channels=channels * 2, out_channels=channels * 4)\n",
    "        self.bottle_neck = StdDoubleConvBlock(in_channels=channels * 4, out_channels=channels * 8)\n",
    "\n",
    "        self.up5_4 = UpsampleBlock(in_channels=channels * 8, out_channels=channels, scale_factor=2)\n",
    "        self.up5_3 = ConvBlock(in_channels=channels * 4, out_channels=channels)\n",
    "        self.down5_2 = DownSampleBlock(in_channels=channels * 2, out_channels=channels, scale_factor=2)\n",
    "        self.down5_3 = DownSampleBlock(in_channels=channels, out_channels=channels, scale_factor=4)\n",
    "\n",
    "        self.conv5 = StdSingleConvBlock(in_channels=channels * 4, out_channels=4 * channels)\n",
    "\n",
    "        self.up6_4 = UpsampleBlock(in_channels=channels * 8, out_channels=channels, scale_factor=4)\n",
    "        self.up6_3 = UpsampleBlock(in_channels=channels * 4, out_channels=channels, scale_factor=2)\n",
    "        self.up6_2 = ConvBlock(in_channels=channels * 2, out_channels=channels)\n",
    "        self.down6_1 = DownSampleBlock(in_channels=channels, out_channels=channels, scale_factor=2)\n",
    "\n",
    "        self.conv6 = StdSingleConvBlock(in_channels=channels * 4, out_channels=4 * channels)\n",
    "\n",
    "        self.up7_4 = UpsampleBlock(in_channels=channels * 8, out_channels=channels, scale_factor=8)\n",
    "        self.up7_3 = UpsampleBlock(in_channels=channels * 4, out_channels=channels, scale_factor=4)\n",
    "        self.up7_2 = UpsampleBlock(in_channels=channels * 4, out_channels=channels, scale_factor=2)\n",
    "        self.conv7_1 = ConvBlock(in_channels=channels, out_channels=channels)\n",
    "\n",
    "        self.conv7 = StdSingleConvBlock(in_channels=channels * 4, out_channels=4 * channels)\n",
    "\n",
    "        self.dec1 = nn.Sequential(nn.BatchNorm3d(channels * 4),\n",
    "                                  nn.ReLU(inplace=True),\n",
    "                                  nn.Conv3d(channels * 4, output_channel, 1, padding=\"same\"),\n",
    "                                  nn.Sigmoid())\n",
    "\n",
    "        for layer in self.dec1:\n",
    "            if isinstance(layer, nn.Conv3d):\n",
    "                he_normal(layer.weight.data)\n",
    "                layer.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        conv1 = self.enc1(x)\n",
    "        pool1 = F.max_pool3d(conv1, 2)\n",
    "\n",
    "        conv2 = self.enc2(pool1)\n",
    "        pool2 = F.max_pool3d(conv2, 2)\n",
    "\n",
    "        conv3 = self.enc3(pool2)\n",
    "        pool3 = F.max_pool3d(conv3, 2)\n",
    "\n",
    "        conv4 = self.bottle_neck(pool3)\n",
    "\n",
    "        up5_4 = self.up5_4(conv4)\n",
    "        up5_3 = self.up5_3(conv3)\n",
    "        down5_2 = self.down5_2(conv2)\n",
    "        down5_3 = self.down5_3(conv1)\n",
    "\n",
    "        conv5 = self.conv5(torch.cat((up5_4, up5_3, down5_2, down5_3), 1))\n",
    "\n",
    "        up6_4 = self.up6_4(conv4)\n",
    "        up6_3 = self.up6_3(conv5)\n",
    "        up6_2 = self.up6_2(conv2)\n",
    "        down6_1 = self.down6_1(conv1)\n",
    "\n",
    "        conv6 = self.conv6(torch.cat((up6_4, up6_3, up6_2, down6_1), 1))\n",
    "\n",
    "        up7_4 = self.up7_4(conv4)\n",
    "        up7_3 = self.up7_3(conv5)\n",
    "        up7_2 = self.up7_2(conv6)\n",
    "        conv7_1 = self.conv7_1(conv1)\n",
    "\n",
    "        x_out = self.dec1(self.conv7(torch.cat((up7_4, up7_3, up7_2, conv7_1), 1)))\n",
    "        return x_out\n",
    "\n",
    "    @staticmethod\n",
    "    def _check_input_size(input_size):\n",
    "        if len(input_size) != 4:\n",
    "            raise ValueError(\"Length of `input_size` is not 4 (channel, height, width, depth)\")\n",
    "\n",
    "        _, height, width, depth = input_size\n",
    "\n",
    "        if height < 8 or not (height / 8.0).is_integer() or width < 8 or not (\n",
    "                width / 8.0).is_integer() or depth < 8 or not (depth / 8.0).is_integer():\n",
    "            raise ValueError(\n",
    "                \"All three height, width and depth of input_size need to be multiples of 8 (8, 16, 32, 48...)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56859428",
   "metadata": {},
   "source": [
    "## Network operations during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec2ad60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (height, width, depth)\n",
    "model = fe.build(model_fn=lambda: UNet3D3Plus((channels, ) + input_shape, num_classes, filters),\n",
    "                 optimizer_fn=lambda x: torch.optim.Adam(params=x, lr=learning_rate),\n",
    "                 model_name=\"unet3d_3plus\")\n",
    "\n",
    "network = fe.Network(ops=[\n",
    "    Resize3D(inputs=\"image\", outputs=\"image\", output_shape=input_shape),\n",
    "    Resize3D(inputs=\"label\", outputs=\"label\", output_shape=input_shape, mode='!infer'),\n",
    "    ModelOp(inputs=\"image\", model=model, outputs=\"pred_segment\"),\n",
    "    CrossEntropy(inputs=(\"pred_segment\", \"label\"), outputs=\"ce_loss\", form=\"binary\"),\n",
    "    UpdateOp(model=model, loss_name=\"ce_loss\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefc3c2c",
   "metadata": {},
   "source": [
    "## Training loop and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3468ab66",
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = [\n",
    "    Dice(true_key=\"label\", pred_key=\"pred_segment\"),\n",
    "    ReduceLROnPlateau(model=model, metric=\"Dice\", patience=4, factor=0.5, best_mode=\"max\"),\n",
    "    BestModelSaver(model=model, save_dir=save_dir, metric='Dice', save_best_mode='max'),\n",
    "    EarlyStopping(monitor=\"Dice\", compare='max', min_delta=0.005, patience=6),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdefce69",
   "metadata": {},
   "source": [
    "## Let's start training\n",
    "\n",
    "The training requires 10 epochs, and the total training time is around 28 mins hours on single Nvidia A100 (32G) GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c0bd1f4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ______           __  ______     __  _                 __            \n",
      "   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n",
      "  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n",
      " / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n",
      "/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n",
      "                                                                        \n",
      "\n",
      "FastEstimator-Start: step: 1; logging_interval: 20; num_device: 1;\n",
      "FastEstimator-Train: step: 1; ce_loss: 0.860717;\n",
      "FastEstimator-Train: step: 20; ce_loss: 0.30017927; steps/sec: 1.36;\n",
      "FastEstimator-Train: step: 40; ce_loss: 0.14708246; steps/sec: 1.31;\n",
      "FastEstimator-Train: step: 60; ce_loss: 0.20333658; steps/sec: 1.31;\n",
      "FastEstimator-Train: step: 75; epoch: 1; epoch_time: 63.65 sec;\n",
      "Eval Progress: 1/25;\n",
      "Eval Progress: 8/25; steps/sec: 2.53;\n",
      "Eval Progress: 16/25; steps/sec: 2.9;\n",
      "Eval Progress: 25/25; steps/sec: 2.71;\n",
      "FastEstimator-BestModelSaver: Saved model to /tmp/tmpadqzggmz/unet3d_3plus_best_Dice.pt\n",
      "FastEstimator-Eval: step: 75; epoch: 1; ce_loss: 0.113719575; Dice: 0.880454; max_Dice: 0.880454; since_best_Dice: 0;\n",
      "FastEstimator-Train: step: 80; ce_loss: 0.15428557; steps/sec: 0.91;\n",
      "FastEstimator-Train: step: 100; ce_loss: 0.09477081; steps/sec: 1.31;\n",
      "FastEstimator-Train: step: 120; ce_loss: 0.104801044; steps/sec: 1.3;\n",
      "FastEstimator-Train: step: 140; ce_loss: 0.094412394; steps/sec: 1.31;\n",
      "FastEstimator-Train: step: 150; epoch: 2; epoch_time: 63.92 sec;\n",
      "Eval Progress: 1/25;\n",
      "Eval Progress: 8/25; steps/sec: 2.6;\n",
      "Eval Progress: 16/25; steps/sec: 3.24;\n",
      "Eval Progress: 25/25; steps/sec: 3.22;\n",
      "FastEstimator-BestModelSaver: Saved model to /tmp/tmpadqzggmz/unet3d_3plus_best_Dice.pt\n",
      "FastEstimator-Eval: step: 150; epoch: 2; ce_loss: 0.111118965; Dice: 0.88797957; max_Dice: 0.88797957; since_best_Dice: 0;\n",
      "FastEstimator-Train: step: 160; ce_loss: 0.09105138; steps/sec: 0.92;\n",
      "FastEstimator-Train: step: 180; ce_loss: 0.16602638; steps/sec: 1.31;\n",
      "FastEstimator-Train: step: 200; ce_loss: 0.10248997; steps/sec: 1.31;\n",
      "FastEstimator-Train: step: 220; ce_loss: 0.08631318; steps/sec: 1.31;\n",
      "FastEstimator-Train: step: 225; epoch: 3; epoch_time: 63.79 sec;\n",
      "Eval Progress: 1/25;\n",
      "Eval Progress: 8/25; steps/sec: 2.24;\n",
      "Eval Progress: 16/25; steps/sec: 3.23;\n",
      "Eval Progress: 25/25; steps/sec: 3.19;\n",
      "FastEstimator-Eval: step: 225; epoch: 3; ce_loss: 0.10988836; Dice: 0.8545232; max_Dice: 0.88797957; since_best_Dice: 1;\n",
      "FastEstimator-Train: step: 240; ce_loss: 0.07501848; steps/sec: 0.89;\n",
      "FastEstimator-Train: step: 260; ce_loss: 0.06222808; steps/sec: 1.31;\n",
      "FastEstimator-Train: step: 280; ce_loss: 0.055759806; steps/sec: 1.31;\n",
      "FastEstimator-Train: step: 300; ce_loss: 0.117641486; steps/sec: 1.31;\n",
      "FastEstimator-Train: step: 300; epoch: 4; epoch_time: 64.4 sec;\n",
      "Eval Progress: 1/25;\n",
      "Eval Progress: 8/25; steps/sec: 2.43;\n",
      "Eval Progress: 16/25; steps/sec: 3.1;\n",
      "Eval Progress: 25/25; steps/sec: 3.21;\n",
      "FastEstimator-BestModelSaver: Saved model to /tmp/tmpadqzggmz/unet3d_3plus_best_Dice.pt\n",
      "FastEstimator-Eval: step: 300; epoch: 4; ce_loss: 0.08959633; Dice: 0.88925904; max_Dice: 0.88925904; since_best_Dice: 0;\n",
      "FastEstimator-Train: step: 320; ce_loss: 0.11893127; steps/sec: 0.92;\n",
      "FastEstimator-Train: step: 340; ce_loss: 0.06323377; steps/sec: 1.3;\n",
      "FastEstimator-Train: step: 360; ce_loss: 0.0690563; steps/sec: 1.31;\n",
      "FastEstimator-Train: step: 375; epoch: 5; epoch_time: 63.95 sec;\n",
      "Eval Progress: 1/25;\n",
      "Eval Progress: 8/25; steps/sec: 2.33;\n",
      "Eval Progress: 16/25; steps/sec: 3.03;\n",
      "Eval Progress: 25/25; steps/sec: 3.16;\n",
      "FastEstimator-Eval: step: 375; epoch: 5; ce_loss: 0.115550846; Dice: 0.8676957; max_Dice: 0.88925904; since_best_Dice: 1;\n",
      "FastEstimator-Train: step: 380; ce_loss: 0.06684955; steps/sec: 0.96;\n",
      "FastEstimator-Train: step: 400; ce_loss: 0.06854059; steps/sec: 1.3;\n",
      "FastEstimator-Train: step: 420; ce_loss: 0.0912697; steps/sec: 1.31;\n",
      "FastEstimator-Train: step: 440; ce_loss: 0.0863537; steps/sec: 1.31;\n",
      "FastEstimator-Train: step: 450; epoch: 6; epoch_time: 63.52 sec;\n",
      "Eval Progress: 1/25;\n",
      "Eval Progress: 8/25; steps/sec: 1.99;\n",
      "Eval Progress: 16/25; steps/sec: 3.24;\n",
      "Eval Progress: 25/25; steps/sec: 3.27;\n",
      "FastEstimator-Eval: step: 450; epoch: 6; ce_loss: 0.1058943; Dice: 0.8644516; max_Dice: 0.88925904; since_best_Dice: 2;\n",
      "FastEstimator-Train: step: 460; ce_loss: 0.067437865; steps/sec: 0.91;\n",
      "FastEstimator-Train: step: 480; ce_loss: 0.05827997; steps/sec: 1.31;\n",
      "FastEstimator-Train: step: 500; ce_loss: 0.07105688; steps/sec: 1.31;\n",
      "FastEstimator-Train: step: 520; ce_loss: 0.04780322; steps/sec: 1.31;\n",
      "FastEstimator-Train: step: 525; epoch: 7; epoch_time: 63.56 sec;\n",
      "Eval Progress: 1/25;\n",
      "Eval Progress: 8/25; steps/sec: 2.46;\n",
      "Eval Progress: 16/25; steps/sec: 3.29;\n",
      "Eval Progress: 25/25; steps/sec: 3.32;\n",
      "FastEstimator-Eval: step: 525; epoch: 7; ce_loss: 0.095474005; Dice: 0.88438624; max_Dice: 0.88925904; since_best_Dice: 3;\n",
      "FastEstimator-Train: step: 540; ce_loss: 0.0677171; steps/sec: 0.91;\n",
      "FastEstimator-Train: step: 560; ce_loss: 0.068075724; steps/sec: 1.31;\n",
      "FastEstimator-Train: step: 580; ce_loss: 0.07171143; steps/sec: 1.31;\n",
      "FastEstimator-Train: step: 600; ce_loss: 0.098997325; steps/sec: 1.31;\n",
      "FastEstimator-Train: step: 600; epoch: 8; epoch_time: 63.88 sec;\n",
      "Eval Progress: 1/25;\n",
      "Eval Progress: 8/25; steps/sec: 2.56;\n",
      "Eval Progress: 16/25; steps/sec: 3.33;\n",
      "Eval Progress: 25/25; steps/sec: 3.35;\n",
      "FastEstimator-ReduceLROnPlateau: learning rate reduced to 0.0005000000237487257\n",
      "FastEstimator-Eval: step: 600; epoch: 8; ce_loss: 0.15154563; Dice: 0.77215034; max_Dice: 0.88925904; since_best_Dice: 4; unet3d_3plus_lr: 0.0005;\n",
      "FastEstimator-Train: step: 620; ce_loss: 0.042999014; steps/sec: 0.96;\n",
      "FastEstimator-Train: step: 640; ce_loss: 0.08237079; steps/sec: 1.31;\n",
      "FastEstimator-Train: step: 660; ce_loss: 0.054438036; steps/sec: 1.31;\n",
      "FastEstimator-Train: step: 675; epoch: 9; epoch_time: 62.93 sec;\n",
      "Eval Progress: 1/25;\n",
      "Eval Progress: 8/25; steps/sec: 2.49;\n",
      "Eval Progress: 16/25; steps/sec: 3.26;\n",
      "Eval Progress: 25/25; steps/sec: 3.29;\n",
      "FastEstimator-Eval: step: 675; epoch: 9; ce_loss: 0.09528239; Dice: 0.88113344; max_Dice: 0.88925904; since_best_Dice: 5;\n",
      "FastEstimator-Train: step: 680; ce_loss: 0.06399549; steps/sec: 0.92;\n",
      "FastEstimator-Train: step: 700; ce_loss: 0.076678984; steps/sec: 1.31;\n",
      "FastEstimator-Train: step: 720; ce_loss: 0.052137267; steps/sec: 1.31;\n",
      "FastEstimator-Train: step: 740; ce_loss: 0.051360596; steps/sec: 1.31;\n",
      "FastEstimator-Train: step: 750; epoch: 10; epoch_time: 63.68 sec;\n",
      "Eval Progress: 1/25;\n",
      "Eval Progress: 8/25; steps/sec: 2.49;\n",
      "Eval Progress: 16/25; steps/sec: 3.24;\n",
      "Eval Progress: 25/25; steps/sec: 3.24;\n",
      "FastEstimator-Eval: step: 750; epoch: 10; ce_loss: 0.08994463; Dice: 0.88465273; max_Dice: 0.88925904; since_best_Dice: 6;\n",
      "FastEstimator-Train: step: 760; ce_loss: 0.03805439; steps/sec: 0.91;\n",
      "FastEstimator-Train: step: 780; ce_loss: 0.05151623; steps/sec: 1.31;\n",
      "FastEstimator-Train: step: 800; ce_loss: 0.05967456; steps/sec: 1.3;\n",
      "FastEstimator-Train: step: 820; ce_loss: 0.040179186; steps/sec: 1.31;\n",
      "FastEstimator-Train: step: 825; epoch: 11; epoch_time: 64.15 sec;\n",
      "Eval Progress: 1/25;\n",
      "Eval Progress: 8/25; steps/sec: 1.68;\n",
      "Eval Progress: 16/25; steps/sec: 3.32;\n",
      "Eval Progress: 25/25; steps/sec: 3.32;\n",
      "FastEstimator-Eval: step: 825; epoch: 11; ce_loss: 0.09308983; Dice: 0.8852134; max_Dice: 0.88925904; since_best_Dice: 7;\n",
      "FastEstimator-Train: step: 840; ce_loss: 0.07429245; steps/sec: 0.86;\n",
      "FastEstimator-Train: step: 860; ce_loss: 0.05145695; steps/sec: 1.31;\n",
      "FastEstimator-Train: step: 880; ce_loss: 0.062384356; steps/sec: 1.31;\n",
      "FastEstimator-Train: step: 900; ce_loss: 0.054897696; steps/sec: 1.31;\n",
      "FastEstimator-Train: step: 900; epoch: 12; epoch_time: 65.03 sec;\n",
      "Eval Progress: 1/25;\n",
      "Eval Progress: 8/25; steps/sec: 2.07;\n",
      "Eval Progress: 16/25; steps/sec: 2.76;\n",
      "Eval Progress: 25/25; steps/sec: 2.88;\n",
      "FastEstimator-BestModelSaver: Saved model to /tmp/tmpadqzggmz/unet3d_3plus_best_Dice.pt\n",
      "FastEstimator-Eval: step: 900; epoch: 12; ce_loss: 0.087427914; Dice: 0.89036316; max_Dice: 0.89036316; since_best_Dice: 0;\n",
      "FastEstimator-Train: step: 920; ce_loss: 0.06436136; steps/sec: 0.94;\n",
      "FastEstimator-Train: step: 940; ce_loss: 0.05660579; steps/sec: 1.3;\n",
      "FastEstimator-Train: step: 960; ce_loss: 0.08483134; steps/sec: 1.31;\n",
      "FastEstimator-Train: step: 975; epoch: 13; epoch_time: 63.52 sec;\n",
      "Eval Progress: 1/25;\n",
      "Eval Progress: 8/25; steps/sec: 2.77;\n",
      "Eval Progress: 16/25; steps/sec: 3.29;\n",
      "Eval Progress: 25/25; steps/sec: 3.28;\n",
      "FastEstimator-BestModelSaver: Saved model to /tmp/tmpadqzggmz/unet3d_3plus_best_Dice.pt\n",
      "FastEstimator-Eval: step: 975; epoch: 13; ce_loss: 0.08438554; Dice: 0.8931165; max_Dice: 0.8931165; since_best_Dice: 0;\n",
      "FastEstimator-Train: step: 980; ce_loss: 0.070409335; steps/sec: 0.92;\n",
      "FastEstimator-Train: step: 1000; ce_loss: 0.062857024; steps/sec: 1.31;\n",
      "FastEstimator-Train: step: 1020; ce_loss: 0.05621997; steps/sec: 1.31;\n",
      "FastEstimator-Train: step: 1040; ce_loss: 0.057092488; steps/sec: 1.31;\n",
      "FastEstimator-Train: step: 1050; epoch: 14; epoch_time: 63.74 sec;\n",
      "Eval Progress: 1/25;\n",
      "Eval Progress: 8/25; steps/sec: 2.37;\n",
      "Eval Progress: 16/25; steps/sec: 3.33;\n",
      "Eval Progress: 25/25; steps/sec: 3.34;\n",
      "FastEstimator-Eval: step: 1050; epoch: 14; ce_loss: 0.08777376; Dice: 0.8887184; max_Dice: 0.8931165; since_best_Dice: 1;\n",
      "FastEstimator-Train: step: 1060; ce_loss: 0.05754202; steps/sec: 0.95;\n",
      "FastEstimator-Train: step: 1080; ce_loss: 0.04412094; steps/sec: 1.31;\n",
      "FastEstimator-Train: step: 1100; ce_loss: 0.05394041; steps/sec: 1.3;\n",
      "FastEstimator-Train: step: 1120; ce_loss: 0.05314338; steps/sec: 1.31;\n",
      "FastEstimator-Train: step: 1125; epoch: 15; epoch_time: 63.17 sec;\n",
      "Eval Progress: 1/25;\n",
      "Eval Progress: 8/25; steps/sec: 2.73;\n",
      "Eval Progress: 16/25; steps/sec: 3.3;\n",
      "Eval Progress: 25/25; steps/sec: 3.34;\n",
      "FastEstimator-Eval: step: 1125; epoch: 15; ce_loss: 0.100568116; Dice: 0.8811522; max_Dice: 0.8931165; since_best_Dice: 2;\n",
      "FastEstimator-Train: step: 1140; ce_loss: 0.035860907; steps/sec: 0.94;\n",
      "FastEstimator-Train: step: 1160; ce_loss: 0.049917944; steps/sec: 1.31;\n",
      "FastEstimator-Train: step: 1180; ce_loss: 0.05748305; steps/sec: 1.31;\n",
      "FastEstimator-Train: step: 1200; ce_loss: 0.040020917; steps/sec: 1.31;\n",
      "FastEstimator-Train: step: 1200; epoch: 16; epoch_time: 63.34 sec;\n",
      "Eval Progress: 1/25;\n",
      "Eval Progress: 8/25; steps/sec: 2.33;\n",
      "Eval Progress: 16/25; steps/sec: 3.15;\n",
      "Eval Progress: 25/25; steps/sec: 3.17;\n",
      "FastEstimator-BestModelSaver: Saved model to /tmp/tmpadqzggmz/unet3d_3plus_best_Dice.pt\n",
      "FastEstimator-Eval: step: 1200; epoch: 16; ce_loss: 0.08662397; Dice: 0.8979423; max_Dice: 0.8979423; since_best_Dice: 0;\n",
      "FastEstimator-Train: step: 1220; ce_loss: 0.05159781; steps/sec: 0.88;\n",
      "FastEstimator-Train: step: 1240; ce_loss: 0.07497774; steps/sec: 1.31;\n",
      "FastEstimator-Train: step: 1260; ce_loss: 0.050315596; steps/sec: 1.31;\n",
      "FastEstimator-Train: step: 1275; epoch: 17; epoch_time: 64.93 sec;\n",
      "Eval Progress: 1/25;\n",
      "Eval Progress: 8/25; steps/sec: 2.47;\n",
      "Eval Progress: 16/25; steps/sec: 3.26;\n",
      "Eval Progress: 25/25; steps/sec: 3.26;\n",
      "FastEstimator-BestModelSaver: Saved model to /tmp/tmpadqzggmz/unet3d_3plus_best_Dice.pt\n",
      "FastEstimator-Eval: step: 1275; epoch: 17; ce_loss: 0.083677255; Dice: 0.89819264; max_Dice: 0.89819264; since_best_Dice: 0;\n",
      "FastEstimator-Train: step: 1280; ce_loss: 0.048477802; steps/sec: 0.94;\n",
      "FastEstimator-Train: step: 1300; ce_loss: 0.046098597; steps/sec: 1.31;\n",
      "FastEstimator-Train: step: 1320; ce_loss: 0.04713911; steps/sec: 1.31;\n",
      "FastEstimator-Train: step: 1340; ce_loss: 0.04125367; steps/sec: 1.31;\n",
      "FastEstimator-Train: step: 1350; epoch: 18; epoch_time: 63.22 sec;\n",
      "Eval Progress: 1/25;\n",
      "Eval Progress: 8/25; steps/sec: 1.98;\n",
      "Eval Progress: 16/25; steps/sec: 3.27;\n",
      "Eval Progress: 25/25; steps/sec: 3.3;\n",
      "FastEstimator-Eval: step: 1350; epoch: 18; ce_loss: 0.09032212; Dice: 0.890743; max_Dice: 0.89819264; since_best_Dice: 1;\n",
      "FastEstimator-Train: step: 1360; ce_loss: 0.03792895; steps/sec: 0.92;\n",
      "FastEstimator-Train: step: 1380; ce_loss: 0.05078878; steps/sec: 1.31;\n",
      "FastEstimator-Train: step: 1400; ce_loss: 0.039842892; steps/sec: 1.31;\n",
      "FastEstimator-Train: step: 1420; ce_loss: 0.032456607; steps/sec: 1.31;\n",
      "FastEstimator-Train: step: 1425; epoch: 19; epoch_time: 63.76 sec;\n",
      "Eval Progress: 1/25;\n",
      "Eval Progress: 8/25; steps/sec: 2.46;\n",
      "Eval Progress: 16/25; steps/sec: 3.25;\n",
      "Eval Progress: 25/25; steps/sec: 3.25;\n",
      "FastEstimator-Eval: step: 1425; epoch: 19; ce_loss: 0.086813554; Dice: 0.8951727; max_Dice: 0.89819264; since_best_Dice: 2;\n",
      "FastEstimator-Train: step: 1440; ce_loss: 0.03859105; steps/sec: 0.92;\n",
      "FastEstimator-Train: step: 1460; ce_loss: 0.038506884; steps/sec: 1.31;\n",
      "FastEstimator-Train: step: 1480; ce_loss: 0.044680107; steps/sec: 1.31;\n",
      "FastEstimator-Train: step: 1500; ce_loss: 0.036222447; steps/sec: 1.31;\n",
      "FastEstimator-Train: step: 1500; epoch: 20; epoch_time: 63.66 sec;\n",
      "Eval Progress: 1/25;\n",
      "Eval Progress: 8/25; steps/sec: 2.54;\n",
      "Eval Progress: 16/25; steps/sec: 3.36;\n",
      "Eval Progress: 25/25; steps/sec: 3.38;\n",
      "FastEstimator-Eval: step: 1500; epoch: 20; ce_loss: 0.09286546; Dice: 0.89298964; max_Dice: 0.89819264; since_best_Dice: 3;\n",
      "FastEstimator-Finish: step: 1500; total_time: 1580.35 sec; unet3d_3plus_lr: 0.0005;\n"
     ]
    }
   ],
   "source": [
    "estimator = fe.Estimator(network=network,\n",
    "                         pipeline=pipeline,\n",
    "                         epochs=epochs,\n",
    "                         log_steps=log_steps,\n",
    "                         traces=traces,\n",
    "                         train_steps_per_epoch=train_steps_per_epoch,\n",
    "                         eval_steps_per_epoch=eval_steps_per_epoch)\n",
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfce1ec",
   "metadata": {},
   "source": [
    "## Inferencing\n",
    "\n",
    "After training the network, let's inference our trained model and visualize their results in comparison to the ground truth. For visualization, we will use validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14609fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_val_batch = pipeline.get_results(mode=\"eval\", shuffle=True)\n",
    "sample_val_batch = network.transform(data=sample_val_batch, mode=\"eval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fe9683",
   "metadata": {},
   "source": [
    "## Visualizing sample segmentation prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ee07e4",
   "metadata": {},
   "source": [
    "<img src='https://github.com/fastestimator-util/fastestimator-misc/blob/master/resource/pictures/apphub/unet3d_3plus/sample_output.gif?raw=true' width=\"1200\" height=\"500\">"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "interpreter": {
   "hash": "73b4b63fab75275cc22557d8583617f6e244a28eab482e785f2d595a4709032f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
