{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Named Entity Recognition using BERT Fine tuning</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the downstream NLP tasks such as question answering, named entity recognition, and language inference, pre-trained word representations tend to perform better. BERT which fine tunes deep bi-directional representation on series of tasks achieves state-of-the-art results. Unlike traditional Tranformer, BERT is trained on “masked language modeling,” which means that it is allowed to see the whole sentence and does not limit the context it can take into account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we are leveraging transformers library to load BERT model and other config files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import os\n",
    "import numpy as np\n",
    "from typing import Callable, Iterable, List, Union\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "import fastestimator as fe\n",
    "from fastestimator.dataset.data import german_ner\n",
    "from fastestimator.op.numpyop.numpyop import NumpyOp\n",
    "from fastestimator.op.numpyop.univariate import PadSequence, Tokenize, WordtoId\n",
    "from fastestimator.op.tensorop import TensorOp\n",
    "from fastestimator.op.tensorop.loss import CrossEntropy\n",
    "from fastestimator.op.tensorop.model import ModelOp, UpdateOp\n",
    "from fastestimator.trace.metric import Accuracy\n",
    "from fastestimator.trace.io import BestModelSaver\n",
    "from fastestimator.backend import feed_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 20\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "save_dir=tempfile.mkdtemp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReshapeOp defines the reshape operation that is performed on prediction and ground truth before passing them to loss calculation.<br>\n",
    "For example, prediction shape [batch_size, sequence_length, num_classes] -> [batch_size * sequence_length, num_classes]<br>\n",
    "ground truth shape [batch_size, sequence_length] -> [batch_size * sequence_length,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReshapeOp(TensorOp):\n",
    "    def __init__(self,\n",
    "                 inputs: Union[None, str, Iterable[str], Callable] = None,\n",
    "                 outputs: Union[None, str, Iterable[str]] = None,\n",
    "                 mode: Union[None, str, Iterable[str]] = \"!infer\"):\n",
    "        super().__init__(inputs=inputs, outputs=outputs, mode=mode)\n",
    "\n",
    "    def forward(self, data, state):\n",
    "        inp_shape = data.get_shape()\n",
    "        if tf.keras.backend.ndim(data) < 3:\n",
    "            return tf.reshape(data, [\n",
    "                inp_shape[0] * inp_shape[1],\n",
    "            ])\n",
    "        else:\n",
    "            return tf.reshape(data, [inp_shape[0] * inp_shape[1], inp_shape[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom NumpyOp that constructs attention masks for input sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionMask(NumpyOp):\n",
    "    def forward(self, data, state):\n",
    "        masks = [float(i > 0) for i in data]\n",
    "        return np.array(masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "char2idx function creates look-up table for the corresponding ids for the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char2idx(data):\n",
    "    tag2idx = {t: i for i, t in enumerate(data)}\n",
    "    return tag2idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Building components</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step 1: Prepare training & evaluation data and define pipeline</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NER dataset from GermEval contains sequences and entity tags from german wikipedia and news corpora. We are loading train and eval sequences dataset along with data and label vocabulary. For this example other nouns are omitted for the simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, eval_data, data_vocab, label_vocab = german_ner.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a pipeline to tokenize and pad the input sequences and construct attention masks. Attention masks are used to avoid performing attention operation on padded tokens. We are using BERT tokenizer for input sequences tokenization and max length 50 for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "tag2idx = char2idx(label_vocab)\n",
    "pipeline = fe.Pipeline(\n",
    "    train_data=train_data,\n",
    "    eval_data=eval_data,\n",
    "    batch_size=batch_size,\n",
    "    ops=[\n",
    "        Tokenize(inputs=\"x\", outputs=\"x\", tokenize_fn=tokenizer.tokenize),\n",
    "        WordtoId(inputs=\"x\", outputs=\"x\", mapping=tokenizer.convert_tokens_to_ids),\n",
    "        WordtoId(inputs=\"y\", outputs=\"y\", mapping=tag2idx),\n",
    "        PadSequence(max_len=max_len, inputs=\"x\", outputs=\"x\"),\n",
    "        PadSequence(max_len=max_len, value=len(tag2idx), inputs=\"y\", outputs=\"y\"),\n",
    "        AttentionMask(inputs=\"x\", outputs=\"x_masks\")\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step 2: Create model and FastEstimator network</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network architecture has pretrained weights as initialization for downsteam task. Whole network is then trained during the fine tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_model():\n",
    "    token_inputs = Input((max_len), dtype=tf.int32, name='input_words')\n",
    "    mask_inputs = Input((max_len), dtype=tf.int32, name='input_masks')\n",
    "    bert_model = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
    "    seq_output, _ = bert_model(token_inputs, attention_mask=mask_inputs)\n",
    "    output = Dense(24, activation='softmax')(seq_output)\n",
    "    model = Model([token_inputs, mask_inputs], output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model definition is then intantiated by calling fe.build which also associates the model with specific optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fe.build(model_fn=ner_model, optimizer_fn=lambda: tf.optimizers.Adam(1e-5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fe.Network takes series of operators and here we feed our model in the ModelOp with inputs and outputs. Here, ReshapeOp transforms the prediction and ground truth to scalar or two dimensional vector before feeding it to loss calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = fe.Network(ops=[\n",
    "        ModelOp(model=model, inputs=[\"x\", \"x_masks\"], outputs=\"y_pred\"),\n",
    "        ReshapeOp(inputs=\"y\", outputs=\"y\"),\n",
    "        ReshapeOp(inputs=\"y_pred\", outputs=\"y_pred\"),\n",
    "        CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"loss\"),\n",
    "        UpdateOp(model=model, loss_name=\"loss\")\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step 3: Prepare Estimator and configure the training loop</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimator basically has four arguments network, pipeline, epochs and traces. During the training, we want the accuracy metric and save the model with minimum loss, we will define that in Trace class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = [Accuracy(true_key=\"y\", pred_key=\"y_pred\"), BestModelSaver(model=model, save_dir=save_dir)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = fe.Estimator(network=network,\n",
    "                             pipeline=pipeline,\n",
    "                             epochs=epochs,\n",
    "                             traces=traces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Training</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ______           __  ______     __  _                 __            \n",
      "   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n",
      "  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n",
      " / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n",
      "/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n",
      "                                                                        \n",
      "\n",
      "FastEstimator-Start: step: 1; model_lr: 1e-05; \n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "FastEstimator-Train: step: 1; loss: 3.295821; \n",
      "FastEstimator-Train: step: 100; loss: 0.6649006; steps/sec: 1.96; \n",
      "FastEstimator-Train: step: 125; epoch: 1; epoch_time: 77.27 sec; \n",
      "FastEstimator-ModelSaver: saved model to /tmp/tmp5ftaeyeg/model_best_loss.h5\n",
      "FastEstimator-Eval: step: 125; epoch: 1; loss: 0.6276959; min_loss: 0.6276959; since_best: 0; accuracy: 0.851825; \n",
      "FastEstimator-Train: step: 200; loss: 0.4632114; steps/sec: 1.92; \n",
      "FastEstimator-Train: step: 250; epoch: 2; epoch_time: 65.06 sec; \n",
      "FastEstimator-ModelSaver: saved model to /tmp/tmp5ftaeyeg/model_best_loss.h5\n",
      "FastEstimator-Eval: step: 250; epoch: 2; loss: 0.36729997; min_loss: 0.36729997; since_best: 0; accuracy: 0.886275; \n",
      "FastEstimator-Train: step: 300; loss: 0.44376746; steps/sec: 1.91; \n",
      "FastEstimator-Train: step: 375; epoch: 3; epoch_time: 65.35 sec; \n",
      "FastEstimator-ModelSaver: saved model to /tmp/tmp5ftaeyeg/model_best_loss.h5\n",
      "FastEstimator-Eval: step: 375; epoch: 3; loss: 0.30040693; min_loss: 0.30040693; since_best: 0; accuracy: 0.902725; \n",
      "FastEstimator-Train: step: 400; loss: 0.39258525; steps/sec: 1.91; \n",
      "FastEstimator-Train: step: 500; loss: 0.33055496; steps/sec: 1.93; \n",
      "FastEstimator-Train: step: 500; epoch: 4; epoch_time: 65.19 sec; \n",
      "FastEstimator-ModelSaver: saved model to /tmp/tmp5ftaeyeg/model_best_loss.h5\n",
      "FastEstimator-Eval: step: 500; epoch: 4; loss: 0.27842075; min_loss: 0.27842075; since_best: 0; accuracy: 0.90795; \n",
      "FastEstimator-Train: step: 600; loss: 0.25585985; steps/sec: 1.91; \n",
      "FastEstimator-Train: step: 625; epoch: 5; epoch_time: 65.28 sec; \n",
      "FastEstimator-ModelSaver: saved model to /tmp/tmp5ftaeyeg/model_best_loss.h5\n",
      "FastEstimator-Eval: step: 625; epoch: 5; loss: 0.26954836; min_loss: 0.26954836; since_best: 0; accuracy: 0.911025; \n",
      "FastEstimator-Train: step: 700; loss: 0.2588492; steps/sec: 1.92; \n",
      "FastEstimator-Train: step: 750; epoch: 6; epoch_time: 65.16 sec; \n",
      "FastEstimator-ModelSaver: saved model to /tmp/tmp5ftaeyeg/model_best_loss.h5\n",
      "FastEstimator-Eval: step: 750; epoch: 6; loss: 0.25883296; min_loss: 0.25883296; since_best: 0; accuracy: 0.9164; \n",
      "FastEstimator-Train: step: 800; loss: 0.30945948; steps/sec: 1.91; \n",
      "FastEstimator-Train: step: 875; epoch: 7; epoch_time: 65.25 sec; \n",
      "FastEstimator-ModelSaver: saved model to /tmp/tmp5ftaeyeg/model_best_loss.h5\n",
      "FastEstimator-Eval: step: 875; epoch: 7; loss: 0.2473741; min_loss: 0.2473741; since_best: 0; accuracy: 0.919725; \n",
      "FastEstimator-Train: step: 900; loss: 0.2363513; steps/sec: 1.92; \n",
      "FastEstimator-Train: step: 1000; loss: 0.20820758; steps/sec: 1.93; \n",
      "FastEstimator-Train: step: 1000; epoch: 8; epoch_time: 65.19 sec; \n",
      "FastEstimator-Eval: step: 1000; epoch: 8; loss: 0.24861321; min_loss: 0.2473741; since_best: 1; accuracy: 0.9181; \n",
      "FastEstimator-Train: step: 1100; loss: 0.1515786; steps/sec: 1.92; \n",
      "FastEstimator-Train: step: 1125; epoch: 9; epoch_time: 64.95 sec; \n",
      "FastEstimator-ModelSaver: saved model to /tmp/tmp5ftaeyeg/model_best_loss.h5\n",
      "FastEstimator-Eval: step: 1125; epoch: 9; loss: 0.2396766; min_loss: 0.2396766; since_best: 0; accuracy: 0.92385; \n",
      "FastEstimator-Train: step: 1200; loss: 0.22028522; steps/sec: 1.92; \n",
      "FastEstimator-Train: step: 1250; epoch: 10; epoch_time: 65.18 sec; \n",
      "FastEstimator-ModelSaver: saved model to /tmp/tmp5ftaeyeg/model_best_loss.h5\n",
      "FastEstimator-Eval: step: 1250; epoch: 10; loss: 0.23522836; min_loss: 0.23522836; since_best: 0; accuracy: 0.9262; \n",
      "FastEstimator-Finish: step: 1250; total_time: 757.99 sec; model_lr: 1e-05; \n"
     ]
    }
   ],
   "source": [
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Inferencing</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model weights using <i>fe.build</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model weights from /tmp/tmp5ftaeyeg/model_best_loss.h5\n"
     ]
    }
   ],
   "source": [
    "model_name = 'model_best_loss.h5'\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "trained_model = fe.build(model_fn=ner_model, weights_path=model_path, optimizer_fn=lambda: tf.optimizers.Adam(1e-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth is:  ['B-ORG', 'I-ORG', 'B-ORG', 'I-ORG']\n"
     ]
    }
   ],
   "source": [
    "selected_idx = np.random.randint(1000)\n",
    "print(\"Ground truth is: \",eval_data[selected_idx]['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create data dictionary for the inference. Transform() function in Pipeline and Network applies all the operations on the given data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_data = {\"x\":eval_data[selected_idx]['x'], \"y\":eval_data[selected_idx]['y']}\n",
    "data = pipeline.transform(infer_data, mode=\"infer\")\n",
    "data = network.transform(data, mode=\"infer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the predictions using <i>feed_forward</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = feed_forward(trained_model, [data[\"x\"],data[\"x_masks\"]], training=False)\n",
    "predictions = np.array(predictions).reshape(20,24)\n",
    "predictions = np.argmax(predictions, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key(val): \n",
    "    for key, value in tag2idx.items(): \n",
    "         if val == value: \n",
    "            return key "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:  ['B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n"
     ]
    }
   ],
   "source": [
    "print(\"Predictions: \", [get_key(pred) for pred in predictions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fe",
   "language": "python",
   "name": "fe"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
