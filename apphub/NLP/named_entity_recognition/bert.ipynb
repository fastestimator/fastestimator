{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition using BERT Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For downstream NLP tasks such as question answering, named entity recognition, and language inference, models built on pre-trained word representations tend to perform better. BERT, which fine tunes a deep bi-directional representation on a series of tasks, achieves state-of-the-art results. Unlike traditional transformers, BERT is trained on \"masked language modeling,\" which means that it is allowed to see the whole sentence and does not limit the context it can take into account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we are leveraging the transformers library to load a BERT model, along with some config files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "import fastestimator as fe\n",
    "from fastestimator.dataset.data import mitmovie_ner\n",
    "from fastestimator.op.numpyop.numpyop import NumpyOp\n",
    "from fastestimator.op.numpyop.univariate import PadSequence, Tokenize, WordtoId\n",
    "from fastestimator.op.tensorop import Reshape\n",
    "from fastestimator.op.tensorop.loss import CrossEntropy\n",
    "from fastestimator.op.tensorop.model import ModelOp, UpdateOp\n",
    "from fastestimator.trace.metric import Accuracy\n",
    "from fastestimator.trace.io import BestModelSaver\n",
    "from fastestimator.backend import feed_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "max_len = 20\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "train_steps_per_epoch = None\n",
    "eval_steps_per_epoch = None\n",
    "save_dir = tempfile.mkdtemp()\n",
    "data_dir = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need a custom `NumpyOp` that constructs attention masks for input sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionMask(NumpyOp):\n",
    "    def forward(self, data, state):\n",
    "        masks = [float(i > 0) for i in data]\n",
    "        return np.array(masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our `char2idx` function creates a look-up table to match ids and labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char2idx(data):\n",
    "    tag2idx = {t: i for i, t in enumerate(data)}\n",
    "    return tag2idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Building components</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Prepare training & evaluation data and define a `Pipeline`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are loading train and eval sequences from the MIT Movie datasets that is semantically tagged along with data and label vocabulary. For this example other nouns are omitted for the simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/fe/fastestimator/fastestimator/dataset/data/mitmovie_ner.py:101: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  y_train = np.array(y_train)\n",
      "/home/ubuntu/fe/fastestimator/fastestimator/dataset/data/mitmovie_ner.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  y_eval = np.array(y_eval)\n"
     ]
    }
   ],
   "source": [
    "train_data, eval_data, data_vocab, label_vocab = mitmovie_ner.load_data(root_dir=data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a pipeline to tokenize and pad the input sequences and construct attention masks. Attention masks are used to avoid performing attention operations on padded tokens. We are using the BERT tokenizer for input sequence tokenization, and limiting our sequences to a max length of 50 for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "tag2idx = char2idx(label_vocab)\n",
    "pipeline = fe.Pipeline(\n",
    "    train_data=train_data,\n",
    "    eval_data=eval_data,\n",
    "    batch_size=batch_size,\n",
    "    ops=[\n",
    "        Tokenize(inputs=\"x\", outputs=\"x\", tokenize_fn=tokenizer.tokenize),\n",
    "        WordtoId(inputs=\"x\", outputs=\"x\", mapping=tokenizer.convert_tokens_to_ids),\n",
    "        WordtoId(inputs=\"y\", outputs=\"y\", mapping=tag2idx),\n",
    "        PadSequence(max_len=max_len, inputs=\"x\", outputs=\"x\"),\n",
    "        PadSequence(max_len=max_len, value=len(tag2idx), inputs=\"y\", outputs=\"y\"),\n",
    "        AttentionMask(inputs=\"x\", outputs=\"x_masks\")\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create `model` and FastEstimator `Network`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our neural network architecture leverages pre-trained weights as initialization for downstream tasks. The whole network is then trained during the fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_model():\n",
    "    token_inputs = Input((max_len), dtype=tf.int32, name='input_words')\n",
    "    mask_inputs = Input((max_len), dtype=tf.int32, name='input_masks')\n",
    "    bert_model = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
    "    bert_output = bert_model(token_inputs, attention_mask=mask_inputs) # use the last hidden state\n",
    "    output = Dense(len(label_vocab) + 1, activation='softmax')(bert_output[0])\n",
    "    model = Model([token_inputs, mask_inputs], output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the model, it is then instantiated by calling fe.build which also associates the model with a specific optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fe.build(model_fn=ner_model, optimizer_fn=lambda: tf.optimizers.Adam(1e-5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fe.Network` takes a series of operators. In this case we use a `ModelOp` to run forward passes through the neural network. The `ReshapeOp` is then used to transform the prediction and ground truth to a two dimensional vector or scalar respectively before feeding them to the loss calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = fe.Network(ops=[\n",
    "        ModelOp(model=model, inputs=[\"x\", \"x_masks\"], outputs=\"y_pred\"),\n",
    "        Reshape(inputs=\"y\", outputs=\"y\", shape=(-1, )),\n",
    "        Reshape(inputs=\"y_pred\", outputs=\"y_pred\", shape=(-1, len(label_vocab) + 1)),\n",
    "        CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"loss\"),\n",
    "        UpdateOp(model=model, loss_name=\"loss\")\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Prepare `Estimator` and configure the training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Estimator` takes four important arguments: network, pipeline, epochs, and traces. During the training, we want to compute accuracy as well as to save the model with the minimum loss. This can be done using `Traces`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = [Accuracy(true_key=\"y\", pred_key=\"y_pred\"), BestModelSaver(model=model, save_dir=save_dir)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = fe.Estimator(network=network,\n",
    "                         pipeline=pipeline,\n",
    "                         epochs=epochs,\n",
    "                         traces=traces, \n",
    "                         train_steps_per_epoch=train_steps_per_epoch,\n",
    "                         eval_steps_per_epoch=eval_steps_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Training</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ______           __  ______     __  _                 __            \n",
      "   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n",
      "  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n",
      " / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n",
      "/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n",
      "                                                                        \n",
      "\n",
      "FastEstimator-Start: step: 1; num_device: 1; logging_interval: 100; \n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "FastEstimator-Train: step: 1; loss: 3.534539; \n",
      "FastEstimator-Train: step: 100; loss: 0.7910271; steps/sec: 2.04; \n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "FastEstimator-Train: step: 153; epoch: 1; epoch_time: 93.27 sec; \n",
      "FastEstimator-BestModelSaver: Saved model to /tmp/tmpjyf_2m8t/model_best_loss.h5\n",
      "FastEstimator-Eval: step: 153; epoch: 1; loss: 0.4999621; accuracy: 0.8571633237822349; since_best_loss: 0; min_loss: 0.4999621; \n",
      "FastEstimator-Train: step: 200; loss: 0.4584582; steps/sec: 1.71; \n",
      "FastEstimator-Train: step: 300; loss: 0.34510213; steps/sec: 1.97; \n",
      "FastEstimator-Train: step: 306; epoch: 2; epoch_time: 77.24 sec; \n",
      "FastEstimator-BestModelSaver: Saved model to /tmp/tmpjyf_2m8t/model_best_loss.h5\n",
      "FastEstimator-Eval: step: 306; epoch: 2; loss: 0.33649036; accuracy: 0.9036635284486287; since_best_loss: 0; min_loss: 0.33649036; \n",
      "FastEstimator-Train: step: 400; loss: 0.28147238; steps/sec: 1.89; \n",
      "FastEstimator-Train: step: 459; epoch: 3; epoch_time: 81.27 sec; \n",
      "FastEstimator-BestModelSaver: Saved model to /tmp/tmpjyf_2m8t/model_best_loss.h5\n",
      "FastEstimator-Eval: step: 459; epoch: 3; loss: 0.27175826; accuracy: 0.920384772820303; since_best_loss: 0; min_loss: 0.27175826; \n",
      "FastEstimator-Train: step: 500; loss: 0.2756353; steps/sec: 1.92; \n",
      "FastEstimator-Train: step: 600; loss: 0.22491762; steps/sec: 2.0; \n",
      "FastEstimator-Train: step: 612; epoch: 4; epoch_time: 76.74 sec; \n",
      "FastEstimator-BestModelSaver: Saved model to /tmp/tmpjyf_2m8t/model_best_loss.h5\n",
      "FastEstimator-Eval: step: 612; epoch: 4; loss: 0.23077382; accuracy: 0.9328694228407696; since_best_loss: 0; min_loss: 0.23077382; \n",
      "FastEstimator-Train: step: 700; loss: 0.21992946; steps/sec: 2.0; \n",
      "FastEstimator-Train: step: 765; epoch: 5; epoch_time: 76.65 sec; \n",
      "FastEstimator-BestModelSaver: Saved model to /tmp/tmpjyf_2m8t/model_best_loss.h5\n",
      "FastEstimator-Eval: step: 765; epoch: 5; loss: 0.20521004; accuracy: 0.9419566107245191; since_best_loss: 0; min_loss: 0.20521004; \n",
      "FastEstimator-Train: step: 800; loss: 0.22478268; steps/sec: 2.0; \n",
      "FastEstimator-Train: step: 900; loss: 0.18137912; steps/sec: 1.99; \n",
      "FastEstimator-Train: step: 918; epoch: 6; epoch_time: 76.67 sec; \n",
      "FastEstimator-BestModelSaver: Saved model to /tmp/tmpjyf_2m8t/model_best_loss.h5\n",
      "FastEstimator-Eval: step: 918; epoch: 6; loss: 0.19312857; accuracy: 0.947359803520262; since_best_loss: 0; min_loss: 0.19312857; \n",
      "FastEstimator-Train: step: 1000; loss: 0.16234711; steps/sec: 1.99; \n",
      "FastEstimator-Train: step: 1071; epoch: 7; epoch_time: 76.88 sec; \n",
      "FastEstimator-BestModelSaver: Saved model to /tmp/tmpjyf_2m8t/model_best_loss.h5\n",
      "FastEstimator-Eval: step: 1071; epoch: 7; loss: 0.17946187; accuracy: 0.951105198526402; since_best_loss: 0; min_loss: 0.17946187; \n",
      "FastEstimator-Train: step: 1100; loss: 0.1411412; steps/sec: 1.99; \n",
      "FastEstimator-Train: step: 1200; loss: 0.21398099; steps/sec: 2.0; \n",
      "FastEstimator-Train: step: 1224; epoch: 8; epoch_time: 76.62 sec; \n",
      "FastEstimator-BestModelSaver: Saved model to /tmp/tmpjyf_2m8t/model_best_loss.h5\n",
      "FastEstimator-Eval: step: 1224; epoch: 8; loss: 0.17292295; accuracy: 0.9533974621367172; since_best_loss: 0; min_loss: 0.17292295; \n",
      "FastEstimator-Train: step: 1300; loss: 0.104645446; steps/sec: 1.98; \n",
      "FastEstimator-Train: step: 1377; epoch: 9; epoch_time: 77.22 sec; \n",
      "FastEstimator-BestModelSaver: Saved model to /tmp/tmpjyf_2m8t/model_best_loss.h5\n",
      "FastEstimator-Eval: step: 1377; epoch: 9; loss: 0.17160487; accuracy: 0.95440032746623; since_best_loss: 0; min_loss: 0.17160487; \n",
      "FastEstimator-Train: step: 1400; loss: 0.114030465; steps/sec: 1.99; \n",
      "FastEstimator-Train: step: 1500; loss: 0.118343726; steps/sec: 2.0; \n",
      "FastEstimator-Train: step: 1530; epoch: 10; epoch_time: 76.59 sec; \n",
      "FastEstimator-BestModelSaver: Saved model to /tmp/tmpjyf_2m8t/model_best_loss.h5\n",
      "FastEstimator-Eval: step: 1530; epoch: 10; loss: 0.16745299; accuracy: 0.9565083913221449; since_best_loss: 0; min_loss: 0.16745299; \n",
      "FastEstimator-Finish: step: 1530; total_time: 897.96 sec; model_lr: 1e-05; \n"
     ]
    }
   ],
   "source": [
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Inferencing</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model weights using <i>fe.build</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model_best_loss.h5'\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "trained_model = fe.build(model_fn=ner_model, weights_path=model_path, optimizer_fn=lambda: tf.optimizers.Adam(1e-5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take random phrase about a movie and predict it's named entities in BIO format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = 'have you seen The dark night trilogy'\n",
    "test_ground_truth = ['O', 'O', 'O', 'O', 'B-TITLE', 'I-TITLE', 'I-TITLE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a data dictionary for the inference. The `transform()` function in `Pipeline` and `Network` applies all their operations on the given data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_data = {\"x\":test_input, \"y\":test_ground_truth}\n",
    "data = pipeline.transform(infer_data, mode=\"infer\")\n",
    "data = network.transform(data, mode=\"infer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the predictions using <i>feed_forward</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = feed_forward(trained_model, [data[\"x\"],data[\"x_masks\"]], training=False)\n",
    "predictions = np.array(predictions).reshape(20, len(label_vocab) + 1)\n",
    "predictions = np.argmax(predictions, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key(val): \n",
    "    for key, value in tag2idx.items(): \n",
    "         if val == value: \n",
    "            return key "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:  ['O', 'O', 'O', 'O', 'B-TITLE', 'I-TITLE', 'I-TITLE', None, None, None, None, None, None, None, None, None, None, None, None, None]\n"
     ]
    }
   ],
   "source": [
    "print(\"Predictions: \", [get_key(pred) for pred in predictions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
