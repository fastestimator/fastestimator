{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Prediction in IMDB Reviews using an LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import os\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as fn\n",
    "import fastestimator as fe\n",
    "from fastestimator.dataset.data import imdb_review\n",
    "from fastestimator.op.numpyop.univariate.reshape import Reshape\n",
    "from fastestimator.op.tensorop.loss import CrossEntropy\n",
    "from fastestimator.op.tensorop.model import ModelOp, UpdateOp\n",
    "from fastestimator.trace.io import BestModelSaver\n",
    "from fastestimator.trace.metric import Accuracy\n",
    "from fastestimator.backend import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "MAX_WORDS = 10000\n",
    "MAX_LEN = 500\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "train_steps_per_epoch = None\n",
    "eval_steps_per_epoch = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Building components</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Prepare training & evaluation data and define a `Pipeline`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are loading the dataset from tf.keras.datasets.imdb which contains movie reviews and sentiment scores. All the words have been replaced with the integers that specifies the popularity of the word in corpus. To ensure all the sequences are of same length we need to pad the input sequences before defining the `Pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, eval_data = imdb_review.load_data(MAX_LEN, MAX_WORDS)\n",
    "pipeline = fe.Pipeline(train_data=train_data,\n",
    "                       eval_data=eval_data,\n",
    "                       batch_size=batch_size,\n",
    "                       ops=Reshape(1, inputs=\"y\", outputs=\"y\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create a `model` and FastEstimator `Network`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we have to define the neural network architecture, and then pass the definition, associated model name, and optimizer into fe.build:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewSentiment(nn.Module):\n",
    "    def __init__(self, embedding_size=64, hidden_units=64):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(MAX_WORDS, embedding_size)\n",
    "        self.conv1d = nn.Conv1d(in_channels=64, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.maxpool1d = nn.MaxPool1d(kernel_size=4)\n",
    "        self.lstm = nn.LSTM(input_size=125, hidden_size=hidden_units, num_layers=1)\n",
    "        self.fc1 = nn.Linear(in_features=hidden_units, out_features=250)\n",
    "        self.fc2 = nn.Linear(in_features=250, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.permute((0, 2, 1))\n",
    "        x = self.conv1d(x)\n",
    "        x = fn.relu(x)\n",
    "        x = self.maxpool1d(x)\n",
    "        output, _ = self.lstm(x)\n",
    "        x = output[:, -1]  # sequence output of only last timestamp\n",
    "        x = fn.tanh(x)\n",
    "        x = self.fc1(x)\n",
    "        x = fn.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = fn.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Network` is the object that defines the whole training graph, including models, loss functions, optimizers etc. A `Network` can have several different models and loss functions (ex. GANs). `fe.Network` takes a series of operators, in this case just the basic `ModelOp`, loss op, and `UpdateOp` will suffice. It should be noted that \"y_pred\" is the key in the data dictionary which will store the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fe.build(model_fn=lambda: ReviewSentiment(), optimizer_fn=\"adam\")\n",
    "network = fe.Network(ops=[\n",
    "    ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),\n",
    "    CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"loss\"),\n",
    "    UpdateOp(model=model, loss_name=\"loss\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Prepare `Estimator` and configure the training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Estimator` is the API that wraps the `Pipeline`, `Network` and other training metadata together. `Estimator` also contains `Traces`, which are similar to the callbacks of Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the training loop, we want to measure the validation loss and save the model that has the minimum loss. `BestModelSaver` is a convenient `Trace` to achieve this. Let's also measure accuracy over time using another `Trace`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = tempfile.mkdtemp()\n",
    "traces = [Accuracy(true_key=\"y\", pred_key=\"y_pred\"), BestModelSaver(model=model, save_dir=model_dir)]\n",
    "estimator = fe.Estimator(network=network,\n",
    "                         pipeline=pipeline,\n",
    "                         epochs=epochs,\n",
    "                         traces=traces,\n",
    "                         train_steps_per_epoch=train_steps_per_epoch,\n",
    "                         eval_steps_per_epoch=eval_steps_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Training</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ______           __  ______     __  _                 __            \n",
      "   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n",
      "  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n",
      " / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n",
      "/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n",
      "                                                                        \n",
      "\n",
      "FastEstimator-Start: step: 1; model_lr: 0.001; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/fe_env/lib/python3.6/site-packages/torch/nn/functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "/home/ubuntu/anaconda3/envs/fe_env/lib/python3.6/site-packages/torch/nn/functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastEstimator-Train: step: 1; loss: 0.6905144; \n",
      "FastEstimator-Train: step: 100; loss: 0.69094294; steps/sec: 46.99; \n",
      "FastEstimator-Train: step: 200; loss: 0.6621749; steps/sec: 48.85; \n",
      "FastEstimator-Train: step: 300; loss: 0.5835465; steps/sec: 54.12; \n",
      "FastEstimator-Train: step: 391; epoch: 1; epoch_time: 7.74 sec; \n",
      "Saved model to /tmp/tmp69qyfzvm/model_best_loss.pt\n",
      "FastEstimator-Eval: step: 391; epoch: 1; loss: 0.5250161; min_loss: 0.5250161; since_best: 0; accuracy: 0.7377667446412374; \n",
      "FastEstimator-Train: step: 400; loss: 0.51533854; steps/sec: 55.54; \n",
      "FastEstimator-Train: step: 500; loss: 0.6381638; steps/sec: 60.01; \n",
      "FastEstimator-Train: step: 600; loss: 0.4390931; steps/sec: 58.14; \n",
      "FastEstimator-Train: step: 700; loss: 0.32808638; steps/sec: 59.57; \n",
      "FastEstimator-Train: step: 782; epoch: 2; epoch_time: 6.7 sec; \n",
      "Saved model to /tmp/tmp69qyfzvm/model_best_loss.pt\n",
      "FastEstimator-Eval: step: 782; epoch: 2; loss: 0.41990075; min_loss: 0.41990075; since_best: 0; accuracy: 0.8072277653124552; \n",
      "FastEstimator-Train: step: 800; loss: 0.4495564; steps/sec: 56.01; \n",
      "FastEstimator-Train: step: 900; loss: 0.38001418; steps/sec: 60.14; \n",
      "FastEstimator-Train: step: 1000; loss: 0.28246647; steps/sec: 60.33; \n",
      "FastEstimator-Train: step: 1100; loss: 0.36126548; steps/sec: 60.51; \n",
      "FastEstimator-Train: step: 1173; epoch: 3; epoch_time: 6.63 sec; \n",
      "Saved model to /tmp/tmp69qyfzvm/model_best_loss.pt\n",
      "FastEstimator-Eval: step: 1173; epoch: 3; loss: 0.39232534; min_loss: 0.39232534; since_best: 0; accuracy: 0.8241752995655702; \n",
      "FastEstimator-Train: step: 1200; loss: 0.32620478; steps/sec: 55.57; \n",
      "FastEstimator-Train: step: 1300; loss: 0.33430642; steps/sec: 60.1; \n",
      "FastEstimator-Train: step: 1400; loss: 0.21134894; steps/sec: 62.23; \n",
      "FastEstimator-Train: step: 1500; loss: 0.34480703; steps/sec: 62.4; \n",
      "FastEstimator-Train: step: 1564; epoch: 4; epoch_time: 6.47 sec; \n",
      "FastEstimator-Eval: step: 1564; epoch: 4; loss: 0.3997118; min_loss: 0.39232534; since_best: 1; accuracy: 0.8274693273499785; \n",
      "FastEstimator-Train: step: 1600; loss: 0.14769143; steps/sec: 57.72; \n",
      "FastEstimator-Train: step: 1700; loss: 0.17477548; steps/sec: 60.4; \n",
      "FastEstimator-Train: step: 1800; loss: 0.34234992; steps/sec: 60.82; \n",
      "FastEstimator-Train: step: 1900; loss: 0.34789586; steps/sec: 61.12; \n",
      "FastEstimator-Train: step: 1955; epoch: 5; epoch_time: 6.55 sec; \n",
      "FastEstimator-Eval: step: 1955; epoch: 5; loss: 0.39978975; min_loss: 0.39232534; since_best: 2; accuracy: 0.8300950016708837; \n",
      "FastEstimator-Train: step: 2000; loss: 0.21192178; steps/sec: 56.29; \n",
      "FastEstimator-Train: step: 2100; loss: 0.24565384; steps/sec: 60.85; \n",
      "FastEstimator-Train: step: 2200; loss: 0.21373041; steps/sec: 60.08; \n",
      "FastEstimator-Train: step: 2300; loss: 0.24357724; steps/sec: 61.3; \n",
      "FastEstimator-Train: step: 2346; epoch: 6; epoch_time: 6.57 sec; \n",
      "FastEstimator-Eval: step: 2346; epoch: 6; loss: 0.39285892; min_loss: 0.39232534; since_best: 3; accuracy: 0.8357282665775528; \n",
      "FastEstimator-Train: step: 2400; loss: 0.08471558; steps/sec: 56.66; \n",
      "FastEstimator-Train: step: 2500; loss: 0.20877948; steps/sec: 60.78; \n",
      "FastEstimator-Train: step: 2600; loss: 0.09914401; steps/sec: 61.27; \n",
      "FastEstimator-Train: step: 2700; loss: 0.15458922; steps/sec: 60.94; \n",
      "FastEstimator-Train: step: 2737; epoch: 7; epoch_time: 6.53 sec; \n",
      "FastEstimator-Eval: step: 2737; epoch: 7; loss: 0.43396476; min_loss: 0.39232534; since_best: 4; accuracy: 0.8326729364586815; \n",
      "FastEstimator-Train: step: 2800; loss: 0.16790089; steps/sec: 56.49; \n",
      "FastEstimator-Train: step: 2900; loss: 0.09017545; steps/sec: 60.99; \n",
      "FastEstimator-Train: step: 3000; loss: 0.06604583; steps/sec: 61.62; \n",
      "FastEstimator-Train: step: 3100; loss: 0.2692815; steps/sec: 61.39; \n",
      "FastEstimator-Train: step: 3128; epoch: 8; epoch_time: 6.52 sec; \n",
      "FastEstimator-Eval: step: 3128; epoch: 8; loss: 0.47958812; min_loss: 0.39232534; since_best: 5; accuracy: 0.8260371413567575; \n",
      "FastEstimator-Train: step: 3200; loss: 0.12287539; steps/sec: 56.58; \n",
      "FastEstimator-Train: step: 3300; loss: 0.16652104; steps/sec: 62.04; \n",
      "FastEstimator-Train: step: 3400; loss: 0.08797251; steps/sec: 59.71; \n",
      "FastEstimator-Train: step: 3500; loss: 0.08476864; steps/sec: 60.8; \n",
      "FastEstimator-Train: step: 3519; epoch: 9; epoch_time: 6.56 sec; \n",
      "FastEstimator-Eval: step: 3519; epoch: 9; loss: 0.51876205; min_loss: 0.39232534; since_best: 6; accuracy: 0.824270778631785; \n",
      "FastEstimator-Train: step: 3600; loss: 0.14876236; steps/sec: 56.08; \n",
      "FastEstimator-Train: step: 3700; loss: 0.11151114; steps/sec: 60.72; \n",
      "FastEstimator-Train: step: 3800; loss: 0.05140955; steps/sec: 60.18; \n",
      "FastEstimator-Train: step: 3900; loss: 0.062084932; steps/sec: 59.42; \n",
      "FastEstimator-Train: step: 3910; epoch: 10; epoch_time: 6.62 sec; \n",
      "FastEstimator-Eval: step: 3910; epoch: 10; loss: 0.5560739; min_loss: 0.39232534; since_best: 7; accuracy: 0.831049792333031; \n",
      "FastEstimator-Finish: step: 3910; total_time: 87.54 sec; model_lr: 0.001; \n"
     ]
    }
   ],
   "source": [
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Inferencing</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For inferencing, first we have to load the trained model weights. We previously saved model weights corresponding to our minimum loss, and now we will load the weights using `load_model()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model weights from /tmp/tmp69qyfzvm/model_best_loss.pt\n"
     ]
    }
   ],
   "source": [
    "model_name = 'model_best_loss.pt'\n",
    "model_path = os.path.join(model_dir, model_name)\n",
    "load_model(model, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get some random sequence and compare the prediction with the ground truth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth is:  0\n"
     ]
    }
   ],
   "source": [
    "selected_idx = np.random.randint(10000)\n",
    "print(\"Ground truth is: \",eval_data[selected_idx]['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create data dictionary for the inference. The `Transform()` function in Pipeline and Network applies all the operations on the given data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_data = {\"x\":eval_data[selected_idx]['x'], \"y\":eval_data[selected_idx]['y']}\n",
    "data = pipeline.transform(infer_data, mode=\"infer\")\n",
    "data = network.transform(data, mode=\"infer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, print the inferencing results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for the input sequence:  0.30634004\n"
     ]
    }
   ],
   "source": [
    "print(\"Prediction for the input sequence: \", np.array(data[\"y_pred\"])[0][0])"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
