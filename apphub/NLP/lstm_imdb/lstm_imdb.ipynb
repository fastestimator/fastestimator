{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB Reviews Sentiments prediction with LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tempfile\n",
    "import fastestimator as fe\n",
    "\n",
    "from tensorflow.python.keras import layers\n",
    "from fastestimator.op.tensorop import BinaryCrossentropy, ModelOp, Reshape\n",
    "from fastestimator.trace import Accuracy, ModelSaver\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example we are defining the vocabulary size to 10000 and maximum sequence length to 500. That can also be changed later on as hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "MAX_WORDS = 10000\n",
    "MAX_LEN = 500\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "steps_per_epoch=None\n",
    "validation_steps=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 : Prepare training and evaluation dataset, define FastEstimator Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are loading the dataset from the tf.keras.datasets.imdb which contains movie reviews and sentiment scores.\n",
    "All the words have been replaced with the integers that specifies the popularity of the word in corpus. To ensure all the sequences are of same length we need to pad the input sequences before defining the Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(input_list, padding_size, padding_value):\n",
    "    return input_list + [padding_value] * abs((len(input_list) - padding_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data and pad the sequences\n",
    "(x_train, y_train), (x_eval, y_eval) = tf.keras.datasets.imdb.load_data(maxlen=MAX_LEN, num_words=MAX_WORDS)\n",
    "data = {\n",
    "        \"train\": {\n",
    "            \"x\": np.array([pad(x, MAX_LEN, 0) for x in x_train]),\n",
    "            \"y\": y_train\n",
    "        },\n",
    "        \"eval\": {\n",
    "            \"x\": np.array([pad(x, MAX_LEN, 0) for x in x_eval]),\n",
    "            \"y\": y_eval\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can define the Pipeline passing the batch size and data dictionary. We need to reshape the groud truth from (batch_size,) to (batch_size, 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = fe.Pipeline(batch_size=batch_size, data=data, ops=Reshape([1], inputs=\"y\", outputs=\"y\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create model and FastEstimator Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following function define the architecture of the model. Model consists of one dimensional convolution layers and LSTM layers to handle longer sequences. The architecture definition needs to be fed into FEModel along with model name and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Embedding(MAX_WORDS, 64, input_length=MAX_LEN))\n",
    "    model.add(layers.Conv1D(32, 3, padding='same', activation='relu'))\n",
    "    model.add(layers.MaxPooling1D(pool_size=4))\n",
    "    model.add(layers.LSTM(64))\n",
    "    model.add(layers.Dense(250, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, fe.Network takes series of operators and here we feed our FEModel in the ModelOp with inputs and outputs. It should be noted that the <i>y_pred</i> is the key in the data dictionary which will store the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fe.build(model_def=create_lstm, model_name='lstm_imdb', optimizer='adam', loss_name=\"loss\")\n",
    "#define the network\n",
    "network = fe.Network(ops=[\n",
    "        ModelOp(inputs=\"x\", model=model, outputs=\"y_pred\"),\n",
    "                        BinaryCrossentropy(y_true=\"y\", y_pred=\"y_pred\", outputs=\"loss\")\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Prepare estimator and configure the training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the training loop, we want to measure the validation loss and save the model that has the minimum loss. ModelSaver and Accuracy in the Trace class provide this convenient feature of storing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = tempfile.mkdtemp()\n",
    "traces = [Accuracy(true_key=\"y\", pred_key=\"y_pred\", output_name='acc'),\n",
    "         ModelSaver(model_name=\"lstm_imdb\", save_dir=save_dir, save_best=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define the estimator specifying the traning configurations and fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "estimator = fe.Estimator(network=network, \n",
    "                         steps_per_epoch=steps_per_epoch,\n",
    "                         validation_steps=validation_steps,\n",
    "                         pipeline=pipeline, \n",
    "                         epochs=epochs,\n",
    "                         traces=traces)\n",
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferencing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model is stored in temporary directory. We can load the model and perform the inference on sampled sequence from evaluation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'lstm_imdb_best_loss.h5'\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "trained_model = tf.keras.models.load_model(model_path, compile=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get any random sequence and compare the prediction with the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth is:  0\n",
      "Prediction for the input sequence:  [[0.01495596]]\n"
     ]
    }
   ],
   "source": [
    "selected_idx = np.random.randint(10000)\n",
    "print(\"Ground truth is: \",y_eval[selected_idx])\n",
    "padded_seq = np.array([pad(x_eval[selected_idx], MAX_LEN, 0)])\n",
    "prediction = trained_model.predict(padded_seq)\n",
    "print(\"Prediction for the input sequence: \",prediction)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
