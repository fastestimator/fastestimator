{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Languge Modeling using LSTM on Penn Treebank\n",
    "\n",
    "Language Modeling is the development of models to predict the next word of the sequence given the words that precede it. In this notebook we will demonstrate how to predict next word of a sequence using an LSTM. We will be using Penn Treebank dataset which contains 888K words for training, 70K for validation, and 79K for testing, with a vocabulary size of 10K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "import fastestimator as fe\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from fastestimator.op.numpyop import NumpyOp\n",
    "from fastestimator.op.tensorop.loss import CrossEntropy\n",
    "from fastestimator.op.tensorop.model import ModelOp, UpdateOp\n",
    "from fastestimator.trace import Trace\n",
    "from fastestimator.trace.adapt import EarlyStopping, LRScheduler\n",
    "from fastestimator.trace.io import BestModelSaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "epochs=30\n",
    "batch_size=128\n",
    "seq_length=20\n",
    "vocab_size=10000\n",
    "data_dir=None\n",
    "train_steps_per_epoch=None\n",
    "save_dir=tempfile.mkdtemp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Components\n",
    "\n",
    "### Downloading the data\n",
    "\n",
    "First, we will download the Penn Treebank dataset via our dataset API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastestimator.dataset.data.penn_treebank import load_data\n",
    "train_data, eval_data, _, vocab = load_data(root_dir=data_dir, seq_length=seq_length + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create `Pipeline`\n",
    "\n",
    "We will create a custom NumpyOp to generate input and target sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateInputAndTarget(NumpyOp):\n",
    "    def forward(self, data, state):\n",
    "        return data[:-1], data[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = fe.Pipeline(train_data=train_data,\n",
    "                       eval_data=eval_data,\n",
    "                       batch_size=batch_size,\n",
    "                       ops=CreateInputAndTarget(inputs=\"x\", outputs=(\"x\", \"y\")),\n",
    "                       drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create `Network`\n",
    "\n",
    "The architecture of our model is a LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, seq_length):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[None, seq_length]),\n",
    "        tf.keras.layers.LSTM(rnn_units, return_sequences=True, recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fe.build(model_fn=lambda: build_model(vocab_size, embedding_dim=300, rnn_units=600, seq_length=seq_length),\n",
    "                     optimizer_fn=lambda: tf.optimizers.SGD(1.0, momentum=0.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the `Network` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = fe.Network(ops=[\n",
    "    ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),\n",
    "    CrossEntropy(\n",
    "        inputs=(\"y_pred\", \"y\"), outputs=\"ce\", form=\"sparse\", from_logits=True),\n",
    "    UpdateOp(model=model, loss_name=\"ce\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we will also use the following traces:\n",
    "\n",
    "1. A custom trace to calculate Perplexity.\n",
    "2. LRScheduler to apply custom learning rate schedule.\n",
    "3. BestModelSaver for saving the best model. For illustration purpose, we will save these models in a temporary directory.\n",
    "4. EarlyStopping Trace for stopping early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(step, init_lr):\n",
    "    if step <= 1725:\n",
    "        lr = init_lr + init_lr * (step - 1) / 1725\n",
    "    else:\n",
    "        lr = max(2 * init_lr * ((6900 - step + 1725) / 6900), 1.0)\n",
    "    return lr\n",
    "\n",
    "\n",
    "class Perplexity(Trace):\n",
    "    def on_epoch_end(self, data):\n",
    "        ce = data[\"ce\"]\n",
    "        data.write_with_log(self.outputs[0], np.exp(ce))\n",
    "\n",
    "\n",
    "traces = [\n",
    "    Perplexity(inputs=\"ce\", outputs=\"perplexity\", mode=\"eval\"),\n",
    "    LRScheduler(model=model, lr_fn=lambda step: lr_schedule(step, init_lr=1.0)),\n",
    "    BestModelSaver(model=model, save_dir=save_dir, metric='perplexity', save_best_mode='min', load_best_final=True),\n",
    "    EarlyStopping(monitor=\"perplexity\", patience=5)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create `Estimator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = fe.Estimator(pipeline=pipeline,\n",
    "                         network=network,\n",
    "                         epochs=epochs,\n",
    "                         traces=traces,\n",
    "                         train_steps_per_epoch=train_steps_per_epoch, \n",
    "                         log_steps=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ______           __  ______     __  _                 __            \n",
      "   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n",
      "  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n",
      " / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n",
      "/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n",
      "                                                                        \n",
      "\n",
      "FastEstimator-Start: step: 1; num_device: 1; logging_interval: 300; \n",
      "FastEstimator-Train: step: 1; ce: 9.210202; model_lr: 1.0; \n",
      "FastEstimator-Train: step: 300; ce: 6.110634; steps/sec: 8.33; model_lr: 1.1733333; \n",
      "FastEstimator-Train: step: 345; epoch: 1; epoch_time: 43.55 sec; \n",
      "FastEstimator-BestModelSaver: Saved model to /tmp/tmptl5d8hgb/model_best_perplexity.h5\n",
      "FastEstimator-Eval: step: 345; epoch: 1; ce: 5.8996396; perplexity: 364.90594; since_best_perplexity: 0; min_perplexity: 364.90594; \n",
      "FastEstimator-Train: step: 600; ce: 5.7039967; steps/sec: 8.27; model_lr: 1.3472464; \n",
      "FastEstimator-Train: step: 690; epoch: 2; epoch_time: 41.65 sec; \n",
      "FastEstimator-BestModelSaver: Saved model to /tmp/tmptl5d8hgb/model_best_perplexity.h5\n",
      "FastEstimator-Eval: step: 690; epoch: 2; ce: 5.5457325; perplexity: 256.14215; since_best_perplexity: 0; min_perplexity: 256.14215; \n",
      "FastEstimator-Train: step: 900; ce: 5.636613; steps/sec: 8.27; model_lr: 1.5211594; \n",
      "FastEstimator-Train: step: 1035; epoch: 3; epoch_time: 41.89 sec; \n",
      "FastEstimator-BestModelSaver: Saved model to /tmp/tmptl5d8hgb/model_best_perplexity.h5\n",
      "FastEstimator-Eval: step: 1035; epoch: 3; ce: 5.3436885; perplexity: 209.28323; since_best_perplexity: 0; min_perplexity: 209.28323; \n",
      "FastEstimator-Train: step: 1200; ce: 5.3110213; steps/sec: 8.23; model_lr: 1.6950724; \n",
      "FastEstimator-Train: step: 1380; epoch: 4; epoch_time: 41.82 sec; \n",
      "FastEstimator-BestModelSaver: Saved model to /tmp/tmptl5d8hgb/model_best_perplexity.h5\n",
      "FastEstimator-Eval: step: 1380; epoch: 4; ce: 5.209713; perplexity: 183.04152; since_best_perplexity: 0; min_perplexity: 183.04152; \n",
      "FastEstimator-Train: step: 1500; ce: 5.1398573; steps/sec: 8.25; model_lr: 1.8689855; \n",
      "FastEstimator-Train: step: 1725; epoch: 5; epoch_time: 41.79 sec; \n",
      "FastEstimator-BestModelSaver: Saved model to /tmp/tmptl5d8hgb/model_best_perplexity.h5\n",
      "FastEstimator-Eval: step: 1725; epoch: 5; ce: 5.107191; perplexity: 165.20566; since_best_perplexity: 0; min_perplexity: 165.20566; \n",
      "FastEstimator-Train: step: 1800; ce: 5.003286; steps/sec: 8.25; model_lr: 1.9782609; \n",
      "FastEstimator-Train: step: 2070; epoch: 6; epoch_time: 41.9 sec; \n",
      "FastEstimator-BestModelSaver: Saved model to /tmp/tmptl5d8hgb/model_best_perplexity.h5\n",
      "FastEstimator-Eval: step: 2070; epoch: 6; ce: 5.018823; perplexity: 151.23322; since_best_perplexity: 0; min_perplexity: 151.23322; \n",
      "FastEstimator-Train: step: 2100; ce: 4.9688864; steps/sec: 8.23; model_lr: 1.8913044; \n",
      "FastEstimator-Train: step: 2400; ce: 4.8305387; steps/sec: 8.23; model_lr: 1.8043479; \n",
      "FastEstimator-Train: step: 2415; epoch: 7; epoch_time: 41.97 sec; \n",
      "FastEstimator-BestModelSaver: Saved model to /tmp/tmptl5d8hgb/model_best_perplexity.h5\n",
      "FastEstimator-Eval: step: 2415; epoch: 7; ce: 4.9463377; perplexity: 140.65887; since_best_perplexity: 0; min_perplexity: 140.65887; \n",
      "FastEstimator-Train: step: 2700; ce: 4.5900016; steps/sec: 8.23; model_lr: 1.7173913; \n",
      "FastEstimator-Train: step: 2760; epoch: 8; epoch_time: 41.98 sec; \n",
      "FastEstimator-BestModelSaver: Saved model to /tmp/tmptl5d8hgb/model_best_perplexity.h5\n",
      "FastEstimator-Eval: step: 2760; epoch: 8; ce: 4.900966; perplexity: 134.41959; since_best_perplexity: 0; min_perplexity: 134.41959; \n",
      "FastEstimator-Train: step: 3000; ce: 4.6566253; steps/sec: 8.21; model_lr: 1.6304348; \n",
      "FastEstimator-Train: step: 3105; epoch: 9; epoch_time: 41.64 sec; \n",
      "FastEstimator-BestModelSaver: Saved model to /tmp/tmptl5d8hgb/model_best_perplexity.h5\n",
      "FastEstimator-Eval: step: 3105; epoch: 9; ce: 4.8612027; perplexity: 129.17947; since_best_perplexity: 0; min_perplexity: 129.17947; \n",
      "FastEstimator-Train: step: 3300; ce: 4.6201677; steps/sec: 8.34; model_lr: 1.5434783; \n",
      "FastEstimator-Train: step: 3450; epoch: 10; epoch_time: 41.66 sec; \n",
      "FastEstimator-BestModelSaver: Saved model to /tmp/tmptl5d8hgb/model_best_perplexity.h5\n",
      "FastEstimator-Eval: step: 3450; epoch: 10; ce: 4.833195; perplexity: 125.61168; since_best_perplexity: 0; min_perplexity: 125.61168; \n",
      "FastEstimator-Train: step: 3600; ce: 4.672325; steps/sec: 8.27; model_lr: 1.4565217; \n",
      "FastEstimator-Train: step: 3795; epoch: 11; epoch_time: 41.92 sec; \n",
      "FastEstimator-BestModelSaver: Saved model to /tmp/tmptl5d8hgb/model_best_perplexity.h5\n",
      "FastEstimator-Eval: step: 3795; epoch: 11; ce: 4.8110547; perplexity: 122.86113; since_best_perplexity: 0; min_perplexity: 122.86113; \n",
      "FastEstimator-Train: step: 3900; ce: 4.5373406; steps/sec: 8.21; model_lr: 1.3695652; \n",
      "FastEstimator-Train: step: 4140; epoch: 12; epoch_time: 41.83 sec; \n",
      "FastEstimator-BestModelSaver: Saved model to /tmp/tmptl5d8hgb/model_best_perplexity.h5\n",
      "FastEstimator-Eval: step: 4140; epoch: 12; ce: 4.802991; perplexity: 121.87438; since_best_perplexity: 0; min_perplexity: 121.87438; \n",
      "FastEstimator-Train: step: 4200; ce: 4.412928; steps/sec: 8.26; model_lr: 1.2826087; \n",
      "FastEstimator-Train: step: 4485; epoch: 13; epoch_time: 41.72 sec; \n",
      "FastEstimator-BestModelSaver: Saved model to /tmp/tmptl5d8hgb/model_best_perplexity.h5\n",
      "FastEstimator-Eval: step: 4485; epoch: 13; ce: 4.7911124; perplexity: 120.43527; since_best_perplexity: 0; min_perplexity: 120.43527; \n",
      "FastEstimator-Train: step: 4500; ce: 4.402304; steps/sec: 8.26; model_lr: 1.1956521; \n",
      "FastEstimator-Train: step: 4800; ce: 4.4627676; steps/sec: 8.31; model_lr: 1.1086956; \n",
      "FastEstimator-Train: step: 4830; epoch: 14; epoch_time: 41.58 sec; \n",
      "FastEstimator-BestModelSaver: Saved model to /tmp/tmptl5d8hgb/model_best_perplexity.h5\n",
      "FastEstimator-Eval: step: 4830; epoch: 14; ce: 4.78295; perplexity: 119.45622; since_best_perplexity: 0; min_perplexity: 119.45622; \n",
      "FastEstimator-Train: step: 5100; ce: 4.2155848; steps/sec: 8.24; model_lr: 1.0217391; \n",
      "FastEstimator-Train: step: 5175; epoch: 15; epoch_time: 41.92 sec; \n",
      "FastEstimator-BestModelSaver: Saved model to /tmp/tmptl5d8hgb/model_best_perplexity.h5\n",
      "FastEstimator-Eval: step: 5175; epoch: 15; ce: 4.777499; perplexity: 118.80688; since_best_perplexity: 0; min_perplexity: 118.80688; \n",
      "FastEstimator-Train: step: 5400; ce: 4.2096825; steps/sec: 8.24; model_lr: 1.0; \n",
      "FastEstimator-Train: step: 5520; epoch: 16; epoch_time: 41.88 sec; \n",
      "FastEstimator-Eval: step: 5520; epoch: 16; ce: 4.786487; perplexity: 119.87951; since_best_perplexity: 1; min_perplexity: 118.80688; \n",
      "FastEstimator-Train: step: 5700; ce: 4.19374; steps/sec: 8.22; model_lr: 1.0; \n",
      "FastEstimator-Train: step: 5865; epoch: 17; epoch_time: 41.86 sec; \n",
      "FastEstimator-Eval: step: 5865; epoch: 17; ce: 4.791095; perplexity: 120.43314; since_best_perplexity: 2; min_perplexity: 118.80688; \n",
      "FastEstimator-Train: step: 6000; ce: 4.040009; steps/sec: 8.27; model_lr: 1.0; \n",
      "FastEstimator-Train: step: 6210; epoch: 18; epoch_time: 41.74 sec; \n",
      "FastEstimator-Eval: step: 6210; epoch: 18; ce: 4.7963467; perplexity: 121.067314; since_best_perplexity: 3; min_perplexity: 118.80688; \n",
      "FastEstimator-Train: step: 6300; ce: 4.0719166; steps/sec: 8.24; model_lr: 1.0; \n",
      "FastEstimator-Train: step: 6555; epoch: 19; epoch_time: 41.94 sec; \n",
      "FastEstimator-Eval: step: 6555; epoch: 19; ce: 4.799467; perplexity: 121.44568; since_best_perplexity: 4; min_perplexity: 118.80688; \n",
      "FastEstimator-Train: step: 6600; ce: 4.110601; steps/sec: 8.24; model_lr: 1.0; \n",
      "FastEstimator-Train: step: 6900; ce: 3.9709384; steps/sec: 8.3; model_lr: 1.0; \n",
      "FastEstimator-Train: step: 6900; epoch: 20; epoch_time: 41.54 sec; \n",
      "FastEstimator-EarlyStopping: 'perplexity' triggered an early stop. Its best value was 118.80687713623047 at epoch 15\n",
      "FastEstimator-Eval: step: 6900; epoch: 20; ce: 4.8052564; perplexity: 122.150795; since_best_perplexity: 5; min_perplexity: 118.80688; \n",
      "FastEstimator-BestModelSaver: Restoring model from /tmp/tmptl5d8hgb/model_best_perplexity.h5\n",
      "FastEstimator-Finish: step: 6900; total_time: 864.19 sec; model_lr: 1.0; \n"
     ]
    }
   ],
   "source": [
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferencing\n",
    "\n",
    "Once the training is finished, we will use the model to generate some sequences of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_word(data, vocab):\n",
    "    output = network.transform(data, mode=\"infer\") \n",
    "    index = output[\"y_pred\"].numpy().squeeze()[-1].argmax()\n",
    "    if index == 44:    # Removing unkwown predicition\n",
    "        index = output[\"y_pred\"].numpy().squeeze()[-1].argsort()[-2]\n",
    "    return index\n",
    "\n",
    "def generate_sequence(inp_seq, vocab, min_paragraph_len=50):\n",
    "    data = pipeline.transform({\"x\": inp_seq}, mode=\"infer\")\n",
    "    generated_seq = data[\"x\"]\n",
    "    counter=0\n",
    "    next_entry=0\n",
    "    # Stopping at <eos> tag or after min_paragraph_len+30 words\n",
    "    while (counter<min_paragraph_len or next_entry != 43) and counter<min_paragraph_len+30:  \n",
    "        next_entry = get_next_word(data, vocab)\n",
    "        generated_seq = np.concatenate([generated_seq.squeeze(), [next_entry]])\n",
    "        data = {\"x\": generated_seq[-20:].reshape((1, 20))}\n",
    "        counter+=1\n",
    "\n",
    "    return \" \".join([vocab[i] for i in generated_seq])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will provide a text sequence from the validation dataset to the model and generate a paragraph with the input text sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sequence: the pictures <eos> the state <unk> noted that <unk> banking practices are grounds for removing an officer or director and\n",
      "\n",
      "Generated Sequence: the pictures <eos> the state <unk> noted that <unk> banking practices are grounds for removing an officer or director and chief executive officer <eos> mr. guber and mr. peters have been working on the board <eos> the company said the company will be able to pay for the $ N million of the company 's common shares outstanding <eos> the company said the company 's net income rose N N to $ N million from $ N million <eos>\n",
      "\n",
      "\n",
      "Input Sequence: the russians in iran the russians seem to have lost interest in the whole subject <eos> meanwhile congress is cutting\n",
      "\n",
      "Generated Sequence: the russians in iran the russians seem to have lost interest in the whole subject <eos> meanwhile congress is cutting the capital-gains tax cut to the u.s. and the u.s. trade deficit <eos> the u.s. trade deficit has been the highest since august N <eos> the dollar was mixed <eos> the dollar was mixed <eos> the nasdaq composite index rose N to N <eos> the index gained N to N <eos>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _ in range(2):\n",
    "    idx = np.random.choice(len(eval_data))\n",
    "    inp_seq = eval_data[\"x\"][idx]\n",
    "    print(\"Input Sequence:\", \" \".join([vocab[i] for i in inp_seq[:20]]))\n",
    "    gen_seq = generate_sequence(inp_seq, vocab, 50)\n",
    "    print(\"\\nGenerated Sequence:\", gen_seq)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the network is able to generate meaningful sentences."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fe_jupyter]",
   "language": "python",
   "name": "conda-env-fe_jupyter-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
